[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "研究室と講義の R コード",
    "section": "",
    "text": "水圏植物生態学研究室 の基礎統計学, 教育関係共同利用拠点の 水産海洋データ解析演習 の講義資料です。\nマニュアルの更新日：2022年09月17日 11:28:38 JST\n【重要】R コードに自身あるが、日本語はチンプンカンプンかも。It is easier to explain this stuff in English.\nコードは考えずにコピペしないでください。 コードに使用したデータは研究室のサーバにあるので、公開されていません。\n研究室の皆さん： 研究室用のデータは RStudio の ~/Lab_Data/ に入っています。\nはここ https://cran.r-project.org/ からダウンロードできます。Windows ユーザは Rtools42 もインストールしましょう。ページ内の Rtools42 installer のリンクを探してね。\nRStudio もインストールしましょう。Rのデフォルトの IDE はとても使いにくいです。\n重要: Windows にインストールするとき、OneDrive にインストールされないように注意してください。 詳細は 奥村先生のサイト を参考にしてください。\nSession Information"
  },
  {
    "objectID": "index.html#この資料について",
    "href": "index.html#この資料について",
    "title": "研究室と講義の R コード",
    "section": "この資料について",
    "text": "この資料について\n\nFira Code プログラミング用等幅フォントを使っています。 このフォントにより、演算子は見やすくなりますが、すこしなれる必要はあります。\n\nたとえば\n\n<- は < と - の合字 (リガチャー, ligature) です (ALT + -)\n|> は | と > の合字です (CTRL + SHIFT + M)\n<= は < と = の合字です\n!= は ! と = の合字です\n\nよく使うリンク\n\nCross Validated\nokumuralab.org\n私たちのR: ベストプラクティスの探究\npsychometrtics_syllabus\nRによる社会調査データ分析の手引き\nbiostatistics\ncucumber flesh (ブログ)"
  },
  {
    "objectID": "index.html#quick-reference",
    "href": "index.html#quick-reference",
    "title": "研究室と講義の R コード",
    "section": "Quick Reference",
    "text": "Quick Reference\nヘルプ\n\n?mean                          # mean() 関数のヘルプをみる\nhelp.search(\"mean\")            # 文字列で検索\nhelp(package = \"tidyverse\")    # パッケージのヘルプをみる\n\nライブラリー (library)\n\ninstall.packages(\"tidyverse\")   # CRANサーバからパッケージをインストールする\n\n一回インストールすればいいので、install.packages() は毎回する必要はない。\n\nlibrary(tidyverse)              # パッケージをライブラリーから読み込む\nnlstools::preview()             # パッケージ内の関数を直接使う\n\nプログラミング文の基礎\nfor loop\nfor (変数名 in シークエンス) {\n  繰り返し実行するコード\n}\n\nfor(i in 1:10) {\n  j = j + i\n  print(j)\n}\n\nwhile loop\nwhile(条件) {\n  繰り返し実行するコード\n}\n\ni = 0\nwhile(i <= 10) {\n  i = i + 1\n  print(i)\n}\n\nif else\nif(条件) {\n  コード\n} else {\n　条件を満たさないときのコード\n}\n\nx1 = sample(1:6, 1) \nx2 = sample(1:6, 1)\nif(near(x1 + x2, 7)) {\n  print(paste(x1, x2))\n} else {\n  print(x1 + x2)}\n\n条件判定\n\nx == y : x と y は等しい\nx != y : x と y は等しくない\nx > y : x が y より大きい\nx < y : x が y より小さい\nx >= y : x が y 以上\nx <= y : x が y 以下\nis.na(x) : x が欠損値である\nis.null(x) : x が null 値である\nany(x %in% y) : x が y に含まれる\n\nfunction\n関数名 = function(変数1, 変数2, ...) {\n  コード\n}\n\nse = function(x, na.rm = FALSE) {\n  s = sd(x, na.rm = na.rm)\n  n = length(na.omit(x))\n  s / sqrt(n - 1)\n}\n\nファイルの入出力\n研究室では、ファイルの入出力に tidyverse　パッケージの関数を使っています。\n\nlibrary(tidyerse)\n\n\n# ファイルの読み込み\ndset = read_table(\"filename.txt\") # タブ・コンマ区切りのテキストファイル\ndset = read_csv(\"filename.csv\")   # コンマ区切りの csv ファイル\ndset = read_rds(\"filename.rds\")   # R オブジェクトファイル\n\n\n# ファイルの書き込み。文字のエンコーディングは UTF-8 です。\nwrite_tsv(dset, \"filename.txt\")         # タブ区切りの txt ファイル\nwrite_csv(dset, \"filename.csv\")         # コンマ区切りの csv ファイル\nwrite_excel_csv(dset, \"filename.csv\")   # Excel用 コンマ区切りの csv ファイル\nwrite_rds(dset, \"filename.rds\")         # R オブジェクトファイル\n\n資料は自由に使ってください (MIT License)。 サイトは Quarto で作成しました。"
  },
  {
    "objectID": "part01.html",
    "href": "part01.html",
    "title": "R の基本操作",
    "section": "",
    "text": "代入は = か <- (< と -) です。伝統的に使われる代入は <- ですが、私は = を使っています。\n左辺は変数名、右辺は値です。\n\n\n# 二種類の代入と c() 関数\na = 4.2\nb <- 5.0\nc(a, b)\n\n[1] 4.2 5.0\n\n\n\nc() は渡された引数を結合します。\n# の後から続く文字列はコードとして実行されません。実行されない文書はコメントと呼びます。\n\n\n(a + b) * c(a, b)\n\n[1] 38.64 46.00\n\n\n\nR はベクトル処理という実行機構が特徴的です。\n上のコードは (a + b) \\times a と (a + b) \\times b を求めています。\n\nRStudio の場合 <- は ALT + - のショートカットを定義しています。"
  },
  {
    "objectID": "part01.html#r-の主なデータタイプとデータ構造",
    "href": "part01.html#r-の主なデータタイプとデータ構造",
    "title": "R の基本操作",
    "section": "R の主なデータタイプとデータ構造",
    "text": "R の主なデータタイプとデータ構造\n\n整数 integer\n実数 double, numeric\n複素数 complex number\n時系列 time-series (POSIX)\n文字列 character\n論理値 logical\n因子 factor\nベクトル　vector\n配列 array, matrix\nリスト list\nテーブル（データフレーム） dataframe"
  },
  {
    "objectID": "part01.html#データの作り方",
    "href": "part01.html#データの作り方",
    "title": "R の基本操作",
    "section": "データの作り方",
    "text": "データの作り方\nベクトル\n\na = c(10.3, 20.2, 30.1)\nb = c(\"rabbit\", \"cat\", \"mouse\", \"dog\")\nd = c(TRUE, FALSE, T)\ne = factor(c(\"nagasaki\", \"kagoshima\", \"fukuoka\"))\n\nリスト\nベクトルの長さは異なってもいい。 ここでは、リストの要素名を指定しました。\n\nz1 = list(\"A\" = a, \"B\"= b, \"D\" = d, \"E\" = e)\n\nデータフレーム\nベクトルの長さを揃える必要がある。 ここではb[1:3]をbに渡すことで、変数名を指定しました。\n\nz2 = data.frame(a, b = b[1:3], d, e)"
  },
  {
    "objectID": "part01.html#リストの構造を確認しよう",
    "href": "part01.html#リストの構造を確認しよう",
    "title": "R の基本操作",
    "section": "リストの構造を確認しよう",
    "text": "リストの構造を確認しよう\nRオブジェクトの構造 (structure) は str() で確認します。\n\nstr(z1)\n\nList of 4\n $ A: num [1:3] 10.3 20.2 30.1\n $ B: chr [1:4] \"rabbit\" \"cat\" \"mouse\" \"dog\"\n $ D: logi [1:3] TRUE FALSE TRUE\n $ E: Factor w/ 3 levels \"fukuoka\",\"kagoshima\",..: 3 2 1"
  },
  {
    "objectID": "part01.html#リストからデータを抽出する",
    "href": "part01.html#リストからデータを抽出する",
    "title": "R の基本操作",
    "section": "リストからデータを抽出する",
    "text": "リストからデータを抽出する\nリストの要素は次のように抽出できます。\n\nz1$A\n\n[1] 10.3 20.2 30.1\n\n\nリストからの抽出方法は $ 以外に, [ や [[ でもできます。\n\nz1[c(\"A\", \"D\")]\nz1[c(1,4)]\nz1[[2]]\nz1[[c(2,3)]]\nz1[[2]][c(1,2)]"
  },
  {
    "objectID": "part01.html#データフレームの構造を確認しよう",
    "href": "part01.html#データフレームの構造を確認しよう",
    "title": "R の基本操作",
    "section": "データフレームの構造を確認しよう",
    "text": "データフレームの構造を確認しよう\n\nstr(z2)\n\n'data.frame':   3 obs. of  4 variables:\n $ a: num  10.3 20.2 30.1\n $ b: chr  \"rabbit\" \"cat\" \"mouse\"\n $ d: logi  TRUE FALSE TRUE\n $ e: Factor w/ 3 levels \"fukuoka\",\"kagoshima\",..: 3 2 1\n\n\nリストと似ていますが、そもそもデータフレームはリストです。 つまり、リストと同じように操作できます。\n\nz2$a\n\n[1] 10.3 20.2 30.1\n\n\n\nz2[c(\"a\", \"d\")]\nz2[c(1, 4)]\nz2[[2]]\nz2[[c(2,3)]]\nz2[[2]][c(1,2)]"
  },
  {
    "objectID": "part01.html#比較演算",
    "href": "part01.html#比較演算",
    "title": "R の基本操作",
    "section": "比較演算",
    "text": "比較演算\n\n比較に使う論理演算子：&（論理積 AND）, | （論理和 OR）, !（否定 NOT）\n\n\nA = c(5, 3, 2)\nB = c(5, 2, 1)\n\n# 論理積\n(A[1] > B[1]) & (A[1] == B[1])\n\n[1] FALSE\n\n# 論理和\n(A[1] > B[1]) | (A[1] == B[1])\n\n[1] TRUE\n\n\n\n# 否定と論理積\n!(A[1] > B[1]) & (A[1] == B[1])\n\n[1] TRUE\n\n# 否定と論理和\n(A[1] < B[2]) | !(A[1] == B[1])\n\n[1] FALSE"
  },
  {
    "objectID": "part01.html#比較演算を使ったデータの抽出",
    "href": "part01.html#比較演算を使ったデータの抽出",
    "title": "R の基本操作",
    "section": "比較演算を使ったデータの抽出",
    "text": "比較演算を使ったデータの抽出\n\na = c(10.3, 20.2, 30.1)\nb = c(\"rabbit\", \"cat\", \"mouse\")\nd = c(TRUE, FALSE, T)\ne = factor(c(\"nagasaki\", \n             \"kagoshima\", \n             \"fukuoka\"))\nZ = data.frame(a, b, d, e)\nZ[Z$a > 20, ]\n\n     a     b     d         e\n2 20.2   cat FALSE kagoshima\n3 30.1 mouse  TRUE   fukuoka\n\n\n\nZ[Z$a > 10 & Z$a < 20.2, ]\n\n     a      b    d        e\n1 10.3 rabbit TRUE nagasaki\n\nZ[Z$a > 10 & Z$a <= 20.2, ]\n\n     a      b     d         e\n1 10.3 rabbit  TRUE  nagasaki\n2 20.2    cat FALSE kagoshima\n\nZ[identical(Z$a, 20) | !Z$d, ]\n\n     a   b     d         e\n2 20.2 cat FALSE kagoshima\n\n\n重要! 数値の比較について\nパソコンは 2 進数で計算しているので、数値は正確ではない！\nたとえば：\n\n0.2 * 0.2 / 0.2 == 0.2 # = と =\n\n[1] FALSE\n\n\n数値の比較をする場合は all.equal() を使いましょう。\n\n# 上のコードと同じ\nall.equal(0.2 * 0.2 / 0.2, 0.2, tolerance = 0)\n\n[1] \"Mean relative difference: 1.387779e-16\"\n\n# 機械誤差を考慮した比較\nall.equal(0.2 * 0.2 / 0.2, 0.2, tolerance = .Machine$double.eps)\n\n[1] TRUE\n\n\n\nちなみに比較用記号は <, >, >= (> と =), <= (< と =), != (! と =), == (= と =)　です。"
  },
  {
    "objectID": "part02.html",
    "href": "part02.html",
    "title": "R 関数の基本",
    "section": "",
    "text": "関数をつくることにより、 での作業がとても楽になります。 コードを繰り返して使うなら関数をつくりましょう。\n\n\nR の関数に 2 つのパーツがあります。\n\nArguments: 引数\nCode block: 関数のコードは {} の間に納めます。\n\n\nhello = function(x) {\n  if(!is.character(x)) {\n    stop(\"Please provide a character string.\")\n  }\n  sprintf(\"Hello %s!\", x)\n}\n\nhello(214)\n\nError in hello(214): Please provide a character string.\n\nhello(\"Yukio\")\n\n[1] \"Hello Yukio!\"\n\n\n\n\n\n関数の中に作ったものは、関数の中にしか存在しない。\n\n\n\n\nsumofsquare = function(x) {\n  ss = (x - mean(x))^2 # 関数の外から見れない\n  ssq = sum(ss) # 関数の外から見れない\n  ssq # 関数の外に返す\n}\ndata = sample(1:10, 5, replace = TRUE)\ndata\n\n[1]  7  6 10  7  4\n\nvalue = sumofsquare(data)\nvalue\n\n[1] 18.8\n\n\n\n\n\nところが、関数は外の環境に存在するものは見れます。 このように関数を作ると、バグを起こしやすいので、注意。\n\nsumofsquare = function(x) {\n  ss = (s - mean(s))^2 # s は関数の外にあるが、関数の引数ではない\n  ssq = sum(ss) # 関数の外から見れない\n  ssq # 関数の外に返す\n}\ns = sample(100:1000, 5, replace = TRUE)\ns\n\n[1] 169 291 862 713 721\n\ndata = sample(1:10, 5, replace = TRUE)\ndata\n\n[1] 5 7 9 7 3\n\nvalue = sumofsquare(data)\nvalue　# これは s の平方和です。\n\n[1] 365388.8\n\n\n\n\n\n関数は次のようにもかけます。 \\(x){...} はラムダ式 (lambda expression) とも呼ばれています。\n\nadd_one = \\(x) { x + 1}\nadd_one(5)\n\n[1] 6\n\n\n無名関数をつくるときに便利な書き方です。\n\n# どちれも無名関数ですが、２つ目の関数がはラムダ式です。\nz = 1:5\nsapply(z, FUN = function(s){s^2})\n\n[1]  1  4  9 16 25\n\nsapply(z, FUN = \\(s){s^2})\n\n[1]  1  4  9 16 25"
  },
  {
    "objectID": "part03.html",
    "href": "part03.html",
    "title": "Tidyverse",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.4.0 \n✔ readr   2.1.2      ✔ forcats 0.5.1 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "part03.html#tidyverse-1",
    "href": "part03.html#tidyverse-1",
    "title": "Tidyverse",
    "section": "tidyverse",
    "text": "tidyverse\ntidyverse はメタパッケージなので、library(tidyverse) を実行すると次の 8 つのパッケージが読み込まれます。\n\ndplyr：データの変形・加工\nforcats：factor() 因子が使いやすくなります\nggplot2：データの可視化・作図\npurrr ：関数型プログラミング\nreadr ：CSV、TSVデータの読み込み\nstringr ：文字列の操作が楽になる\ntibble ：データフレームの操作が楽になる\ntidyr ：データをタイディ (tidy) にして操作しやすくなる\n\ntidyverse の概念をもっと知りたい方は tidyverse のマニフェスト を読みましょう。"
  },
  {
    "objectID": "part03.html#tibble-には-list-列を入れられる",
    "href": "part03.html#tibble-には-list-列を入れられる",
    "title": "Tidyverse",
    "section": "tibble には list 列を入れられる",
    "text": "tibble には list 列を入れられる\nちょっと高度の方法ですが, list を変数の要素として記録できます。\n\na1 = list(1,5,1,3,5,1)\na2 = list(2,3,5,2)\na3 = list(\"A\",\"b\",\"E\")\ntibble(a = 1:3, values = list(a1, a2, a3))\n\n# A tibble: 3 × 2\n      a values    \n  <int> <list>    \n1     1 <list [6]>\n2     2 <list [4]>\n3     3 <list [3]>\n\n\nvalues 列は list の list ですね。"
  },
  {
    "objectID": "part03.html#列名変数名について",
    "href": "part03.html#列名変数名について",
    "title": "Tidyverse",
    "section": "列名・変数名について",
    "text": "列名・変数名について\ndata.frame() は無効な変数名を自動的に変更します。\n\ndata.frame(`1 name` = 1) |> names()\n\n[1] \"X1.name\"\n\n\ntibble() はそのままにしてくれます。\n\ntibble(`1 name` = 1) |> names()\n\n[1] \"1 name\"\n\n\n\n\n\ndata.frame()の場合 * 有効な変数名：文字または、ドット(.)と文字から始まる文字列。変数名に使用できるものは文字、数字、ドットとアンダースコア (_) だけです。 * 無効な変数名の例：2021FY, 2021 FY, 2020-FY, FY-2021 は自動的に X2021FY, X2021.FY, X2020.FY, FY.2021 に変更されます。\ntibble()の場合 * 変数名はそのまま使えますが、使うときは ` ` （バクチック）に囲んでください。\nところが! どうしても data.frame() に無効な変数名を使いたいのであれば、check.names = F を渡してください。\n\ndata.frame(`1 name` = 1, check.names = FALSE) |> names()\n\n[1] \"1 name\""
  },
  {
    "objectID": "part03.html#引数を連続的に使える",
    "href": "part03.html#引数を連続的に使える",
    "title": "Tidyverse",
    "section": "引数を連続的に使える",
    "text": "引数を連続的に使える\ntibble()はこのように, 計算処理をしながらデータフレームを構築できます。\n\ntibble(x = 1:4, `x^2` = x^2, `sqrt(x)` = sqrt(x))\n\n# A tibble: 4 × 3\n      x `x^2` `sqrt(x)`\n  <int> <dbl>     <dbl>\n1     1     1      1   \n2     2     4      1.41\n3     3     9      1.73\n4     4    16      2"
  },
  {
    "objectID": "part03.html#ベクトルをリサイクルしない",
    "href": "part03.html#ベクトルをリサイクルしない",
    "title": "Tidyverse",
    "section": "ベクトルをリサイクルしない",
    "text": "ベクトルをリサイクルしない\n二つのベクトルの長さが異なるときに, データフレームを作ると, 小さいほうのベクトルは先頭から繰り返して使われます。ただし長いベクトルの要素数は短いベクトルの要素数で除算できる必要があります。\n\nx = 1:4\ny = 1:8\ndata.frame(x, y)\n\n  x y\n1 1 1\n2 2 2\n3 3 3\n4 4 4\n5 1 5\n6 2 6\n7 3 7\n8 4 8\n\n\nところが, この機能はデータ解析時にバグの原因になります。tibble()はベクトルのリサイクルはできません。\n\nx = 1:4\ny = 1:8\ntibble(x, y)\n\nError:\n! Tibble columns must have compatible sizes.\n• Size 4: Existing data.\n• Size 8: Column at position 2.\nℹ Only values of size one are recycled."
  },
  {
    "objectID": "part03.html#io-関係の関数",
    "href": "part03.html#io-関係の関数",
    "title": "Tidyverse",
    "section": "I/O 関係の関数",
    "text": "I/O 関係の関数\n読み込み関数\n\nread_delim()：一般性の高い関数, 区切りの指定が必要\nread_csv()：コンマ区切りフィアルの読み込み（csv ファイル）\nread_table()：ホワイトスペース区切りファイルの読み込み（タブ・スペース区切りファイル）\nread_rds()：R オブジェクトの読み込み\n\n書き出し関数\n\nwrite_delim()：一般性の高い関数, 区切りの指定が必要\nwrite_csv()：コンマ区切りフィアルの書き出し\nwrite_excel_csv()：Excel 用にコンマ区切りフィアルを書き出す\nwrite_table()：ホワイトスペース区切りファイルの書き出し（タブ・スペース区切りファイル）\nwrite_rds()：R オブジェクトの書き出し\nggsave(): ggplot2 でつくった図を書き出し"
  },
  {
    "objectID": "part03.html#read_csv-の重要な引数",
    "href": "part03.html#read_csv-の重要な引数",
    "title": "Tidyverse",
    "section": "read_csv() の重要な引数",
    "text": "read_csv() の重要な引数\n\nfile：パスとファイル名\ncol_names = TRUE：TRUEのとき, 1行目は列名として使う, FALSE のときは列名を自動的に作成する, 文字列ベクトルを渡せば読み込み中に列名を付けられます\ncol_types = NULL：列のデータ型を指定できるが NULL のときは関数に任せる\ncomment = \"\"：コメント記号を指定し, コメント記号後の文字を無視する\nskip = 0 先頭から無視する行数\nlocale：ロケール（地域の設定）\nn_max = Inf：読み込む行数、デフォルトは全ての行数"
  },
  {
    "objectID": "part03.html#read_csvの使い方",
    "href": "part03.html#read_csvの使い方",
    "title": "Tidyverse",
    "section": "read_csv()の使い方",
    "text": "read_csv()の使い方\nread_csv()\n\nrabbits = read_csv(\"Assignment_06_Dataset01.csv\")\nrabbits\n\n\n\nRows: 37 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): host\ndbl (1): scutum.width\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 37 × 2\n   host    scutum.width\n   <chr>          <dbl>\n 1 Rabbit1          380\n 2 Rabbit1          376\n 3 Rabbit1          360\n 4 Rabbit1          368\n 5 Rabbit1          372\n 6 Rabbit1          366\n 7 Rabbit1          374\n 8 Rabbit1          382\n 9 Rabbit2          350\n10 Rabbit2          356\n# … with 27 more rows"
  },
  {
    "objectID": "part03.html#readxl-パッケージ",
    "href": "part03.html#readxl-パッケージ",
    "title": "Tidyverse",
    "section": "readxl パッケージ",
    "text": "readxl パッケージ\nreadxl は Microsoft Excelファイルの読み込みに使えるパッケージです。\n\nlibrary(readxl)\n\nファイルの読み込みには read_excel() を使いますが、研究室では read_xlsx() もよく使います。 read_excel() は read_xlsx() のラッパーです。 使い方は全くおなじです。\n重要： エクセルでデータの管理をした場合エクセルのオートコレクト機能によってデータがかってに変換されるので気をつけましょう。遺伝子の名前のオートコレクトによく問題が発生すると報告されています。とくに Excel と Google Sheets のオートコレクトはアグレッシブです。Abeysooriya et al. 2021. PLOS Computational Biology。"
  },
  {
    "objectID": "part03.html#read_excel-の主な引数",
    "href": "part03.html#read_excel-の主な引数",
    "title": "Tidyverse",
    "section": "read_excel() の主な引数",
    "text": "read_excel() の主な引数\n\npath：パスとファイル名\nsheet = NULL：読み込むシート名またはシートインデックス\nrange = NULL：読み込む範囲, 例えば “B3:D8” または, “Data!B3:D8”\ncol_names = TRUE：1行目を列名として使う論理値\ncol_types = NULL：読み込む列のデータ型を指定できます (デフォルトは guess)\nna = \"\"：欠損値の定義, 空セルは欠損値とされます\nskip = 0：無視する行数\nn_max = Inf：読み込む最大行数"
  },
  {
    "objectID": "part03.html#read_excel-の使用例１",
    "href": "part03.html#read_excel-の使用例１",
    "title": "Tidyverse",
    "section": "read_excel() の使用例（１）",
    "text": "read_excel() の使用例（１）\n最初のシート (sheet = 1) の先頭から1行無視して (skip = 1) データを読み込む。\n\nfilename = \"Table 2.xlsx\"\nexceldata = read_excel(filename, sheet = 1, skip = 1)\nexceldata\n\n\n\nNew names:\n• `WT (˚C)` -> `WT (˚C)...2`\n• `S.D.**` -> `S.D.**...3`\n• `` -> `...4`\n• `WT (˚C)` -> `WT (˚C)...5`\n• `S.D.**` -> `S.D.**...6`\n\n\n# A tibble: 12 × 6\n   Month `WT (˚C)...2` `S.D.**...3` ...4  `WT (˚C)...5` `S.D.**...6`\n   <chr>         <dbl>        <dbl> <lgl>         <dbl>        <dbl>\n 1 Jan.           21.1        0.446 NA             16.5        0.428\n 2 Feb.           21.3        0.441 NA             16.3        0.483\n 3 Mar.           21.5        0.470 NA             16.5        0.579\n 4 Apr.           21.8        0.554 NA             18.3        1.27 \n 5 May            23.4        0.726 NA             21.1        1.08 \n 6 Jun.           25.5        1.20  NA             22.9        1.02 \n 7 Jul.           28.6        0.491 NA             26.6        1.15 \n 8 Aug.           28.8        0.546 NA             28.5        0.470\n 9 Sep.           28.5        0.375 NA             27.7        0.794\n10 Oct.           26.6        0.893 NA             24.4        1.05 \n11 Nov.           24.7        0.516 NA             21.6        0.928\n12 Dec.           22.8        0.720 NA             19.1        0.893"
  },
  {
    "objectID": "part03.html#read_excel-の使用例２",
    "href": "part03.html#read_excel-の使用例２",
    "title": "Tidyverse",
    "section": "read_excel() の使用例（２）",
    "text": "read_excel() の使用例（２）\n先程のように読み込むと、不都合な変数名に変換されました。次は、変数名も指定して読み込みます。\n\nfilename = \"Table 2.xlsx\"\ncol_names = c(\"month\", \"temperature1\", \"sd1\", \"empty\",\"temperature2\", \"sd2\")\nexceldata = read_excel(filename, sheet = 1, skip = 2, col_name = col_names)\nexceldata |> print(n = 4)\n\n\n\n# A tibble: 12 × 6\n  month temperature1   sd1 empty temperature2   sd2\n  <chr>        <dbl> <dbl> <lgl>        <dbl> <dbl>\n1 Jan.          21.1 0.446 NA            16.5 0.428\n2 Feb.          21.3 0.441 NA            16.3 0.483\n3 Mar.          21.5 0.470 NA            16.5 0.579\n4 Apr.          21.8 0.554 NA            18.3 1.27 \n# … with 8 more rows\n\n\nシートの２行目には変数名が記録されているので、skip = 2 を渡しました。"
  },
  {
    "objectID": "part03.html#データの出力",
    "href": "part03.html#データの出力",
    "title": "Tidyverse",
    "section": "データの出力",
    "text": "データの出力\nCSVファイルの出力\n\nfname = \"table2_output.csv\"\nexceldata |> write_csv(file = fname) # 文字コードは UTF-8 です。\n\nエクセルにCSVファイルを読み込んで文字化けした場合、write_excel_csv()を試してみてください。\n\nexceldata |> write_excel_csv(file = fname)\n\nRDSファイルの出力\nRのオブジェクトをバイナリファイルとして保存したい場合は write_rds() を使います。\n\nfname = \"table2_output.rds\"\nexceldata |> write_rds(file = fname)"
  },
  {
    "objectID": "part04.html",
    "href": "part04.html",
    "title": "データの操作",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.4.0 \n✔ readr   2.1.2      ✔ forcats 0.5.1 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "part04.html#理想的なデータ構造",
    "href": "part04.html#理想的なデータ構造",
    "title": "データの操作",
    "section": "理想的なデータ構造",
    "text": "理想的なデータ構造\n\n1 行 = 1 観測値\n1 列 = 1 変数"
  },
  {
    "objectID": "part04.html#データ加工操作用関数",
    "href": "part04.html#データ加工操作用関数",
    "title": "データの操作",
    "section": "データ加工・操作用関数",
    "text": "データ加工・操作用関数\nデータの結合 (mutating join)\nx, y, by は関数の引数です。by で指定したキー（変数名）が一致するように行を合わせることができる。\n\nfull_join(x, y, by)：全ての x と y 行と列を結合する。\ninner_join(x, y, by)：x と y で共通する行と列を結合する。\nleft_join(x, y, by)：左側（）第 1 引数のtibble に y の変数を追加する。\nright_join(x, y, by)：右側（）第 2 引数のtibble に x の変数を追加する。\n\nデータの結合 (join)\n\nbind_cols()：渡したtibbleを横に結合する（行数が異なったらエラーが発生する）。\nbind_rows()：渡した tibble を立てに結合する（一致する変数名を合わせてくれます）。"
  },
  {
    "objectID": "part04.html#mutating-join-のつかいかた",
    "href": "part04.html#mutating-join-のつかいかた",
    "title": "データの操作",
    "section": "mutating join のつかいかた",
    "text": "mutating join のつかいかた\n\nX = tibble(x = c(\"A\", \"B\", \"C\", \"G\"), y = c(NA, rnorm(3, mean = 5)))\nY = tibble(x = c(\"A\", \"C\", \"D\", \"E\"), z = c(rpois(3, lambda = 5), NA))\n\n\nX\n\n# A tibble: 4 × 2\n  x         y\n  <chr> <dbl>\n1 A     NA   \n2 B      5.21\n3 C      6.81\n4 G      6.13\n\n\n\nY\n\n# A tibble: 4 × 2\n  x         z\n  <chr> <int>\n1 A         9\n2 C         8\n3 D         5\n4 E        NA\n\n\n\nfull_join(X,Y, by = \"x\")\n\n# A tibble: 6 × 3\n  x         y     z\n  <chr> <dbl> <int>\n1 A     NA        9\n2 B      5.21    NA\n3 C      6.81     8\n4 G      6.13    NA\n5 D     NA        5\n6 E     NA       NA\n\n\n\ninner_join(X, Y, by = \"x\")\n\n# A tibble: 2 × 3\n  x         y     z\n  <chr> <dbl> <int>\n1 A     NA        9\n2 C      6.81     8\n\n\n\nleft_join(X, Y, by = \"x\")\n\n# A tibble: 4 × 3\n  x         y     z\n  <chr> <dbl> <int>\n1 A     NA        9\n2 B      5.21    NA\n3 C      6.81     8\n4 G      6.13    NA\n\n\n\nright_join(X, Y, by = \"x\")\n\n# A tibble: 4 × 3\n  x         y     z\n  <chr> <dbl> <int>\n1 A     NA        9\n2 C      6.81     8\n3 D     NA        5\n4 E     NA       NA\n\n\n\nbind_rows(X, Y)\n\n# A tibble: 8 × 3\n  x         y     z\n  <chr> <dbl> <int>\n1 A     NA       NA\n2 B      5.21    NA\n3 C      6.81    NA\n4 G      6.13    NA\n5 A     NA        9\n6 C     NA        8\n7 D     NA        5\n8 E     NA       NA\n\n\n\nbind_rows(\"X\" = X, \"Y\" = Y, .id = \"origin\")\n\n# A tibble: 8 × 4\n  origin x         y     z\n  <chr>  <chr> <dbl> <int>\n1 X      A     NA       NA\n2 X      B      5.21    NA\n3 X      C      6.81    NA\n4 X      G      6.13    NA\n5 Y      A     NA        9\n6 Y      C     NA        8\n7 Y      D     NA        5\n8 Y      E     NA       NA\n\n\n\nbind_cols(X, Y)\n\nNew names:\n• `x` -> `x...1`\n• `x` -> `x...3`\n\n\n# A tibble: 4 × 4\n  x...1     y x...3     z\n  <chr> <dbl> <chr> <int>\n1 A     NA    A         9\n2 B      5.21 C         8\n3 C      6.81 D         5\n4 G      6.13 E        NA"
  },
  {
    "objectID": "part04.html#行と列の加工操作用関数",
    "href": "part04.html#行と列の加工操作用関数",
    "title": "データの操作",
    "section": "行と列の加工・操作用関数",
    "text": "行と列の加工・操作用関数\n列における操作\n\nmutate()：既存の変数の書き換えや変数の追加する\nselect()：既存の変数を選らぶ\nrename()：既存の変数の名前を変える\npull()：既存の変数をリストとして抽出する\n`relocate()``：指定した列の位置を変える\n\n行における操作\n\nfilter()：条件を満たした行を返す\ndistinct()：指定した変数から重複している行を外す\nslice()：指定した行インデックスを返す\narrange()：指定した列の昇順で行を並べ替える"
  },
  {
    "objectID": "part04.html#列の加工",
    "href": "part04.html#列の加工",
    "title": "データの操作",
    "section": "列の加工",
    "text": "列の加工\n\niris |> as_tibble() |> mutate(P2 = Petal.Length^2)\n\n# A tibble: 150 × 6\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species    P2\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>   <dbl>\n 1          5.1         3.5          1.4         0.2 setosa   1.96\n 2          4.9         3            1.4         0.2 setosa   1.96\n 3          4.7         3.2          1.3         0.2 setosa   1.69\n 4          4.6         3.1          1.5         0.2 setosa   2.25\n 5          5           3.6          1.4         0.2 setosa   1.96\n 6          5.4         3.9          1.7         0.4 setosa   2.89\n 7          4.6         3.4          1.4         0.3 setosa   1.96\n 8          5           3.4          1.5         0.2 setosa   2.25\n 9          4.4         2.9          1.4         0.2 setosa   1.96\n10          4.9         3.1          1.5         0.1 setosa   2.25\n# … with 140 more rows\n\n\n\niris |> as_tibble() |> select(Species, Petal.Length)\n\n# A tibble: 150 × 2\n   Species Petal.Length\n   <fct>          <dbl>\n 1 setosa           1.4\n 2 setosa           1.4\n 3 setosa           1.3\n 4 setosa           1.5\n 5 setosa           1.4\n 6 setosa           1.7\n 7 setosa           1.4\n 8 setosa           1.5\n 9 setosa           1.4\n10 setosa           1.5\n# … with 140 more rows\n\n\n\niris |> as_tibble() |> select(matches(\"Length\"))\n\n# A tibble: 150 × 2\n   Sepal.Length Petal.Length\n          <dbl>        <dbl>\n 1          5.1          1.4\n 2          4.9          1.4\n 3          4.7          1.3\n 4          4.6          1.5\n 5          5            1.4\n 6          5.4          1.7\n 7          4.6          1.4\n 8          5            1.5\n 9          4.4          1.4\n10          4.9          1.5\n# … with 140 more rows\n\n\n\niris |> as_tibble() |> rename(PL = Petal.Length)\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width    PL Petal.Width Species\n          <dbl>       <dbl> <dbl>       <dbl> <fct>  \n 1          5.1         3.5   1.4         0.2 setosa \n 2          4.9         3     1.4         0.2 setosa \n 3          4.7         3.2   1.3         0.2 setosa \n 4          4.6         3.1   1.5         0.2 setosa \n 5          5           3.6   1.4         0.2 setosa \n 6          5.4         3.9   1.7         0.4 setosa \n 7          4.6         3.4   1.4         0.3 setosa \n 8          5           3.4   1.5         0.2 setosa \n 9          4.4         2.9   1.4         0.2 setosa \n10          4.9         3.1   1.5         0.1 setosa \n# … with 140 more rows\n\n\n\niris |> as_tibble() |> \n  rename_with(~str_replace_all(.x, \"[(a-z.)]\", \"\"), .cols = matches(\"(Pet)|(Sep)\"))\n\n# A tibble: 150 × 5\n      SL    SW    PL    PW Species\n   <dbl> <dbl> <dbl> <dbl> <fct>  \n 1   5.1   3.5   1.4   0.2 setosa \n 2   4.9   3     1.4   0.2 setosa \n 3   4.7   3.2   1.3   0.2 setosa \n 4   4.6   3.1   1.5   0.2 setosa \n 5   5     3.6   1.4   0.2 setosa \n 6   5.4   3.9   1.7   0.4 setosa \n 7   4.6   3.4   1.4   0.3 setosa \n 8   5     3.4   1.5   0.2 setosa \n 9   4.4   2.9   1.4   0.2 setosa \n10   4.9   3.1   1.5   0.1 setosa \n# … with 140 more rows\n\n\n\niris |> as_tibble() |> pull(Species)\n\n  [1] setosa     setosa     setosa     setosa     setosa     setosa    \n  [7] setosa     setosa     setosa     setosa     setosa     setosa    \n [13] setosa     setosa     setosa     setosa     setosa     setosa    \n [19] setosa     setosa     setosa     setosa     setosa     setosa    \n [25] setosa     setosa     setosa     setosa     setosa     setosa    \n [31] setosa     setosa     setosa     setosa     setosa     setosa    \n [37] setosa     setosa     setosa     setosa     setosa     setosa    \n [43] setosa     setosa     setosa     setosa     setosa     setosa    \n [49] setosa     setosa     versicolor versicolor versicolor versicolor\n [55] versicolor versicolor versicolor versicolor versicolor versicolor\n [61] versicolor versicolor versicolor versicolor versicolor versicolor\n [67] versicolor versicolor versicolor versicolor versicolor versicolor\n [73] versicolor versicolor versicolor versicolor versicolor versicolor\n [79] versicolor versicolor versicolor versicolor versicolor versicolor\n [85] versicolor versicolor versicolor versicolor versicolor versicolor\n [91] versicolor versicolor versicolor versicolor versicolor versicolor\n [97] versicolor versicolor versicolor versicolor virginica  virginica \n[103] virginica  virginica  virginica  virginica  virginica  virginica \n[109] virginica  virginica  virginica  virginica  virginica  virginica \n[115] virginica  virginica  virginica  virginica  virginica  virginica \n[121] virginica  virginica  virginica  virginica  virginica  virginica \n[127] virginica  virginica  virginica  virginica  virginica  virginica \n[133] virginica  virginica  virginica  virginica  virginica  virginica \n[139] virginica  virginica  virginica  virginica  virginica  virginica \n[145] virginica  virginica  virginica  virginica  virginica  virginica \nLevels: setosa versicolor virginica\n\n\n\niris |> as_tibble() |> relocate(Species, .before = \"Sepal.Length\")\n\n# A tibble: 150 × 5\n   Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n   <fct>          <dbl>       <dbl>        <dbl>       <dbl>\n 1 setosa           5.1         3.5          1.4         0.2\n 2 setosa           4.9         3            1.4         0.2\n 3 setosa           4.7         3.2          1.3         0.2\n 4 setosa           4.6         3.1          1.5         0.2\n 5 setosa           5           3.6          1.4         0.2\n 6 setosa           5.4         3.9          1.7         0.4\n 7 setosa           4.6         3.4          1.4         0.3\n 8 setosa           5           3.4          1.5         0.2\n 9 setosa           4.4         2.9          1.4         0.2\n10 setosa           4.9         3.1          1.5         0.1\n# … with 140 more rows\n\n\n\niris |> as_tibble() |> relocate(Species, matches(\"Length\"), .before = \"Sepal.Length\")\n\n# A tibble: 150 × 5\n   Species Sepal.Length Petal.Length Sepal.Width Petal.Width\n   <fct>          <dbl>        <dbl>       <dbl>       <dbl>\n 1 setosa           5.1          1.4         3.5         0.2\n 2 setosa           4.9          1.4         3           0.2\n 3 setosa           4.7          1.3         3.2         0.2\n 4 setosa           4.6          1.5         3.1         0.2\n 5 setosa           5            1.4         3.6         0.2\n 6 setosa           5.4          1.7         3.9         0.4\n 7 setosa           4.6          1.4         3.4         0.3\n 8 setosa           5            1.5         3.4         0.2\n 9 setosa           4.4          1.4         2.9         0.2\n10 setosa           4.9          1.5         3.1         0.1\n# … with 140 more rows"
  },
  {
    "objectID": "part04.html#行の加工",
    "href": "part04.html#行の加工",
    "title": "データの操作",
    "section": "行の加工",
    "text": "行の加工\n\niris |> as_tibble() |> filter(str_detect(Species, \"versicolor\"))\n\n# A tibble: 50 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n 1          7           3.2          4.7         1.4 versicolor\n 2          6.4         3.2          4.5         1.5 versicolor\n 3          6.9         3.1          4.9         1.5 versicolor\n 4          5.5         2.3          4           1.3 versicolor\n 5          6.5         2.8          4.6         1.5 versicolor\n 6          5.7         2.8          4.5         1.3 versicolor\n 7          6.3         3.3          4.7         1.6 versicolor\n 8          4.9         2.4          3.3         1   versicolor\n 9          6.6         2.9          4.6         1.3 versicolor\n10          5.2         2.7          3.9         1.4 versicolor\n# … with 40 more rows\n\n\n\niris |> as_tibble() |> filter(Petal.Length > 6 & Sepal.Length > 7.5)\n\n# A tibble: 6 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n         <dbl>       <dbl>        <dbl>       <dbl> <fct>    \n1          7.6         3            6.6         2.1 virginica\n2          7.7         3.8          6.7         2.2 virginica\n3          7.7         2.6          6.9         2.3 virginica\n4          7.7         2.8          6.7         2   virginica\n5          7.9         3.8          6.4         2   virginica\n6          7.7         3            6.1         2.3 virginica\n\n\n\niris |> as_tibble() |> distinct(Species)\n\n# A tibble: 3 × 1\n  Species   \n  <fct>     \n1 setosa    \n2 versicolor\n3 virginica \n\n\n\niris |> as_tibble() |> distinct(Petal.Length, .keep_all = T)\n\n# A tibble: 43 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n 1          5.1         3.5          1.4         0.2 setosa    \n 2          4.7         3.2          1.3         0.2 setosa    \n 3          4.6         3.1          1.5         0.2 setosa    \n 4          5.4         3.9          1.7         0.4 setosa    \n 5          4.8         3.4          1.6         0.2 setosa    \n 6          4.3         3            1.1         0.1 setosa    \n 7          5.8         4            1.2         0.2 setosa    \n 8          4.6         3.6          1           0.2 setosa    \n 9          4.8         3.4          1.9         0.2 setosa    \n10          7           3.2          4.7         1.4 versicolor\n# … with 33 more rows\n\n\n\niris |> as_tibble() |> slice(1:5)\n\n# A tibble: 5 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n1          5.1         3.5          1.4         0.2 setosa \n2          4.9         3            1.4         0.2 setosa \n3          4.7         3.2          1.3         0.2 setosa \n4          4.6         3.1          1.5         0.2 setosa \n5          5           3.6          1.4         0.2 setosa \n\niris |> as_tibble() |> slice_head(n = 2)\n\n# A tibble: 2 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n1          5.1         3.5          1.4         0.2 setosa \n2          4.9         3            1.4         0.2 setosa \n\niris |> as_tibble() |> slice_tail(n = 2)\n\n# A tibble: 2 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n         <dbl>       <dbl>        <dbl>       <dbl> <fct>    \n1          6.2         3.4          5.4         2.3 virginica\n2          5.9         3            5.1         1.8 virginica\n\n\n\niris |> as_tibble() |> slice_min(Petal.Length)\n\n# A tibble: 1 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n1          4.6         3.6            1         0.2 setosa \n\niris |> as_tibble() |> slice_max(Petal.Length)\n\n# A tibble: 1 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n         <dbl>       <dbl>        <dbl>       <dbl> <fct>    \n1          7.7         2.6          6.9         2.3 virginica\n\niris |> as_tibble() |> slice_sample(n = 3)\n\n# A tibble: 3 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species   \n         <dbl>       <dbl>        <dbl>       <dbl> <fct>     \n1          7.7         2.8          6.7         2   virginica \n2          7.2         3            5.8         1.6 virginica \n3          5.7         2.6          3.5         1   versicolor\n\n\n\niris |> as_tibble() |> arrange(Sepal.Length)\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          4.3         3            1.1         0.1 setosa \n 2          4.4         2.9          1.4         0.2 setosa \n 3          4.4         3            1.3         0.2 setosa \n 4          4.4         3.2          1.3         0.2 setosa \n 5          4.5         2.3          1.3         0.3 setosa \n 6          4.6         3.1          1.5         0.2 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          4.6         3.6          1           0.2 setosa \n 9          4.6         3.2          1.4         0.2 setosa \n10          4.7         3.2          1.3         0.2 setosa \n# … with 140 more rows\n\n\n\niris |> as_tibble() |> \n  arrange(desc(Sepal.Length), desc(Sepal.Width))\n\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species  \n          <dbl>       <dbl>        <dbl>       <dbl> <fct>    \n 1          7.9         3.8          6.4         2   virginica\n 2          7.7         3.8          6.7         2.2 virginica\n 3          7.7         3            6.1         2.3 virginica\n 4          7.7         2.8          6.7         2   virginica\n 5          7.7         2.6          6.9         2.3 virginica\n 6          7.6         3            6.6         2.1 virginica\n 7          7.4         2.8          6.1         1.9 virginica\n 8          7.3         2.9          6.3         1.8 virginica\n 9          7.2         3.6          6.1         2.5 virginica\n10          7.2         3.2          6           1.8 virginica\n# … with 140 more rows"
  },
  {
    "objectID": "part04.html#グループ化ネストに関する関数",
    "href": "part04.html#グループ化ネストに関する関数",
    "title": "データの操作",
    "section": "グループ化・ネストに関する関数",
    "text": "グループ化・ネストに関する関数\n\ngroup_by()：tibble をグループ化する\ngroup_nest()：グループ化した tibble をネスト（入れ子）する\nnest()：渡した列をネストする\nunnest()：ネストされている列を展開（アンネスト）する\ngroup_map()：グループ化した tibble に関数を適応して、リストを返す\ngroup_modify()：グループ化した tibble に関数を適応して、tibble を返す"
  },
  {
    "objectID": "part04.html#tibble-のグループ化",
    "href": "part04.html#tibble-のグループ化",
    "title": "データの操作",
    "section": "tibble のグループ化",
    "text": "tibble のグループ化\n\niris |> as_tibble() |> select(1:3)\n\n# A tibble: 150 × 3\n   Sepal.Length Sepal.Width Petal.Length\n          <dbl>       <dbl>        <dbl>\n 1          5.1         3.5          1.4\n 2          4.9         3            1.4\n 3          4.7         3.2          1.3\n 4          4.6         3.1          1.5\n 5          5           3.6          1.4\n 6          5.4         3.9          1.7\n 7          4.6         3.4          1.4\n 8          5           3.4          1.5\n 9          4.4         2.9          1.4\n10          4.9         3.1          1.5\n# … with 140 more rows\n\n\n\niris |> as_tibble() |> group_by(Species) |> select(1:3)\n\nAdding missing grouping variables: `Species`\n\n\n# A tibble: 150 × 4\n# Groups:   Species [3]\n   Species Sepal.Length Sepal.Width Petal.Length\n   <fct>          <dbl>       <dbl>        <dbl>\n 1 setosa           5.1         3.5          1.4\n 2 setosa           4.9         3            1.4\n 3 setosa           4.7         3.2          1.3\n 4 setosa           4.6         3.1          1.5\n 5 setosa           5           3.6          1.4\n 6 setosa           5.4         3.9          1.7\n 7 setosa           4.6         3.4          1.4\n 8 setosa           5           3.4          1.5\n 9 setosa           4.4         2.9          1.4\n10 setosa           4.9         3.1          1.5\n# … with 140 more rows\n\n\n\niris |> as_tibble() |> group_nest(Species)\n\n# A tibble: 3 × 2\n  Species                  data\n  <fct>      <list<tibble[,4]>>\n1 setosa               [50 × 4]\n2 versicolor           [50 × 4]\n3 virginica            [50 × 4]\n\n\n\niris |> as_tibble() |> nest(data = matches(\"Length|Width\"))\n\n# A tibble: 3 × 2\n  Species    data             \n  <fct>      <list>           \n1 setosa     <tibble [50 × 4]>\n2 versicolor <tibble [50 × 4]>\n3 virginica  <tibble [50 × 4]>\n\n\n\niris |> as_tibble() |> group_nest(Species) |> unnest(data)\n\n# A tibble: 150 × 5\n   Species Sepal.Length Sepal.Width Petal.Length Petal.Width\n   <fct>          <dbl>       <dbl>        <dbl>       <dbl>\n 1 setosa           5.1         3.5          1.4         0.2\n 2 setosa           4.9         3            1.4         0.2\n 3 setosa           4.7         3.2          1.3         0.2\n 4 setosa           4.6         3.1          1.5         0.2\n 5 setosa           5           3.6          1.4         0.2\n 6 setosa           5.4         3.9          1.7         0.4\n 7 setosa           4.6         3.4          1.4         0.3\n 8 setosa           5           3.4          1.5         0.2\n 9 setosa           4.4         2.9          1.4         0.2\n10 setosa           4.9         3.1          1.5         0.1\n# … with 140 more rows\n\n\n\niris |> as_tibble() |> group_by(Species) |> group_map(~head(.x, n = 2))\n\n[[1]]\n# A tibble: 2 × 4\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n         <dbl>       <dbl>        <dbl>       <dbl>\n1          5.1         3.5          1.4         0.2\n2          4.9         3            1.4         0.2\n\n[[2]]\n# A tibble: 2 × 4\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n         <dbl>       <dbl>        <dbl>       <dbl>\n1          7           3.2          4.7         1.4\n2          6.4         3.2          4.5         1.5\n\n[[3]]\n# A tibble: 2 × 4\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n         <dbl>       <dbl>        <dbl>       <dbl>\n1          6.3         3.3          6           2.5\n2          5.8         2.7          5.1         1.9\n\n\n\niris |> as_tibble() |> group_by(Species) |> group_modify(~head(.x, n = 2))\n\n# A tibble: 6 × 5\n# Groups:   Species [3]\n  Species    Sepal.Length Sepal.Width Petal.Length Petal.Width\n  <fct>             <dbl>       <dbl>        <dbl>       <dbl>\n1 setosa              5.1         3.5          1.4         0.2\n2 setosa              4.9         3            1.4         0.2\n3 versicolor          7           3.2          4.7         1.4\n4 versicolor          6.4         3.2          4.5         1.5\n5 virginica           6.3         3.3          6           2.5\n6 virginica           5.8         2.7          5.1         1.9"
  },
  {
    "objectID": "part04.html#その他の関数",
    "href": "part04.html#その他の関数",
    "title": "データの操作",
    "section": "その他の関数",
    "text": "その他の関数\n\ndrop_na()：NA（欠損値）を含む行を削除\nreplace_na()：NAを他の値と書き換える\nfill()：NAを直前の値で埋める\nseparate()：文字列の変数を任意の区切りで複数変数に分裂する\nunite()：複数の変数を任意の区切りで 1 列にまとめる\n\n\nX\n\n# A tibble: 4 × 2\n  x         y\n  <chr> <dbl>\n1 A     NA   \n2 B      5.21\n3 C      6.81\n4 G      6.13\n\n\n\nX |> drop_na()\n\n# A tibble: 3 × 2\n  x         y\n  <chr> <dbl>\n1 B      5.21\n2 C      6.81\n3 G      6.13\n\n\n\nX |> replace_na(list(x = \"Z\", y = 0))\n\n# A tibble: 4 × 2\n  x         y\n  <chr> <dbl>\n1 A      0   \n2 B      5.21\n3 C      6.81\n4 G      6.13\n\n\n\nX |> mutate(y = replace_na(y, 0))\n\n# A tibble: 4 × 2\n  x         y\n  <chr> <dbl>\n1 A      0   \n2 B      5.21\n3 C      6.81\n4 G      6.13\n\n\n\nY |> fill(z)\n\n# A tibble: 4 × 2\n  x         z\n  <chr> <int>\n1 A         9\n2 C         8\n3 D         5\n4 E         5\n\n\n\ntibble(x = c(NA, \"Iris.setosa\", \"Iris.virginica\", \"Iris.versicolor\")) |> \n  separate(x, into = c(\"Genus\", \"Species\"))\n\n# A tibble: 4 × 2\n  Genus Species   \n  <chr> <chr>     \n1 <NA>  <NA>      \n2 Iris  setosa    \n3 Iris  virginica \n4 Iris  versicolor\n\n\n\ntibble(x = rep(\"Iris\", 3), y = c(\"setosa\", \"virginica\", \"versicolor\")) |> \n  unite(Species, x, y, sep = \"_\")\n\n# A tibble: 3 × 1\n  Species        \n  <chr>          \n1 Iris_setosa    \n2 Iris_virginica \n3 Iris_versicolor"
  },
  {
    "objectID": "part04.html#ピボットtibbleを変形する関数",
    "href": "part04.html#ピボットtibbleを変形する関数",
    "title": "データの操作",
    "section": "ピボット・tibbleを変形する関数",
    "text": "ピボット・tibbleを変形する関数\n\npivot_longer()：tibble を wide format （横広）から long format （縦長）に変える\npivot_wider()：tibble をlong format から wide format に変える\n\n\n重要な引数\npivot_longer()\n\ncols：動かす変数\nnames_to：動かした変数の名前の移動先\nvalues_to：動かした変数の値の移動先\nnames_transform：移動先の変数のタイプを変換\n\npivot_wider()\n\nid_cols：行（値）を区別するための列名\nnames_from：移動先の列名になる変数\nvalues_from：移動したい値\nvalues_fill：存在しない要素の埋め込み方法\nvalues_fn：行の区別ができないときの処理（デフォルトはリスト）"
  },
  {
    "objectID": "part04.html#pivot_longer-の使い方",
    "href": "part04.html#pivot_longer-の使い方",
    "title": "データの操作",
    "section": "pivot_longer() の使い方",
    "text": "pivot_longer() の使い方\n\nrelig_income |> as_tibble()\n\n# A tibble: 18 × 11\n   religion      `<$10k` $10-2…¹ $20-3…² $30-4…³ $40-5…⁴ $50-7…⁵ $75-1…⁶ $100-…⁷\n   <chr>           <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 Agnostic           27      34      60      81      76     137     122     109\n 2 Atheist            12      27      37      52      35      70      73      59\n 3 Buddhist           27      21      30      34      33      58      62      39\n 4 Catholic          418     617     732     670     638    1116     949     792\n 5 Don’t know/r…      15      14      15      11      10      35      21      17\n 6 Evangelical …     575     869    1064     982     881    1486     949     723\n 7 Hindu               1       9       7       9      11      34      47      48\n 8 Historically…     228     244     236     238     197     223     131      81\n 9 Jehovah's Wi…      20      27      24      24      21      30      15      11\n10 Jewish             19      19      25      25      30      95      69      87\n11 Mainline Prot     289     495     619     655     651    1107     939     753\n12 Mormon             29      40      48      51      56     112      85      49\n13 Muslim              6       7       9      10       9      23      16       8\n14 Orthodox           13      17      23      32      32      47      38      42\n15 Other Christ…       9       7      11      13      13      14      18      14\n16 Other Faiths       20      33      40      46      49      63      46      40\n17 Other World …       5       2       3       4       2       7       3       4\n18 Unaffiliated      217     299     374     365     341     528     407     321\n# … with 2 more variables: `>150k` <dbl>, `Don't know/refused` <dbl>, and\n#   abbreviated variable names ¹​`$10-20k`, ²​`$20-30k`, ³​`$30-40k`, ⁴​`$40-50k`,\n#   ⁵​`$50-75k`, ⁶​`$75-100k`, ⁷​`$100-150k`\n\n\n\nrelig_income |> as_tibble() |> \npivot_longer(!religion, names_to = \"income\", values_to = \"count\")\n\n# A tibble: 180 × 3\n   religion income             count\n   <chr>    <chr>              <dbl>\n 1 Agnostic <$10k                 27\n 2 Agnostic $10-20k               34\n 3 Agnostic $20-30k               60\n 4 Agnostic $30-40k               81\n 5 Agnostic $40-50k               76\n 6 Agnostic $50-75k              137\n 7 Agnostic $75-100k             122\n 8 Agnostic $100-150k            109\n 9 Agnostic >150k                 84\n10 Agnostic Don't know/refused    96\n# … with 170 more rows\n\n\n\nbillboard |> as_tibble()\n\n# A tibble: 317 × 79\n   artist track date.ent…¹   wk1   wk2   wk3   wk4   wk5   wk6   wk7   wk8   wk9\n   <chr>  <chr> <date>     <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 2 Pac  Baby… 2000-02-26    87    82    72    77    87    94    99    NA    NA\n 2 2Ge+h… The … 2000-09-02    91    87    92    NA    NA    NA    NA    NA    NA\n 3 3 Doo… Kryp… 2000-04-08    81    70    68    67    66    57    54    53    51\n 4 3 Doo… Loser 2000-10-21    76    76    72    69    67    65    55    59    62\n 5 504 B… Wobb… 2000-04-15    57    34    25    17    17    31    36    49    53\n 6 98^0   Give… 2000-08-19    51    39    34    26    26    19     2     2     3\n 7 A*Tee… Danc… 2000-07-08    97    97    96    95   100    NA    NA    NA    NA\n 8 Aaliy… I Do… 2000-01-29    84    62    51    41    38    35    35    38    38\n 9 Aaliy… Try … 2000-03-18    59    53    38    28    21    18    16    14    12\n10 Adams… Open… 2000-08-26    76    76    74    69    68    67    61    58    57\n# … with 307 more rows, 67 more variables: wk10 <dbl>, wk11 <dbl>, wk12 <dbl>,\n#   wk13 <dbl>, wk14 <dbl>, wk15 <dbl>, wk16 <dbl>, wk17 <dbl>, wk18 <dbl>,\n#   wk19 <dbl>, wk20 <dbl>, wk21 <dbl>, wk22 <dbl>, wk23 <dbl>, wk24 <dbl>,\n#   wk25 <dbl>, wk26 <dbl>, wk27 <dbl>, wk28 <dbl>, wk29 <dbl>, wk30 <dbl>,\n#   wk31 <dbl>, wk32 <dbl>, wk33 <dbl>, wk34 <dbl>, wk35 <dbl>, wk36 <dbl>,\n#   wk37 <dbl>, wk38 <dbl>, wk39 <dbl>, wk40 <dbl>, wk41 <dbl>, wk42 <dbl>,\n#   wk43 <dbl>, wk44 <dbl>, wk45 <dbl>, wk46 <dbl>, wk47 <dbl>, wk48 <dbl>, …\n\n\n\nbillboard |> as_tibble() |> \n  pivot_longer(col = starts_with(\"wk\"),\n               names_to = \"week\", names_prefix = \"wk\",\n               values_to = \"rank\", values_drop_na = TRUE)\n\n# A tibble: 5,307 × 5\n   artist  track                   date.entered week   rank\n   <chr>   <chr>                   <date>       <chr> <dbl>\n 1 2 Pac   Baby Don't Cry (Keep... 2000-02-26   1        87\n 2 2 Pac   Baby Don't Cry (Keep... 2000-02-26   2        82\n 3 2 Pac   Baby Don't Cry (Keep... 2000-02-26   3        72\n 4 2 Pac   Baby Don't Cry (Keep... 2000-02-26   4        77\n 5 2 Pac   Baby Don't Cry (Keep... 2000-02-26   5        87\n 6 2 Pac   Baby Don't Cry (Keep... 2000-02-26   6        94\n 7 2 Pac   Baby Don't Cry (Keep... 2000-02-26   7        99\n 8 2Ge+her The Hardest Part Of ... 2000-09-02   1        91\n 9 2Ge+her The Hardest Part Of ... 2000-09-02   2        87\n10 2Ge+her The Hardest Part Of ... 2000-09-02   3        92\n# … with 5,297 more rows\n\n\n\nwho |> as_tibble()\n\n# A tibble: 7,240 × 60\n   country     iso2  iso3   year new_s…¹ new_s…² new_s…³ new_s…⁴ new_s…⁵ new_s…⁶\n   <chr>       <chr> <chr> <int>   <int>   <int>   <int>   <int>   <int>   <int>\n 1 Afghanistan AF    AFG    1980      NA      NA      NA      NA      NA      NA\n 2 Afghanistan AF    AFG    1981      NA      NA      NA      NA      NA      NA\n 3 Afghanistan AF    AFG    1982      NA      NA      NA      NA      NA      NA\n 4 Afghanistan AF    AFG    1983      NA      NA      NA      NA      NA      NA\n 5 Afghanistan AF    AFG    1984      NA      NA      NA      NA      NA      NA\n 6 Afghanistan AF    AFG    1985      NA      NA      NA      NA      NA      NA\n 7 Afghanistan AF    AFG    1986      NA      NA      NA      NA      NA      NA\n 8 Afghanistan AF    AFG    1987      NA      NA      NA      NA      NA      NA\n 9 Afghanistan AF    AFG    1988      NA      NA      NA      NA      NA      NA\n10 Afghanistan AF    AFG    1989      NA      NA      NA      NA      NA      NA\n# … with 7,230 more rows, 50 more variables: new_sp_m65 <int>,\n#   new_sp_f014 <int>, new_sp_f1524 <int>, new_sp_f2534 <int>,\n#   new_sp_f3544 <int>, new_sp_f4554 <int>, new_sp_f5564 <int>,\n#   new_sp_f65 <int>, new_sn_m014 <int>, new_sn_m1524 <int>,\n#   new_sn_m2534 <int>, new_sn_m3544 <int>, new_sn_m4554 <int>,\n#   new_sn_m5564 <int>, new_sn_m65 <int>, new_sn_f014 <int>,\n#   new_sn_f1524 <int>, new_sn_f2534 <int>, new_sn_f3544 <int>, …\n\n\n\nwho %>% as_tibble() |>\n  pivot_longer(cols = new_sp_m014:newrel_f65,\n               names_to = c(\"diagnosis\", \"gender\", \"age\"),\n               names_pattern = \"new_?(.*)_(.)(.*)\",\n               values_to = \"count\", values_drop_na = TRUE)\n\n# A tibble: 76,046 × 8\n   country     iso2  iso3   year diagnosis gender age   count\n   <chr>       <chr> <chr> <int> <chr>     <chr>  <chr> <int>\n 1 Afghanistan AF    AFG    1997 sp        m      014       0\n 2 Afghanistan AF    AFG    1997 sp        m      1524     10\n 3 Afghanistan AF    AFG    1997 sp        m      2534      6\n 4 Afghanistan AF    AFG    1997 sp        m      3544      3\n 5 Afghanistan AF    AFG    1997 sp        m      4554      5\n 6 Afghanistan AF    AFG    1997 sp        m      5564      2\n 7 Afghanistan AF    AFG    1997 sp        m      65        0\n 8 Afghanistan AF    AFG    1997 sp        f      014       5\n 9 Afghanistan AF    AFG    1997 sp        f      1524     38\n10 Afghanistan AF    AFG    1997 sp        f      2534     36\n# … with 76,036 more rows\n\n\n\nanscombe |> as_tibble()\n\n# A tibble: 11 × 8\n      x1    x2    x3    x4    y1    y2    y3    y4\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1    10    10    10     8  8.04  9.14  7.46  6.58\n 2     8     8     8     8  6.95  8.14  6.77  5.76\n 3    13    13    13     8  7.58  8.74 12.7   7.71\n 4     9     9     9     8  8.81  8.77  7.11  8.84\n 5    11    11    11     8  8.33  9.26  7.81  8.47\n 6    14    14    14     8  9.96  8.1   8.84  7.04\n 7     6     6     6     8  7.24  6.13  6.08  5.25\n 8     4     4     4    19  4.26  3.1   5.39 12.5 \n 9    12    12    12     8 10.8   9.13  8.15  5.56\n10     7     7     7     8  4.82  7.26  6.42  7.91\n11     5     5     5     8  5.68  4.74  5.73  6.89\n\n\n\nanscombe %>% as_tibble() |> \n pivot_longer(everything(), names_to = c(\".value\", \"set\"), names_pattern = \"(.)(.)\"\n )\n\n# A tibble: 44 × 3\n   set       x     y\n   <chr> <dbl> <dbl>\n 1 1        10  8.04\n 2 2        10  9.14\n 3 3        10  7.46\n 4 4         8  6.58\n 5 1         8  6.95\n 6 2         8  8.14\n 7 3         8  6.77\n 8 4         8  5.76\n 9 1        13  7.58\n10 2        13  8.74\n# … with 34 more rows"
  },
  {
    "objectID": "part04.html#pivot_wider-の使い方",
    "href": "part04.html#pivot_wider-の使い方",
    "title": "データの操作",
    "section": "pivot_wider() の使い方",
    "text": "pivot_wider() の使い方\n\nfish_encounters\n\n# A tibble: 114 × 3\n   fish  station  seen\n   <fct> <fct>   <int>\n 1 4842  Release     1\n 2 4842  I80_1       1\n 3 4842  Lisbon      1\n 4 4842  Rstr        1\n 5 4842  Base_TD     1\n 6 4842  BCE         1\n 7 4842  BCW         1\n 8 4842  BCE2        1\n 9 4842  BCW2        1\n10 4842  MAE         1\n# … with 104 more rows\n\n\n\nfish_encounters |> as_tibble() |> \n  pivot_wider(names_from = station, values_from = seen)\n\n# A tibble: 19 × 12\n   fish  Release I80_1 Lisbon  Rstr Base_TD   BCE   BCW  BCE2  BCW2   MAE   MAW\n   <fct>   <int> <int>  <int> <int>   <int> <int> <int> <int> <int> <int> <int>\n 1 4842        1     1      1     1       1     1     1     1     1     1     1\n 2 4843        1     1      1     1       1     1     1     1     1     1     1\n 3 4844        1     1      1     1       1     1     1     1     1     1     1\n 4 4845        1     1      1     1       1    NA    NA    NA    NA    NA    NA\n 5 4847        1     1      1    NA      NA    NA    NA    NA    NA    NA    NA\n 6 4848        1     1      1     1      NA    NA    NA    NA    NA    NA    NA\n 7 4849        1     1     NA    NA      NA    NA    NA    NA    NA    NA    NA\n 8 4850        1     1     NA     1       1     1     1    NA    NA    NA    NA\n 9 4851        1     1     NA    NA      NA    NA    NA    NA    NA    NA    NA\n10 4854        1     1     NA    NA      NA    NA    NA    NA    NA    NA    NA\n11 4855        1     1      1     1       1    NA    NA    NA    NA    NA    NA\n12 4857        1     1      1     1       1     1     1     1     1    NA    NA\n13 4858        1     1      1     1       1     1     1     1     1     1     1\n14 4859        1     1      1     1       1    NA    NA    NA    NA    NA    NA\n15 4861        1     1      1     1       1     1     1     1     1     1     1\n16 4862        1     1      1     1       1     1     1     1     1    NA    NA\n17 4863        1     1     NA    NA      NA    NA    NA    NA    NA    NA    NA\n18 4864        1     1     NA    NA      NA    NA    NA    NA    NA    NA    NA\n19 4865        1     1      1    NA      NA    NA    NA    NA    NA    NA    NA\n\n\n\nfish_encounters\n\n# A tibble: 114 × 3\n   fish  station  seen\n   <fct> <fct>   <int>\n 1 4842  Release     1\n 2 4842  I80_1       1\n 3 4842  Lisbon      1\n 4 4842  Rstr        1\n 5 4842  Base_TD     1\n 6 4842  BCE         1\n 7 4842  BCW         1\n 8 4842  BCE2        1\n 9 4842  BCW2        1\n10 4842  MAE         1\n# … with 104 more rows\n\n\n\n# 存在しない組み合わせの要素を埋める\nfish_encounters |>  as_tibble() |> \n  pivot_wider(names_from = station, values_from = seen, values_fill = 0)\n\n# A tibble: 19 × 12\n   fish  Release I80_1 Lisbon  Rstr Base_TD   BCE   BCW  BCE2  BCW2   MAE   MAW\n   <fct>   <int> <int>  <int> <int>   <int> <int> <int> <int> <int> <int> <int>\n 1 4842        1     1      1     1       1     1     1     1     1     1     1\n 2 4843        1     1      1     1       1     1     1     1     1     1     1\n 3 4844        1     1      1     1       1     1     1     1     1     1     1\n 4 4845        1     1      1     1       1     0     0     0     0     0     0\n 5 4847        1     1      1     0       0     0     0     0     0     0     0\n 6 4848        1     1      1     1       0     0     0     0     0     0     0\n 7 4849        1     1      0     0       0     0     0     0     0     0     0\n 8 4850        1     1      0     1       1     1     1     0     0     0     0\n 9 4851        1     1      0     0       0     0     0     0     0     0     0\n10 4854        1     1      0     0       0     0     0     0     0     0     0\n11 4855        1     1      1     1       1     0     0     0     0     0     0\n12 4857        1     1      1     1       1     1     1     1     1     0     0\n13 4858        1     1      1     1       1     1     1     1     1     1     1\n14 4859        1     1      1     1       1     0     0     0     0     0     0\n15 4861        1     1      1     1       1     1     1     1     1     1     1\n16 4862        1     1      1     1       1     1     1     1     1     0     0\n17 4863        1     1      0     0       0     0     0     0     0     0     0\n18 4864        1     1      0     0       0     0     0     0     0     0     0\n19 4865        1     1      1     0       0     0     0     0     0     0     0\n\n\n\nus_rent_income |> as_tibble()\n\n# A tibble: 104 × 5\n   GEOID NAME       variable estimate   moe\n   <chr> <chr>      <chr>       <dbl> <dbl>\n 1 01    Alabama    income      24476   136\n 2 01    Alabama    rent          747     3\n 3 02    Alaska     income      32940   508\n 4 02    Alaska     rent         1200    13\n 5 04    Arizona    income      27517   148\n 6 04    Arizona    rent          972     4\n 7 05    Arkansas   income      23789   165\n 8 05    Arkansas   rent          709     5\n 9 06    California income      29454   109\n10 06    California rent         1358     3\n# … with 94 more rows\n\n\n\nus_rent_income |> as_tibble() |> \n  pivot_wider(names_from = variable, values_from = c(estimate, moe))\n\n# A tibble: 52 × 6\n   GEOID NAME                 estimate_income estimate_rent moe_income moe_rent\n   <chr> <chr>                          <dbl>         <dbl>      <dbl>    <dbl>\n 1 01    Alabama                        24476           747        136        3\n 2 02    Alaska                         32940          1200        508       13\n 3 04    Arizona                        27517           972        148        4\n 4 05    Arkansas                       23789           709        165        5\n 5 06    California                     29454          1358        109        3\n 6 08    Colorado                       32401          1125        109        5\n 7 09    Connecticut                    35326          1123        195        5\n 8 10    Delaware                       31560          1076        247       10\n 9 11    District of Columbia           43198          1424        681       17\n10 12    Florida                        25952          1077         70        3\n# … with 42 more rows\n\n# us_rent_income  |> as_tibble() |> \n#   pivot_wider(names_from = variable,\n#               names_sep = \".\",\n#               values_from = c(estimate, moe))\n\n# us_rent_income  |> as_tibble() |> \n#   pivot_wider(names_from = variable,\n#               names_glue = \"{variable}_{.value}\",\n#               values_from = c(estimate, moe))\n\n\nwarpbreaks |> as_tibble()\n\n# A tibble: 54 × 3\n   breaks wool  tension\n    <dbl> <fct> <fct>  \n 1     26 A     L      \n 2     30 A     L      \n 3     54 A     L      \n 4     25 A     L      \n 5     70 A     L      \n 6     52 A     L      \n 7     51 A     L      \n 8     26 A     L      \n 9     67 A     L      \n10     18 A     M      \n# … with 44 more rows\n\n\n\nwarpbreaks |> as_tibble() |> \n  pivot_wider(names_from = wool,\n              values_from = breaks)\n\nWarning: Values from `breaks` are not uniquely identified; output will contain list-cols.\n* Use `values_fn = list` to suppress this warning.\n* Use `values_fn = {summary_fun}` to summarise duplicates.\n* Use the following dplyr code to identify duplicates.\n  {data} %>%\n    dplyr::group_by(tension, wool) %>%\n    dplyr::summarise(n = dplyr::n(), .groups = \"drop\") %>%\n    dplyr::filter(n > 1L)\n\n\n# A tibble: 3 × 3\n  tension A         B        \n  <fct>   <list>    <list>   \n1 L       <dbl [9]> <dbl [9]>\n2 M       <dbl [9]> <dbl [9]>\n3 H       <dbl [9]> <dbl [9]>\n\n\n\nwarpbreaks |> as_tibble()\n\n# A tibble: 54 × 3\n   breaks wool  tension\n    <dbl> <fct> <fct>  \n 1     26 A     L      \n 2     30 A     L      \n 3     54 A     L      \n 4     25 A     L      \n 5     70 A     L      \n 6     52 A     L      \n 7     51 A     L      \n 8     26 A     L      \n 9     67 A     L      \n10     18 A     M      \n# … with 44 more rows\n\n\n\nwarpbreaks |> as_tibble() |> \n  pivot_wider(names_from = wool,\n              values_from = breaks,\n              values_fn = mean)\n\n# A tibble: 3 × 3\n  tension     A     B\n  <fct>   <dbl> <dbl>\n1 L        44.6  28.2\n2 M        24    28.8\n3 H        24.6  18.8"
  },
  {
    "objectID": "part04.html#不都合なデータ構造",
    "href": "part04.html#不都合なデータ構造",
    "title": "データの操作",
    "section": "不都合なデータ構造",
    "text": "不都合なデータ構造\n\nfname = \"photosynthesis1_low.csv\"\ndset1_low = read_csv(fname)\n\n\ndset1_low\n\n# A tibble: 35 × 12\n   sample   min   `0`   `5`  `10`  `15`  `20`  `25`  `30`  `35`  `40`  `45`\n    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1      1     0  9.99 10.1  10.0  10.0  10.1   9.81 10.0  10.1  10.0  10.2 \n 2      2     0  9.97  9.99 10.1  10.0   9.99  9.94  9.91  9.95  9.79  9.97\n 3      3     0 10.0  10.1  10.1   9.97 10.1  10.0  10.0  10.0   9.88 10.0 \n 4      4     0 10.0   9.87 10.1   9.87 10.0   9.95 10.2  10.0  10.1  10.1 \n 5      5     0  9.92  9.97  9.88  9.86 10.0   9.91 10.0   9.94 10.0   9.95\n 6      1     5  9.67 10.0   9.93 10.5  10.4  10.7  10.8  10.6  10.7  10.6 \n 7      2     5  9.40  9.87 10.1  10.0  10.4  10.6  10.7  10.6  10.6  10.7 \n 8      3     5  9.37  9.84 10.2  10.3  10.5  10.6  10.5  10.6  10.7  10.7 \n 9      4     5  9.52  9.71  9.92 10.1  10.5  10.5  10.7  10.5  10.7  10.8 \n10      5     5  9.65  9.83 10.1  10.4  10.4  10.6  10.5  10.5  10.7  10.7 \n# … with 25 more rows\n\n\nsample と min の列はサンプル番号と時間 (minutes) の変数です。 それぞれに、サンプル番号と時間の値が入っています。 0 から 45 の列には溶存酸素濃度の値が入っています。 この時の変数名は光条件ですね。"
  },
  {
    "objectID": "part04.html#ワイドからロングへ変換",
    "href": "part04.html#ワイドからロングへ変換",
    "title": "データの操作",
    "section": "ワイドからロングへ変換",
    "text": "ワイドからロングへ変換\n\ndset1_low |> \n  pivot_longer(cols = matches(\"[0-9]+\"),　names_to = \"light\",\n               names_transform  = list(light = as.numeric))\n\n# A tibble: 350 × 4\n   sample   min light value\n    <dbl> <dbl> <dbl> <dbl>\n 1      1     0     0  9.99\n 2      1     0     5 10.1 \n 3      1     0    10 10.0 \n 4      1     0    15 10.0 \n 5      1     0    20 10.1 \n 6      1     0    25  9.81\n 7      1     0    30 10.0 \n 8      1     0    35 10.1 \n 9      1     0    40 10.0 \n10      1     0    45 10.2 \n# … with 340 more rows"
  },
  {
    "objectID": "part04.html#残りのデータの読み込み",
    "href": "part04.html#残りのデータの読み込み",
    "title": "データの操作",
    "section": "残りのデータの読み込み",
    "text": "残りのデータの読み込み\n\ndset1_high = read_csv(\"photosynthesis1_high.csv\")\ndset2_low  = read_csv(\"photosynthesis2_low.csv\")\ndset2_high = read_csv(\"photosynthesis2_high.csv\")"
  },
  {
    "objectID": "part04.html#ピボットしてから結合",
    "href": "part04.html#ピボットしてから結合",
    "title": "データの操作",
    "section": "ピボットしてから結合",
    "text": "ピボットしてから結合\n\ndset1_low  = dset1_low  |> pivot_longer(cols = matches(\"[0-9]+\"), names_to = \"light\", names_transform = list(light = as.numeric))\ndset1_high = dset1_high |> pivot_longer(cols = matches(\"[0-9]+\"), names_to = \"light\", names_transform = list(light = as.numeric))\ndset2_low  = dset2_low  |> pivot_longer(cols = matches(\"[0-9]+\"), names_to = \"light\", names_transform = list(light = as.numeric))\ndset2_high = dset2_high |> pivot_longer(cols = matches(\"[0-9]+\"), names_to = \"light\", names_transform = list(light = as.numeric))\nalldata = bind_rows(dset1_low, dset2_low, dset1_high, dset2_high)\nalldata\n\n# A tibble: 910 × 4\n   sample   min light value\n    <dbl> <dbl> <dbl> <dbl>\n 1      1     0     0  9.99\n 2      1     0     5 10.1 \n 3      1     0    10 10.0 \n 4      1     0    15 10.0 \n 5      1     0    20 10.1 \n 6      1     0    25  9.81\n 7      1     0    30 10.0 \n 8      1     0    35 10.1 \n 9      1     0    40 10.0 \n10      1     0    45 10.2 \n# … with 900 more rows"
  },
  {
    "objectID": "part04.html#結合してからピボット",
    "href": "part04.html#結合してからピボット",
    "title": "データの操作",
    "section": "結合してからピボット",
    "text": "結合してからピボット\n\n\nRows: 35 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (12): sample, min, 0, 5, 10, 15, 20, 25, 30, 35, 40, 45\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 35 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): sample, min, 50, 75, 100\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 35 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (12): sample, min, 0, 5, 10, 15, 20, 25, 30, 35, 40, 45\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 35 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): sample, min, 50, 75, 100\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndset1 = full_join(dset1_low, dset1_high, by = c(\"sample\", \"min\"))\ndset2 = full_join(dset2_low, dset2_high, by = c(\"sample\", \"min\"))\nalldata = bind_rows(dset1, dset2)\nalldata = alldata |> \n  pivot_longer(cols = matches(\"[0-9]+\"), names_to = \"light\", \n               names_transform = list(light = as.numeric))"
  },
  {
    "objectID": "part05.html",
    "href": "part05.html",
    "title": "ggplot の図",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.4.0 \n✔ readr   2.1.2      ✔ forcats 0.5.1 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(readxl)\nlibrary(ggpubr)\nlibrary(showtext)\n\nLoading required package: sysfonts\nLoading required package: showtextdb\n\nlibrary(patchwork)"
  },
  {
    "objectID": "part05.html#図の詳細設定",
    "href": "part05.html#図の詳細設定",
    "title": "ggplot の図",
    "section": "図の詳細設定",
    "text": "図の詳細設定\nshowtext パッケージを使って、システムフォントを使えるようにします。 使用可能なフォントは次のように調べます。\n\n#! eval: false\nfont_files() |> as_tibble()\n\n# A tibble: 2,823 × 6\n   path                                 file        family face  version ps_name\n   <chr>                                <chr>       <chr>  <chr> <chr>   <chr>  \n 1 /usr/share/fonts/Fira_Code           FiraCode-V… Fira … Regu… Versio… FiraCo…\n 2 /usr/share/fonts/Fira_Code/static    FiraCode-B… Fira … Bold  Versio… FiraCo…\n 3 /usr/share/fonts/Fira_Code/static    FiraCode-L… Fira … Regu… Versio… FiraCo…\n 4 /usr/share/fonts/Fira_Code/static    FiraCode-M… Fira … Regu… Versio… FiraCo…\n 5 /usr/share/fonts/Fira_Code/static    FiraCode-R… Fira … Regu… Versio… FiraCo…\n 6 /usr/share/fonts/Fira_Code/static    FiraCode-S… Fira … Regu… Versio… FiraCo…\n 7 /usr/share/fonts/Fira_Mono           FiraMono-B… Fira … Bold  Versio… FiraMo…\n 8 /usr/share/fonts/Fira_Mono           FiraMono-M… Fira … Regu… Versio… FiraMo…\n 9 /usr/share/fonts/Fira_Mono           FiraMono-R… Fira … Regu… Versio… FiraMo…\n10 /usr/share/fonts/Fira_Sans_Condensed FiraSansCo… Fira … Regu… Versio… FiraSa…\n# … with 2,813 more rows\n\n\nGoogle の Noto シリーズのフォントを使いたいので、filter() にかけます。\n\nfont_files() |> as_tibble() |> \n  filter(str_detect(ps_name, \"NotoSansCJK|NotoSansSymbol\")) |> \n  select(file, face, ps_name) \n\n# A tibble: 27 × 3\n   file                          face    ps_name                  \n   <chr>                         <chr>   <chr>                    \n 1 NotoSansCJKjp-Black.otf       Regular NotoSansCJKjp-Black      \n 2 NotoSansCJKjp-Bold.otf        Regular NotoSansCJKjp-Bold       \n 3 NotoSansCJKjp-DemiLight.otf   Regular NotoSansCJKjp-DemiLight  \n 4 NotoSansCJKjp-Light.otf       Regular NotoSansCJKjp-Light      \n 5 NotoSansCJKjp-Medium.otf      Regular NotoSansCJKjp-Medium     \n 6 NotoSansCJKjp-Regular.otf     Regular NotoSansCJKjp-Regular    \n 7 NotoSansCJKjp-Thin.otf        Regular NotoSansCJKjp-Thin       \n 8 NotoSansSymbols-Black.ttf     Regular NotoSansSymbols-Black    \n 9 NotoSansSymbols-Bold.ttf      Bold    NotoSansSymbols-Bold     \n10 NotoSansSymbols-ExtraBold.ttf Regular NotoSansSymbols-ExtraBold\n# … with 17 more rows\n\n\nフォントファイルのファイル名は file 変数にあります。 その変数を使って、font_add() 関数で用意します。\n\nfont_add(family = \"notosans\", \n         regular = \"NotoSansCJKjp-Regular.otf\",\n         bold = \"NotoSansCJKjp-Black.otf\",\n         symbol = \"NotoSansSymbols-Regular.ttf\")\n\n図のデフォルトテーマをここで設定します。 base_size はフォントの大きさ。 base_family は font_add() で定義した family です。\n\ntheme_gray(base_size = 10, base_family = \"notosans\") |> theme_set()\nshowtext_auto()\n\n論文用のテーマは ggpubr パッケージの theme_pubr() をおすすめします。\n\ntheme_pubr(base_size = 10, base_family = \"notosans\") |> theme_set()\nshowtext_auto()"
  },
  {
    "objectID": "part05.html#ggplot2-について",
    "href": "part05.html#ggplot2-について",
    "title": "ggplot の図",
    "section": "ggplot2 について",
    "text": "ggplot2 について\n\nggplot2 の関数は + でつなげる\nggplot() はベースレイヤー\ngeom_*() はプロットレイヤー\nscales_*() でエステティク (aesthetics) を調整\ntheme() や theme_() で書式を調整\nfacet_wrap() や facet_grid() は多変量データのプロットのパネル分け"
  },
  {
    "objectID": "part05.html#aesthetics-エステティクとは",
    "href": "part05.html#aesthetics-エステティクとは",
    "title": "ggplot の図",
    "section": "Aesthetics （エステティク）とは",
    "text": "Aesthetics （エステティク）とは\n\n色・透明度\n\ncolor：点と線の色\nfill：面の色\nalpha：透明度（0 – 1 の値）\n\n\n\n大きさ・形状\n\nsize：点と文字の大きさ、線の太さ\nshape：点の形\nlinetype：線の種類\n\n\n\nグループ化\n\ngroup：点や線のグループ化\n\n\n\n座標、始点・終点\n\nx, y\nxmin, ymin\nxend, yend"
  },
  {
    "objectID": "part05.html#geom-の種類",
    "href": "part05.html#geom-の種類",
    "title": "ggplot の図",
    "section": "geom の種類",
    "text": "geom の種類\n散布図\n\ngeom_point()\ngeom_jitter()\n\n折れ線グラフ\n\ngeom_path()\ngeom_line()\ngeom_step()\n\n面グラフ * geom_ribbon() * geom_area() * geom_polygon()\nヒートマップ・コンター図 * geom_tile() * geom_raster() * geom_rect() * geom_contour()\nエラーバー * geom_error() * geom_linerange() * geom_pointrange() * geom_crossbar()"
  },
  {
    "objectID": "part05.html#geom-の種類-1",
    "href": "part05.html#geom-の種類-1",
    "title": "ggplot の図",
    "section": "geom の種類",
    "text": "geom の種類\n曲線など\n\ngeom_smooth()\ngeom_curve()\ngeom_segment()\ngeom_abline()\ngeom_hline()\ngeom_vline()\n\n文字列\n\ngeom_text()\ngeom_label()\n\nヒストグラム・密度曲線 * geom_histogram() * geom_freqpoly() * geom_density() * geom_bin2d() * geom_hex() * geom_dotplot()\n棒グラフ・箱ひげ図 * geom_bar() * geom_col() * geom_boxplot() * geom_violin()"
  },
  {
    "objectID": "part05.html#ggplot2-の付属パッケージ",
    "href": "part05.html#ggplot2-の付属パッケージ",
    "title": "ggplot の図",
    "section": "ggplot2 の付属パッケージ",
    "text": "ggplot2 の付属パッケージ\n研究室が使っているパッケージ\n\nggpubr: theme_pubr(), ggarrange()\nggrepel: geom_text_repel()\nlemon: facet_rep_grid(), facet_rep_wrap()\nshowtext: システムフォントの埋め込み\n\nggplot2 extensions"
  },
  {
    "objectID": "part05.html#データを読み込んだら可視化しよう",
    "href": "part05.html#データを読み込んだら可視化しよう",
    "title": "ggplot の図",
    "section": "データを読み込んだら、可視化しよう",
    "text": "データを読み込んだら、可視化しよう\n\nfilename = \"Table 2.xlsx\"\ncol_names = c(\"month\", \"temperature1\", \"sd1\", \"empty\",\"temperature2\", \"sd2\")\nexceldata = read_excel(filename, sheet = 1, skip = 2, col_name = col_names)\n\n\n\n\n\nggplot(exceldata) + geom_point(aes(x = month, y = temperature1))\n\n\n\n\n横軸の順序がおかしいですね。軸タイトルも変えたほうがいいですね。"
  },
  {
    "objectID": "part05.html#軸タイトルの関数",
    "href": "part05.html#軸タイトルの関数",
    "title": "ggplot の図",
    "section": "軸タイトルの関数",
    "text": "軸タイトルの関数\n軸タイトルや図のタイトルは labs() 関数でします。\n\nxlabel = \"Month\"\nylabel = \"'Temperature ('*degree*'C)'\" # plotmath expression see ?plotmath\nggplot(exceldata) + \n  geom_point(aes(x = month, y = temperature1)) + \n  labs(x = xlabel, \n       y = parse(text = ylabel),\n       title = \"Monthly mean water temperature\")"
  },
  {
    "objectID": "part05.html#論文用に変える",
    "href": "part05.html#論文用に変える",
    "title": "ggplot の図",
    "section": "論文用に変える",
    "text": "論文用に変える\n学術論文に記載する図の場合、図から余計なかざりを外します。 研究室では ggpubr の theme_pubr() 関数を使っています。\n\nxlabel = \"Month\"\nylabel = \"'Temperature ('*degree*'C)'\" # plotmath expression see ?plotmath\nggplot(exceldata) + \n  geom_point(aes(x = month, y = temperature1)) + \n  labs(x = parse(text = xlabel), \n       y = parse(text = ylabel))  +\n  theme_pubr(base_size = 10)"
  },
  {
    "objectID": "part05.html#月の順序をなおす",
    "href": "part05.html#月の順序をなおす",
    "title": "ggplot の図",
    "section": "月の順序をなおす",
    "text": "月の順序をなおす\nもう気づいたと思いますが、横軸の月の順序が間違っています。 factor() で、month 変数を整えます。\n\n# element_text() size is in points (pt)\n# 1 pt = 0.35 mm\nxlabel = \"Month\"\nylabel = \"'Temperature ('*degree*'C)'\" # plotmath expression see ?plotmath\n\nlevels = month.abb\nlevels = str_c(levels, ifelse(levels == \"May\", \"\", \".\"))\n\nexceldata |> \n  mutate(month = factor(month, levels = levels)) |> \n  ggplot() + \n  geom_point(aes(x = month, y = temperature1)) + \n  labs(x = parse(text = xlabel), \n       y = parse(text = ylabel))  +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10))\n\n\n\n\n\n\nLinking to ImageMagick 6.9.11.60\nEnabled features: fontconfig, freetype, fftw, heic, lcms, pango, webp, x11\nDisabled features: cairo, ghostscript, raw, rsvg\n\n\nUsing 32 threads"
  },
  {
    "objectID": "part05.html#図を保存する",
    "href": "part05.html#図を保存する",
    "title": "ggplot の図",
    "section": "図を保存する",
    "text": "図を保存する\n図は PDF と PNG 形式で保存しましょう。\nPDFファイル ggsave() は最後の表示した図を書き出しします。 width と height を指定したら必ず単位も指定しましょう (units = \"mm\")。 PDFファイルにシステムフォントを埋め込むなら、device = cairo_pdfも渡しましょう。\n\nwh = list(width = 80, height = 80) # 図の縦横幅\npdffile = \"temperature_plot.pdf\"\nggsave(pdffile, width = wh$width, height = wh$height, units = \"mm\", device = cairo_pdf)\n\nPNGファイル 直接PNGファイルに保存する場合は、画像の解像度 (dpi = 300) も必要です。\n\npngfile = \"temperature_plot.png\"\nggsave(pngfile, width = wh$width, height = wh$height, units = \"mm\", dpi = 300)"
  },
  {
    "objectID": "part05.html#保存の結果",
    "href": "part05.html#保存の結果",
    "title": "ggplot の図",
    "section": "保存の結果",
    "text": "保存の結果\n\n\n\n\n\n\nwh = list(width = 80, height = 80) は同じだが、図は似ていません。\nモニターでみたとき、PDF の解像度は 96 です。つまり、dpi = 300 のPNGファイルはPDFの約 3 倍の大きさです。"
  },
  {
    "objectID": "part05.html#図のフォントを拡大してpngファイルを修正する",
    "href": "part05.html#図のフォントを拡大してpngファイルを修正する",
    "title": "ggplot の図",
    "section": "図のフォントを拡大して、PNGファイルを修正する",
    "text": "図のフォントを拡大して、PNGファイルを修正する\n\nDPI = 300\nscale = DPI / 96\nexceldata |> \n  mutate(month = factor(month, levels = levels)) |> \n  ggplot() + \n  geom_point(aes(x = month, y = temperature1)) + \n  labs(x = parse(text = xlabel), \n       y = parse(text = ylabel))  +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10 * scale))\n\npngfile = \"temperature_plot.png\"\nwh = list(width = 80, height = 80)\nggsave(pngfile, width = wh$width, height = wh$height, units = \"mm\", dpi = DPI)"
  },
  {
    "objectID": "part05.html#研究室のワークフロー",
    "href": "part05.html#研究室のワークフロー",
    "title": "ggplot の図",
    "section": "研究室のワークフロー",
    "text": "研究室のワークフロー\nPNGファイルのDPIをいじるのが面倒なので、PDFをPNGに変換するのが楽です。 月の頭文字をチックラベルにします。さらに、lemon パッケージの geom_pointline()を使ってみました。\n\nlibrary(lemon)\n\n\nAttaching package: 'lemon'\n\n\nThe following object is masked from 'package:purrr':\n\n    %||%\n\n\nThe following objects are masked from 'package:ggplot2':\n\n    CoordCartesian, element_render\n\nxlabel = \"Month\"\nylabel = \"'Temperature'~(degree*C)\" # plotmath expression see ?plotmath\nlevels = month.abb\nlevels = str_c(levels, ifelse(levels == \"May\", \"\", \".\"))\nlabels = str_sub(month.abb, 1, 1)\n# 図の結果は plot1 にいれます。\nplot1 =   exceldata |> mutate(month = factor(month, levels = levels)) |> \n  ggplot() + \n  geom_point(aes(x = month, y = temperature1)) +\n  scale_x_discrete(name = xlabel, labels = labels) +\n  scale_y_continuous(name = parse(text = ylabel), breaks = seq(21, 29, by = 1)) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10))\n\nまず、PDFファイルを保存します。システムフォントをPDFファイルに入れるためには device = cairo_pdf を渡します。\n\n\n\n\nwh = list(width = 80, height = 80) # 図の縦横幅\npdffile = \"temperature_plot.pdf\"\nggsave(pdffile, width = wh$width, height = wh$height, units = \"mm\", device = cairo_pdf)\n\nImageMagick のAPIを使って、PDFをPNGに変換します。 この方法だと、DPIのややこしい変換は不要です。\nつぎに magick パッケージを読み込みます。\n\nlibrary(magick) # imagemagick パッケージ\n\nつづいて、PDF ファイルを 600 DPI で読み込む。\n\nimg = image_read_pdf(pdffile, density = 600)\n\nPDFファイルをPNGファイルに書き出す。\n\nimg |> image_write(pngfile)"
  },
  {
    "objectID": "part05.html#保存の結果-1",
    "href": "part05.html#保存の結果-1",
    "title": "ggplot の図",
    "section": "保存の結果",
    "text": "保存の結果\n\n\n\n\n\nこのとき、フォントサイズは 10 pt にしました：theme(text = element_text(size = 10))。"
  },
  {
    "objectID": "part05.html#データを追加してプロット",
    "href": "part05.html#データを追加してプロット",
    "title": "ggplot の図",
    "section": "データを追加してプロット",
    "text": "データを追加してプロット\n\nxlabel = \"Month\"\nylabel = \"'Temperature'~(degree*C)\" # plotmath expression see ?plotmath\nlevels = month.abb\nlevels = str_c(levels, ifelse(levels == \"May\", \"\", \".\"))\nlabels = str_sub(month.abb, 1, 1)\nexceldata |> mutate(month = factor(month, levels = levels)) |> \n  ggplot() + \n  geom_pointline(aes(x = month, y = temperature1, color = \"Group 1\", shape = \"Group 1\", group = 1)) +\n  geom_pointline(aes(x = month, y = temperature2, color = \"Group 2\", shape = \"Group 2\", group = 1)) +\n  scale_x_discrete(name = xlabel, labels = labels) +\n  scale_y_continuous(name = parse(text = ylabel), breaks = seq(15, 30, by = 5), limits = c(15, 30)) +\n  scale_color_viridis_d(\"\", option = \"turbo\", begin = 0, end = 0.5) +\n  scale_shape_discrete(\"\") +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10),\n        legend.position = c(1, 0),\n        legend.justification = c(1, 0),\n        legend.background = element_blank(),\n        legend.title = element_blank())\n\n\n\n\n]"
  },
  {
    "objectID": "part05.html#複数パネルのプロット",
    "href": "part05.html#複数パネルのプロット",
    "title": "ggplot の図",
    "section": "複数パネルのプロット",
    "text": "複数パネルのプロット\n\nxlabel = \"Petal width (cm)\"\nylabel = \"Sepal width (cm)\"\niris |> group_nest(Species) |> \n  mutate(L = c(\"A\", \"B\", \"C\")) |> \n  mutate(Species = sprintf(\"italic('I.')~italic('%s')~'(%s)'\",  Species, L)) |> \n  unnest(data) |> \n  ggplot() + \n  geom_point(aes(x = Petal.Width, y = Sepal.Width, color = Species)) +\n  geom_text(aes(x = 3, y = 5, label = Species), parse = TRUE, vjust = 1, hjust = 1,\n            family = \"notosans\", size =3,  check_overlap = TRUE) +\n  scale_x_continuous(name = xlabel, breaks = seq(0, 3), limits = c(0, 3)) +\n  scale_y_continuous(name = ylabel, breaks = seq(0, 5), limits = c(0, 5)) +\n  scale_color_viridis_d(\"\", option = \"turbo\", \n                      begin = 0, end = 0.5, labels = scales::parse_format()) +\n  guides(color = \"none\") +\n  facet_rep_grid(cols = vars(Species)) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10),\n        strip.background = element_blank(),\n        strip.text = element_blank())"
  },
  {
    "objectID": "part05.html#複数プロットの結合",
    "href": "part05.html#複数プロットの結合",
    "title": "ggplot の図",
    "section": "複数プロットの結合",
    "text": "複数プロットの結合\n\nxlabel1 = \"Petal width (cm)\"\nylabel1 = \"Sepal width (cm)\"\nxlabel2 = \"Petal length (cm)\"\nylabel2 = \"Sepal length (cm)\"\n\niris2 = iris |> \n    mutate(Species = sprintf(\"italic('I.')~italic('%s')\",  Species))\n\n\nplot1 = \n  ggplot(iris2) + \n  geom_point(aes(x = Petal.Width, y = Sepal.Width, color = Species)) +\n  scale_x_continuous(name = xlabel1, breaks = seq(0, 8), limits = c(0, 8)) +\n  scale_y_continuous(name = ylabel1, breaks = seq(0, 8), limits = c(0, 8)) +\n  scale_color_viridis_d(\"\", option = \"turbo\", \n                      begin = 0, end = 0.5, labels = scales::parse_format()) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10))\n\n\nplot2 = \n  ggplot(iris2) + \n  geom_point(aes(x = Petal.Length, y = Sepal.Length, color = Species)) +\n  scale_x_continuous(name = xlabel2, breaks = seq(0, 8), limits = c(0, 8)) +\n  scale_y_continuous(name = ylabel2, breaks = seq(0, 8), limits = c(0, 8)) +\n  scale_color_viridis_d(\"\", option = \"turbo\", \n                      begin = 0, end = 0.5, labels = scales::parse_format()) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10))"
  },
  {
    "objectID": "part05.html#複数プロットの結合の結果",
    "href": "part05.html#複数プロットの結合の結果",
    "title": "ggplot の図",
    "section": "複数プロットの結合の結果",
    "text": "複数プロットの結合の結果\n\nplot1 + plot2 + plot_layout(ncol = 2, \n                            nrow = 1, \n                            guides = \"collect\")"
  },
  {
    "objectID": "part05.html#線と点説明変数は離散型変数の場合",
    "href": "part05.html#線と点説明変数は離散型変数の場合",
    "title": "ggplot の図",
    "section": "線と点（説明変数は離散型変数の場合）",
    "text": "線と点（説明変数は離散型変数の場合）\n\nylabel = \"Petal length (cm)\"\niris2 |> group_by(Species) |> \n  summarise(PL = mean(Petal.Length),\n            sd = sd(Petal.Length)) |> \n  mutate(lower = PL - sd,\n         upper = PL + sd) |> \nggplot() + \n  geom_line(aes(x = Species, y = PL, group = 1)) +\n  geom_point(aes(x = Species, y = PL), size = 2, color = \"white\") +\n  geom_point(aes(x = Species, y = PL), size = 1) +\n  geom_errorbar(aes(x = Species, ymin = lower, ymax = upper),\n                width = 0.0) +  \n  scale_x_discrete(name = \"Species\", labels = scales::parse_format()) +\n  scale_y_continuous(name = ylabel, breaks = seq(0, 8), limits = c(0, 8)) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10))"
  },
  {
    "objectID": "part05.html#ボーグラフ",
    "href": "part05.html#ボーグラフ",
    "title": "ggplot の図",
    "section": "ボーグラフ",
    "text": "ボーグラフ\n\nylabel = \"Petal length (cm)\"\niris2 |> group_by(Species) |> \n  summarise(PL = mean(Petal.Length),\n            sd = sd(Petal.Length)) |> \n  mutate(lower = PL - sd,\n         upper = PL + sd) |> \nggplot() + \n  geom_col(aes(x = Species, y = PL, fill = Species)) +\n  geom_errorbar(aes(x = Species, ymin = lower, ymax = upper),\n                width = 0.01) +  \n  scale_x_discrete(name = \"Species\", labels = scales::parse_format()) +\n  scale_y_continuous(name = ylabel, breaks = seq(0, 8), limits = c(0, 8)) +\n  scale_fill_viridis_d(\"\", option = \"turbo\", \n                      begin = 0, end = 0.5, labels = scales::parse_format()) +\n  guides(fill = \"none\") +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10))"
  },
  {
    "objectID": "part05.html#ボーグラフ横向き並び替える",
    "href": "part05.html#ボーグラフ横向き並び替える",
    "title": "ggplot の図",
    "section": "ボーグラフ（横向き・並び替える）",
    "text": "ボーグラフ（横向き・並び替える）\n\nylabel = \"Petal length (cm)\"\niris2 |> group_by(Species) |> \n  summarise(PL = mean(Petal.Length),\n            sd = sd(Petal.Length)) |> \n  mutate(lower = PL - sd,\n         upper = PL + sd) |> \n  ggplot(aes(x = fct_reorder(Species, PL, .desc = T))) + \n  geom_col(aes(y = PL, fill = Species)) +\n  geom_errorbar(aes(ymin = lower, ymax = upper),\n                width = 0.1) +  \n  scale_x_discrete(name = \"Species\", labels = scales::parse_format()) +\n  scale_y_continuous(name = ylabel, breaks = seq(0, 8), limits = c(0, 8)) +\n  scale_fill_viridis_d(\"\", option = \"turbo\", \n                      begin = 0, end = 0.5, labels = scales::parse_format()) +\n  guides(fill = \"none\") +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10))+ \n  coord_flip()"
  },
  {
    "objectID": "part05.html#ヒストグラム",
    "href": "part05.html#ヒストグラム",
    "title": "ggplot の図",
    "section": "ヒストグラム",
    "text": "ヒストグラム\n\nxlabel = \"Petal length (cm)\"\nylabel = \"Frequency\"\niris2 |> \n  ggplot() + \n  geom_histogram(aes(x = Petal.Length, fill = Species)) +\n  scale_x_continuous(name = xlabel) +\n  scale_y_continuous(name = ylabel) +\n  scale_fill_viridis_d(\"\", option = \"turbo\", \n                      begin = 0, end = 0.5, labels = scales::parse_format()) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10),\n        legend.position = c(1,1),\n        legend.justification = c(1,1))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "part05.html#ヒストグラムパネル",
    "href": "part05.html#ヒストグラムパネル",
    "title": "ggplot の図",
    "section": "ヒストグラム・パネル",
    "text": "ヒストグラム・パネル\n\nxlabel = \"Petal length (cm)\"\nylabel = \"Frequency\"\niris2 |> \n  ggplot() + \n  geom_histogram(aes(x = Petal.Length, fill = Species),\n                 binwidth = 0.1, center = 0) +\n  scale_x_continuous(name = xlabel, limits = c(0, 10)) +\n  scale_y_continuous(name = ylabel) +\n  scale_fill_viridis_d(\"\", option = \"turbo\", \n                      begin = 0, end = 0.5, labels = scales::parse_format()) +\n  facet_rep_wrap(facets = vars(Species)) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10),\n        legend.position = c(1,1),\n        legend.justification = c(1,1),\n        strip.background = element_blank(),\n        strip.text = element_blank())\n\nWarning: Removed 6 rows containing missing values (geom_bar)."
  },
  {
    "objectID": "part05.html#時系列",
    "href": "part05.html#時系列",
    "title": "ggplot の図",
    "section": "時系列",
    "text": "時系列\nデータは (https://covid.ourworldindata.org/data/owid-covid-data.csv)。\n\ncovid = read_csv(\"https://covid.ourworldindata.org/data/owid-covid-data.csv\")\n\nRows: 216106 Columns: 67\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (4): iso_code, continent, location, tests_units\ndbl  (62): total_cases, new_cases, new_cases_smoothed, total_deaths, new_dea...\ndate  (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncovid2 = covid |> \n  group_by(continent, date) |> \n  summarise(tc = sum(total_cases_per_million, na.rm=T),\n            td = sum(total_deaths_per_million, na.rm= T)) |> \n  drop_na()\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\n\n\nxlabel = \"Date\"\nylabel = \"COVID cases per million\"\nggplot(covid2) +\n  geom_path(aes(x=date, y = tc, color = continent))+\n  scale_x_date(name = xlabel) +\n  scale_y_continuous(name = ylabel) +\n  scale_color_viridis_d(\"\", option = \"turbo\", begin = 0, end = 0.8) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10),\n        legend.position = c(0,1),\n        legend.justification = c(0,1),\n        strip.background = element_blank(),\n        strip.text = element_blank())"
  },
  {
    "objectID": "part05.html#時系列-1",
    "href": "part05.html#時系列-1",
    "title": "ggplot の図",
    "section": "時系列",
    "text": "時系列\n\nxlabel = \"Date\"\nylabel = \"COVID cases per million\"\nggplot(covid2) +\n  geom_path(aes(x=date, y = tc, color = continent))+\n  scale_x_date(name = xlabel, date_labels = \"%Y-%m-%d\") +\n  scale_y_continuous(name = ylabel, \n                     breaks = 10^seq(-2,7), limits = c(0.01, 10^7),\n                     trans = \"log10\", labels = scales::label_math(format = log10)) +\n  scale_color_viridis_d(\"\", option = \"turbo\", begin = 0, end = 0.8) +\n  guides(color = guide_legend(ncol = 2)) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10),\n        legend.position = c(0.5,0),\n        legend.justification = c(0,0),\n        legend.background = element_blank(),\n        strip.background = element_blank(),\n        strip.text = element_blank())\n\nWarning: Transformation introduced infinite values in continuous y-axis"
  },
  {
    "objectID": "part05.html#時系列軸のカスタムラベル",
    "href": "part05.html#時系列軸のカスタムラベル",
    "title": "ggplot の図",
    "section": "時系列軸のカスタムラベル",
    "text": "時系列軸のカスタムラベル\n\ngnn_date = function() {\n  function(x) {\n    m = format(x, \"%b\")\n    m = str_sub(m, start = 1, end = 1)\n    y = format(x, \"%Y\")\n    ifelse(duplicated(y), m, sprintf(\"%s\\n%s\", m,y))\n  }\n}"
  },
  {
    "objectID": "part05.html#時系列-2",
    "href": "part05.html#時系列-2",
    "title": "ggplot の図",
    "section": "時系列",
    "text": "時系列\n\nxlabel = \"Date\"\nylabel = \"COVID cases per million\"\nggplot(covid2) +\n  geom_path(aes(x=date, y = tc, color = continent))+\n  scale_x_date(name = xlabel, date_breaks = \"months\", labels = gnn_date()) +\n  scale_y_continuous(name = ylabel, \n                     breaks = 10^seq(-2,7, by = 2), limits = c(0.01, 10^7),\n                     trans = \"log10\", labels = scales::label_math(format = log10)) +\n  scale_color_viridis_d(\"\", option = \"turbo\", begin = 0, end = 0.8) +\n  guides(color = guide_legend(ncol = 2)) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10),\n        legend.position = c(0.5,0),\n        legend.justification = c(0,0),\n        legend.background = element_blank(),\n        strip.background = element_blank(),\n        strip.text = element_blank())\n\nWarning: Transformation introduced infinite values in continuous y-axis"
  },
  {
    "objectID": "part05.html#箱ひげ図",
    "href": "part05.html#箱ひげ図",
    "title": "ggplot の図",
    "section": "箱ひげ図",
    "text": "箱ひげ図\n\ncovid2 = covid |> \n  filter(date >= lubridate::ymd(\"2021-01-01\")) |> \n  filter(str_detect(location, \"Indonesia|Japan|South Korea|Taiwan|China\"))\n\n\nxlabel = \"Country\"\nylabel = \"Daily cases per million\"\nggplot(covid2) +\n  geom_boxplot(aes(x = fct_reorder(location, new_cases_per_million, mean, na.rm=T, .desc=T), \n                   y = new_cases_per_million, fill = location)) + \n  scale_x_discrete(name = xlabel) +\n  scale_y_continuous(name = ylabel, \n                     breaks = 10^seq(-3,3, by = 1), limits = 10^c(-3, 3),\n                     trans = \"log10\", labels = scales::label_math(format = log10)) +\n  scale_color_viridis_d(\"\", option = \"turbo\",  begin = 0, end = 0.8) +\n  guides(fill = guide_legend(ncol = 2)) +\n  theme_pubr(base_family = \"notosans\") +\n  theme(text = element_text(size = 10),\n        legend.position = c(0,0),\n        legend.justification = c(0,0),\n        legend.background = element_blank(),\n        legend.title = element_blank(),\n        strip.background = element_blank(),\n        strip.text = element_blank())\n\nWarning: Transformation introduced infinite values in continuous y-axis\n\n\nWarning: Removed 316 rows containing non-finite values (stat_boxplot)."
  },
  {
    "objectID": "summary-statistics.html#rスクリプトの準備",
    "href": "summary-statistics.html#rスクリプトの準備",
    "title": "記述統計",
    "section": "Rスクリプトの準備",
    "text": "Rスクリプトの準備\nRコードは上から下へと実行します。スクリプトの冒頭には少なくともタイトル、作者名、作成日をコメントとして入れましょう。コメントは # の後に 1 つの半角スペースを入れてから書いてください。\n\n# 記述統計量の求め方\n# Greg Nishihara\n# 2022 May 01"
  },
  {
    "objectID": "summary-statistics.html#データの準備",
    "href": "summary-statistics.html#データの準備",
    "title": "記述統計",
    "section": "データの準備",
    "text": "データの準備\n実際の解析の場合、データはエクセルやCSV（コンマ区切り）ファイルから読み込みます。クラウド（Google sheet）からの読み込みも可能です。ここでは、直接スクリプトに書き込みます。\n長崎市気象台から2022年5月1日から14日までの日ごとの平均気温は次の通りです。\n\ntemperature = c(16.3, 16.9, 17.1, 17.3, 20.0, \n                21.1, 21.4, 20.6, 19.7, 21.0, \n                20.6, 21.1, 20.0, 19.4)\n\nc() は複数値を一つのベクトル [^vector] に concatenate（連結）するために使います。c() の結果は temperature というオブジェクトに書き込みました。\n\n\n\n\n\n\nImportant\n\n\n\nRの変数名と関数名には、文字・数字・ドット・アンダースコアをつきますが、先頭の 1 文字は文字かドットじゃないといけません。\n次のような名前はOKです。\n\ntemperature     = c(16.3, 16.9)\nTemperature     = c(16.3, 16.9)\n.temperature    = c(16.3, 16.9)\ntemp.erature    = c(16.3, 16.9)\ntem_pera_ture   = c(16.3, 16.9)\ntemperature2022 = c(16.3, 16.9)"
  },
  {
    "objectID": "summary-statistics.html#平均値や標準偏差などの求め方",
    "href": "summary-statistics.html#平均値や標準偏差などの求め方",
    "title": "記述統計",
    "section": "平均値や標準偏差などの求め方",
    "text": "平均値や標準偏差などの求め方\n平均値 \\overline{x} は次のように定義できます。\n\n\\overline{x} = \\frac{1}{N}\\sum_{n = 1}^{N} x_n\n\nx_n はインデクス n の値です。値は合計 N あります。\n平均値は mean() 関数で求めます。mean() には処理したいベクトルを渡してください。\n\nmean(temperature)\n\n[1] 19.46429\n\n\n分散 s^2=Var(x) は次のように定義します。\n\ns^2 = Var(x) = \\frac{1}{N}\\sum_{n = 1}^{N} \\left(x_n-\\overline{x} \\right)^2\n\n標準偏差は分散の平方根 (s = \\sqrt{s^2}) です。\n分散と標準偏差はそれぞれ、var()と sd() で求めます。\n\nvar(temperature) # 分散\n\n[1] 3.19478\n\nsd(temperature)  # 標準偏差\n\n[1] 1.787395\n\n\n専用の標準誤差の関数はありませんが、関数を組み合わせて、求めまれます。\n標準誤差 (SE) は次の式で求めます。\n\n\\text{SE} =s / \\sqrt{N}\n\n\nsd(temperature) / sqrt(length(temperature))\n\n[1] 0.4777014\n\n\nlength() 関数はベクトルのサイズ（要素の数）を求めてくれます。その結果を sqrt() に直接渡します。標準偏差をこの結果で割れば、標準誤差が求められます。このコードを分解すると、次のようになります。\n\ns = sd(temperature) # 標準偏差\ns\n\n[1] 1.787395\n\nn = length(temperature) # データ数\nn\n\n[1] 14\n\nsqrtn = sqrt(n) # データ数の平方根\nsqrtn\n\n[1] 3.741657\n\ns / sqrtn # 標準誤差\n\n[1] 0.4777014"
  },
  {
    "objectID": "summary-statistics.html#中央値と中央絶対偏差",
    "href": "summary-statistics.html#中央値と中央絶対偏差",
    "title": "記述統計",
    "section": "中央値と中央絶対偏差",
    "text": "中央値と中央絶対偏差\n中央値 (median, \\tilde{x} ) と中央絶対偏差 (MAD, median absolute deviation) も記述統計量の一種です。中央値は次のアルゴリズムで求めます。\n\nデータを小さい順に並べる。\nデータ数が奇数のとき、中央値は (n+1)/2 番目の値です。\nデータ数が偶数のとき、中央値は n/2 と (n/2)+1 番目のデータの平均値です。\n\n\nmedian(temperature)\n\n[1] 20\n\n\nこのとき、データ数は奇数あるので、7番目の値が中央値です。\n\n\n\n\nntemperature116.3216.9317.1417.3519.4619.7720.0820.0920.61020.61121.01221.11321.11421.4\n\n\n中央絶対偏差は次のように定義します。\n\n\\text{MAD}=\\text{median}\\left(|x_n - \\tilde{x}|\\right)\n\n\nmedian(abs(temperature - median(temperature)))\n\n[1] 1.05\n\n\n\n\n\n\n\n\nNote\n\n\n\n中央絶対偏差と標準偏差はデータのばらつきの度合を説明するときに使います。標準偏差は外れ値の大きく影響されますが、中央絶対偏差は外れ値の影響にロバスト (robust)　です。"
  },
  {
    "objectID": "summary-statistics.html#外れ値の影響",
    "href": "summary-statistics.html#外れ値の影響",
    "title": "記述統計",
    "section": "外れ値の影響",
    "text": "外れ値の影響\n標準偏差と中央絶対偏差における外れ値の影響を調べてみましょう。 まずは、中央絶対偏差用の関数を定義します。\n\nmad = function(x) {\n  median(abs(x - median(x)))\n}\n\ntemperature に外れ値を追加します。\n\noutlier = 25\ntemperature_with_outlier = c(temperature, outlier)\n\n外れ値のないときの標準偏差と中央絶対偏差は次の通りです。\n\nsd(temperature)   # 標準偏差\n\n[1] 1.787395\n\nmad(temperature)  # 中央絶対偏差\n\n[1] 1.05\n\n\n\nsd(temperature_with_outlier)   # 標準偏差\n\n[1] 2.238197\n\nmad(temperature_with_outlier)  # 中央絶対偏差\n\n[1] 1.1\n\n\n外れ値が存在するときに、標準偏差の値が大きく変わりましたが、 中央絶対偏差の変化は比較的に小さいです。\n\n\n\n\n\n\nNote\n\n\n\nばらつきを示す指標は、標準偏差と中央絶対偏差以外に、平均絶対偏差 (mean absolute deviation) や四分位偏差 (quartile deviation) もあります。平均絶対偏差は次の式で求めます。 \n\\text{MAD}_\\text{mean} = \\frac{1}{N}\\sum_{n=1}^N |x_n - \\overline{x}|\n 四分位偏差は次の式で求めます。 \nIQR = \\frac{1}{2}(Q_3 - Q1)\n Q_3 は第3四分位数、Q_1 は第1四分位数を示します。\n\nmad2 = function(x) {\n  mean(abs(x - mean(x)))\n}\niqrdev = function(x) {\n  as.numeric(diff(quantile(x, probs = c(0.25, 0.75))))\n}\n\nmad2(temperature_with_outlier)   # 平均絶対偏差\n\n[1] 1.64\n\niqrdev(temperature_with_outlier) # 四分位偏差\n\n[1] 2.7"
  },
  {
    "objectID": "summary-statistics.html#偏差の結果",
    "href": "summary-statistics.html#偏差の結果",
    "title": "記述統計",
    "section": "偏差の結果",
    "text": "偏差の結果\n\n\n\n\n外れ値標準偏差中央絶対偏差平均絶対偏差四分位偏差あり2.2381971.101.640002.700なし1.7873951.051.474493.075"
  },
  {
    "objectID": "ttest.html",
    "href": "ttest.html",
    "title": "2群の比較：t 検定",
    "section": "",
    "text": "ノコギリモク (Sargassum macrocarpum) は褐藻類ホンダワラ属の海藻です。 通年藻場を形成する海藻であり、海洋動物の住処、餌場、炭素固定の場として機能しています。 かつて、九州に広く分布していましたが、温暖化に伴う環境変動と食害によって、局地的に絶滅しています。 ここでは、ノコギリモクの幼体を資料として、2軍における解析手法を紹介します。\nでは、地点 A と B のノコギリモク幼体の幅は Table 1 の通りです。 各地点から合計６個体採取しました。"
  },
  {
    "objectID": "ttest.html#作業仮説を考えましょう",
    "href": "ttest.html#作業仮説を考えましょう",
    "title": "2群の比較：t 検定",
    "section": "作業仮説を考えましょう",
    "text": "作業仮説を考えましょう\n\n\n\n\n\n\n作業仮設1\n1 working hypothesis\n\n\nすべての研究は作業仮説 から始まります。\n今回の例について、作業仮説は 「地点毎に対するノコギリモク幼体の幅は異なる」にしました。"
  },
  {
    "objectID": "ttest.html#帰無仮説有意生検定が必要とする仮説を決めます",
    "href": "ttest.html#帰無仮説有意生検定が必要とする仮説を決めます",
    "title": "2群の比較：t 検定",
    "section": "帰無仮説有意生検定が必要とする仮説を決めます",
    "text": "帰無仮説有意生検定が必要とする仮説を決めます\n作業仮説を定義したら、つぎは検定のための仮説を定義します。 帰無仮説有意性検定 22 null hypothesis signficance testing\n\nH_0 (null hypothesis 帰無仮説): 平均値に違いはない (\\mu_{A} = \\mu_{B})\nH_A (alternative hypothesis 対立仮設): 平均値は異なる (\\mu_{A} \\neq \\mu_{B})\n\nつぎのような対立仮説も思いつきます。\n\nH_P (対立仮設): \\mu_A > \\mu_B\nH_N (対立仮設): \\mu_A < \\mu_B\n\n\n\n\n\n\n\n無限に存在する\n\n\n\n帰無仮説と対立仮説はいくらでも考えられますが、 \\mu_A = \\mu_B は一般的な帰無仮説です。 そして、\\mu_A \\neq \\mu_B も一般的な対立仮説です。"
  },
  {
    "objectID": "ttest.html#ナイーブ-な解析手法",
    "href": "ttest.html#ナイーブ-な解析手法",
    "title": "2群の比較：t 検定",
    "section": "ナイーブ 3 な解析手法3 naive",
    "text": "ナイーブ 3 な解析手法3 naive\n地点 A と B のノコギリモクの大きさの違いが知りたいです。 では、地点同士の大きさの違いを求めます。 地点 A と B の平均値の差を求めてみます。\n\n\\overline{x_A} - \\overline{x_B} = -2.22\n 地点 B のノコギリモクが大きいです。 でも、この大きさはどの程度信用できるかがわかりません。 平均値の差の制度を評価するには、標準誤差 4 を求めないといけないです。 この手法だと、標準誤差は求められません。4 standard error\nでは、かく地点のサンプル番号ごとの差をとってみます。 この場合、 6 つの差を求められます。 6 つあるので、平均値、標準偏差、標準誤差も求められます。\n\n\n\n\nTable 3:  ノコギリモクの幅 (mm) とペア毎の差 \nSampleSite ASite BDifference119.922.319.90 - 22.30 = -2.40220.622.920.60 - 22.90 = -2.30320.322.020.30 - 22.00 = -1.70420.423.720.40 - 23.70 = -3.30520.920.920.90 - 20.90 = 0.00618.121.718.10 - 21.70 = -3.60\n\n\n\n\n\n\n\n\\overline{x} = -2.2\ns = 1.3\n\\text{s.e.} = 0.53\n\n問題は、この差の平均値をどのように評価するのか。"
  },
  {
    "objectID": "ttest.html#平均値の分布",
    "href": "ttest.html#平均値の分布",
    "title": "2群の比較：t 検定",
    "section": "平均値の分布",
    "text": "平均値の分布\n\n\n\n\n\n\n\nFigure 2: 求めた平均値と標準誤差から推定した正規分布。\n\n\n\n\n\n\n中心極限定理 5によると、平均値の分布は正規分布 6に従います。5 central limit theorem6 normal or gaussian distribution\nFigure 2 に示した紫色の部分は 95% の確率密度です。 その幅は 信頼区間 7といいます。 有意水準を \\alpha = 0.05 として定義したとき、 この信頼区間は 95% 信頼区間 8といいます。7 confidence interval8 95% confidence interval\n\n\n\n\n\n\n\n\n信頼区間とは？\n\n\n\n[l, u] の区間を定義したとき、l は区間の下限、u は区間の上限です。 このように定義した区間は信頼区間といいます。\nでは、x に対する区間 [l,u] は 1-\\alpha の確率で次のように定義できます。\n\nP(l \\le x \\le u) = 1-\\alpha\n\n\\overline{x} が標本平均であれば、z値 9と呼ぶ統計量を定義できます。9 z-score\n\nz = \\frac{\\overline{x}-\\mu}{\\sigma}\n\n\\mu は母平均、\\sigma は母分散です。\nつまり、下限と上限を求めるためには\n\nP(l \\le z \\le u) = 1-\\alpha\n\nを解けばいい。\n中心極限定理は次の通りに定義されています。\n\n\\lim_{n\\rightarrow\\infty} \\sqrt{n}\\overbrace{\\left(\\frac{\\overline{x}_n-\\mu}{\\sigma}\\right)}^\\text{この部分は z 値}  \\xrightarrow{d} N(0, 1)\n\nよって、 \\alpha = 0.05　のときの [l, u] は次の通りです。\n\nP\\left(l \\le z \\le u \\right) = 1-0.05 = 0.95\n\n標準化正規分布 N(0,1) のとき、\n\n\\alpha/2=0.05/2=0.025 分位数は l です。\n1-\\alpha/2=1-0.05/2=0.975 分位数は u です。"
  },
  {
    "objectID": "ttest.html#z-値の分位数を求める",
    "href": "ttest.html#z-値の分位数を求める",
    "title": "2群の比較：t 検定",
    "section": "z 値の分位数を求める",
    "text": "z 値の分位数を求める\n\n\n\n\n\nFigure 3: 標準化正規分布\n\n\n\n\n\n[-1 s, 1 s] は 68.3% 区間\n[-2 s, 2 s] は 95.4% 区間\n[-3 s, 3 s] は 99.7% 区間\n\n\n\n\n\n\n\n\nTable 4:  標準化正規分布の分位数 \nSignififance levelpercent± quantile0.5000000000050.000000.67448980.3173105078668.268951.00000000.2000000000080.000001.28155160.1000000000090.000001.64485360.0500000000095.000001.95996400.0455002639095.449972.00000000.0250000000097.500002.24140270.0026997960699.730023.00000000.0000633424899.993674.0000000"
  },
  {
    "objectID": "ttest.html#信頼区間の求め方",
    "href": "ttest.html#信頼区間の求め方",
    "title": "2群の比較：t 検定",
    "section": "信頼区間の求め方",
    "text": "信頼区間の求め方\n平均値は \\overline{x}_{A-B} = 21.142 です。 標準誤差は \\text{s.e.} = 0.431 です。 母分散は \\sigma_A = \\sigma_B = 1 です。 有意水準は \\alpha = 0.05 とします。\n95% 信頼区間は次のように定義しています。 \nP\\left(l \\le \\frac{\\overline{x}-\\mu}{\\sigma}\\le u\\right) = 1-\\alpha = 0.95\n\n書き直すと次のとおりです。\n\nP\\left(\\overline{x} +l \\sigma \\le \\mu \\le \\overline{x} + u\\sigma\\right) = 1-\\alpha = 0.95\n\n\\alpha= 0.05　のとき、 l= -1.96 と u= 1.96 です。\n母分散は先程定義しましたが、\\sigma = 1 です。 それぞれの値を式に代入すると、次のとおりです。\n\n\\begin{split}\nP(\n21.142 +  -1.96 \\times 1\n\\le x \\le\n21.142 +  1.96 \\times 1\n) &=\nP(\n\\overbrace{19.182}^{l}\n\\le x \\le\n\\overbrace{23.102}^{u}\n) \\\\\n&= 0.95\n\\end{split}\n\nつまり、 \\overline{x}= 21.142 の 95% 信頼区間は [19.182, 23.102 ] です。\n\n\n\n\n\nFigure 4: 調査を 20 回行ったときの平均値と信頼区間。真の平均値は -2 です。このとき、全ての実験で求めた信頼区間内に真の平均値が存在します。\n\n\n\n\n信頼区間内に 0 が含まれるときの、帰無仮説は棄却できません。 ちなみに、このときの帰無仮説は「平均値はゼロ」です。\n\n\n\n\n\nFigure 5: ゼロを含む信頼区間。\n\n\n\n\n\n真の平均値は -2 なので、仮定した帰無仮説はそもそも誤りです。\nH_0 を棄却しなかったら、 第2種の誤り 10がおきます。\n8 つの調査の 95% 信頼区間は 0 を含みます。つまり、第2種の誤りは \\beta= 8 / 20 = 0.4 (40%) です。\nこの解析の検出力 (1 - \\beta) は 0.6 です。正しい結果に導く確率は 60% です。\n\n10 Type-II error"
  },
  {
    "objectID": "ttest.html#解析は誤りです",
    "href": "ttest.html#解析は誤りです",
    "title": "2群の比較：t 検定",
    "section": "解析は誤りです!",
    "text": "解析は誤りです!\nz 値は正規分布に従いますが、このとき母平均と母分散は存知です。\n\nz = \\frac{\\overline{x} - \\mu}{\\sigma}\\sim N(0,1)\n\n++ところが、一般的には母平均と母分散は未知です。** 一般的には z 値より、t 値を求めます。\n\nt_{\\overline{x}} = \\frac{\\overline{x} - x_0}{s.e.} = \\frac{\\overline{x} - x_0}{s / \\sqrt{n}}\n\nt 値は t 分布に従います。\n\n\n\n\n\nFigure 6: ｔ分布\n\n\n\n\nこの t 分布の 自由度 11は N-1 = 5 です。11 degrees-of-freedom\n\n\n\n\nTable 5:  自由度 5 のときに t 分布の分位数 \nSignififance levelpercent± quantile0.5000000050.000000.72668680.3632174763.678251.00000000.2000000080.000001.47588400.1019394889.806052.00000000.1000000090.000002.01504840.0500000095.000002.57058180.0300992596.990083.00000000.0250000097.500003.16338140.0103234298.967664.0000000\n\n\n\n\n\n\n\n\nFigure 7: 母分散が未知のときの結果\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFigure 7 は 20 の標本平均とそれぞれの 95% 信頼区間を示しています。 真の平均値が頼区間に含まれている実験は紫色で示しています。 20 の調査のうち、新の平均値を含む回数は 19 回です。\n信頼区間の解釈について\n基本的には、95%　信頼区間を次のように理解できる。 実験を 100 回行い、信頼区間内に真の平均値が含まれる回数は 95 回です。。\n下記で述べた解釈はすべて誤りです。\n\n信頼区間に真の平均値が存在する。\n95% の確率で真の平均値が信頼区間に含まれる。\n95% の確率で次の実験の平均値が信頼区間に含まれる。\n\n\n\n\n\n\n\n\nFigure 8: 信頼区間にゼロが含まれる回数\n\n\n\n\n\n信頼区間に 0 を含む実験は 5つあるので、 \\beta= 5 / 20 = 0.25 (25%) です。\nこの実験の検出力 (1 - \\beta) は 0.75　です。"
  },
  {
    "objectID": "ttest.html#対応ありの-t-検定",
    "href": "ttest.html#対応ありの-t-検定",
    "title": "2群の比較：t 検定",
    "section": "対応ありの t 検定",
    "text": "対応ありの t 検定\n\n\n\n\n\nFigure 9: 対応ありの t 検定\n\n\n\n\nこのときの第２種の誤りををこす確率は \\beta = 5 / 20 = 25% です。 検出力　(1-\\beta) は 0.75です。"
  },
  {
    "objectID": "ttest.html#分散が異なる-t-検定",
    "href": "ttest.html#分散が異なる-t-検定",
    "title": "2群の比較：t 検定",
    "section": "分散が異なる t 検定",
    "text": "分散が異なる t 検定\n\n\n\n\n\nFigure 10: 分散が異なる t 検定（ウェルチの t 検定ともよびます）\n\n\n\n\nこのときの第２種の誤りををこす確率は \\beta = 1 / 20 = 5% です。\n検出力 (1-\\beta) はs 0.95 です。"
  },
  {
    "objectID": "ttest.html#対応ありの-t-検定-1",
    "href": "ttest.html#対応ありの-t-検定-1",
    "title": "2群の比較：t 検定",
    "section": "対応ありの t 検定",
    "text": "対応ありの t 検定\n対応ありのt検定 (paired t-test)\nt 検定の統計量は t 値です。\n\nt^* = \\frac{\\overline{x}_{A-B} - \\mu}{s_{A-B} / \\sqrt{n}}\n\n対応ありの t 検定の自由度は n-1　です。\n観測値がペアとして対応しているときに使います。 たとえば、低い光環境で育て海藻を高い光環境に移した時の成長速度の差を比較するときに使います。"
  },
  {
    "objectID": "ttest.html#標本の-t-検定",
    "href": "ttest.html#標本の-t-検定",
    "title": "2群の比較：t 検定",
    "section": "2標本の t 検定",
    "text": "2標本の t 検定\n2標本 （２群）t 検定には 2 種類あります。\n等分散の t 検定 (equal variance t-test)\n\nt^* = \\frac{\\overline{x}_A - \\overline{x}_B}{s_p \\sqrt{1 / n_A + 1/n_B}}\n \ns_p = \\sqrt{\n\\frac{(n_A-1)s_A^2 + (n_B-1)s_B^2}\n{n_A + n_B -2}}\n 自由度は n_A + n_B - 2　です。\n不等分散の t 検定・ウェルチの t 検定 (Unequal variance, Welch’s t-test)\n\nt^* = \\frac{\\overline{x}_A - \\overline{x}_B}{s_p}\n\n\ns_p = \\sqrt{\n\\frac{s_A^2}{n_A} +\n\\frac{s_B^2}{n_B}}\n 自由度はウェルチ–サタスウェイトの式 (Welch-Satterthwaite Equation) で求めます。\ns は標準偏差、 n サンプル数、 \\overline{x} は平均値、 t^* は t 値。\n\n\\text{degrees-of-freedom} =\n\\frac{\n\\left(\\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B}\\right)^2\n}\n{\\frac{\\left(s_A^2 / n_A\\right)^2}{n_A-1} + \\frac{\\left(s_B^2 / n_B\\right)^2}{n_B-1}}\n\ndegrees-of-freedom は自由度です。"
  },
  {
    "objectID": "ttest.html#ノコギリモク幼体の幅に対する-t-検定",
    "href": "ttest.html#ノコギリモク幼体の幅に対する-t-検定",
    "title": "2群の比較：t 検定",
    "section": "ノコギリモク幼体の幅に対する t 検定",
    "text": "ノコギリモク幼体の幅に対する t 検定\n\n\n対応ありの t 検定\n\n\n\n\n\\begin{aligned}\nt^* &= \\frac{\\overline{x}_{A-B} - \\mu}{s_{A-B} / \\sqrt{n}} \\\\\nt^* &= \\frac{-2.467}{2.642 / \\sqrt{6}} \\\\\nt^* &= -2.287\n\\end{aligned}\n\n\n\n\n\\overline{x}_{A-B}= -2.467\ns_{A-B}= 2.642\n\\mu=0\nn = 6\n\\alpha = 0.05\nt値: -2.287\nP値: 0.071\n\n\n\nノコギリモク幼体のデータはお互いに対応していないので、対応ありの t 検定の結果は誤りです。\n\n\nノコギリモク幼体の正しい解析はウェルチの t 検定です。\n\n\\begin{aligned}\nt^* &= \\frac{\\overline{x}_A -\\overline{x}_B}{s_p} \\\\\ns_p &= \\sqrt{\\frac{s_A^2}{n_A} + \\frac{s_B^2}{n_B}} \\\\\ns_p &= \\sqrt{\\frac{1.995^2}{6} + \\frac{1.961^2}{6}} \\\\\nt^* &= \\frac{10.05 - 12.517}{1.142} \\\\\nt^* &= -2.16 \\\\\n\\text{d.f.} &= 9.997\n\\end{aligned}\n\n\n\n\n\\alpha = 0.05\nt値-value: -2.16\nP値: 0.056\n\nP\\nless \\alpha = 0.05 なので、帰無仮説は棄却できません。"
  },
  {
    "objectID": "ttest.html#サンプル数と-p-値の関係",
    "href": "ttest.html#サンプル数と-p-値の関係",
    "title": "2群の比較：t 検定",
    "section": "サンプル数と p 値の関係",
    "text": "サンプル数と p 値の関係\n\n\n\n\n\n分散等しい t 検定の場合、サンプル数が増えると第 2 種の誤りは減少し、検出力は増加します。第 1 種の誤りは変わりません。\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nsite A の真の平均値は \\mu = 20、site B のは \\mu = 22 です。 site A の真の平均値は \\mu = 20、site B のは \\mu = 22 です。 site A と site B の真の標準偏差 (\\sigma) は 1 と 1 です。"
  },
  {
    "objectID": "ttest.html#behavior-of-the-t-test-unequal-variance",
    "href": "ttest.html#behavior-of-the-t-test-unequal-variance",
    "title": "2群の比較：t 検定",
    "section": "Behavior of the t-test (unequal variance)",
    "text": "Behavior of the t-test (unequal variance)\n\n\n\n\n\n分散が異なる t 検定の場合、第 2 種の誤りと検出力の動きは分散が等しい t 検定と似ていますが、標本数も強く影響します。\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nsite A の真の平均値は \\mu = 20、site B のは \\mu = 22 です。 site A の真の標準偏差は \\mu = 1、ですが、site B の標準偏差は \\sigma_B = k\\times\\sigma_A です。 s_A / s_B \\rightarrow\\inftyのとき、第2種の誤りは増加し、検出力は減少します。 さらに、標本数が増えると、不等分散性の影響が下がります。"
  },
  {
    "objectID": "ttest.html#ウェルチ-t-検定の-r-出力と結果",
    "href": "ttest.html#ウェルチ-t-検定の-r-出力と結果",
    "title": "2群の比較：t 検定",
    "section": "ウェルチ t 検定の R 出力と結果",
    "text": "ウェルチ t 検定の R 出力と結果\n\n\n\n    Welch Two Sample t-test\n\ndata:  value by name\nt = -2.16, df = 9.9971, p-value = 0.05612\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -5.01124979  0.07791646\nsample estimates:\nmean in group A mean in group B \n       10.05000        12.51667 \n\n\n\n# パッケージの読み込み\nlibrary(tidyverse)\n\n# 疑似データの作成\nA = c(9.8,11.1,10.7,10.7,11.8,6.2)\nB = c(12.5,13.8,12.0,15.5,9.8,11.5)\ndata = tibble(A, B)\ndata = data %>% pivot_longer(cols = c(A,B))\n\n# ウェルチ t 検定\nt.test(value ~ name, data = data)\n\n\n# ひと書き方\nt.test(A, B)\n\n\n# two-sample, equal variance t-test (等分散 t 検定)\nt.test(value ~ name, data = data, var.equal = TRUE)"
  },
  {
    "objectID": "ttest.html#パッケージの読み込み",
    "href": "ttest.html#パッケージの読み込み",
    "title": "2群の比較：t 検定",
    "section": "パッケージの読み込み",
    "text": "パッケージの読み込み\nt 検定だけしたいなら、次のパッケージの読み込みは不要です。 そう言っても、自分のワークフローでは、つぎのパッケージは必ず読み込んでいます。 パッケージを読み込もうとしたときに、 Error in library(tidyverse) : there is no package called 'tidyverse' のようなエラーがでたら、パッケージのインストールが必要です。\nパッケージのインストールは次のようにできます。\n\ninstall.packages(\"tidyverse\")\n\nでは、パッケージを読み込みます。\n\nlibrary(tidyverse)  # データの操作・処理・作図用メタパッケージ\nlibrary(readxl) 　　# xlsx ファイルの読み込み用\nlibrary(lubridate)　# 時刻データ用\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union"
  },
  {
    "objectID": "ttest.html#データの準備",
    "href": "ttest.html#データの準備",
    "title": "2群の比較：t 検定",
    "section": "データの準備",
    "text": "データの準備\nデータは CSV (Comma Separated Value; コンマ区切り) ファイルに保存しています。 ファイルの内容は次の通りです。 最初の 3 行にはファイルの説明があります。\n\n\n\n\n\n# 6 Sargassum macrocarpum individuals from 2 sites were measured.\n# site: is the collection site (A, B).\n# size: is the width of the individual in mm.\nsite,size\nA,19.9\nA,20.6\nA,20.3\nA,20.4\nA,20.9\nA,18.1\nB,22.3\nB,22.9\nB,22\nB,23.7\nB,20.9\nB,21.7\n\n\nでは、データを R に読み込みます。\n\nfilename = \"./_data/sargassum_t-test_dataset.csv\"\ndset = read_csv(filename)\n\nWarning: One or more parsing issues, see `problems()` for details\n\n\nRows: 15 Columns: 1\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): # 6 Sargassum macrocarpum individuals from 2 sites were measured.\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n読み込んだデータの内容を確認しましょう。\n\ndset\n\n# A tibble: 15 × 1\n   `# 6 Sargassum macrocarpum individuals from 2 sites were measured.`\n   <chr>                                                              \n 1 # site: is the collection site (A, B).                             \n 2 # size: is the width of the individual in mm.                      \n 3 site,size                                                          \n 4 A,19.9                                                             \n 5 A,20.6                                                             \n 6 A,20.3                                                             \n 7 A,20.4                                                             \n 8 A,20.9                                                             \n 9 A,18.1                                                             \n10 B,22.3                                                             \n11 B,22.9                                                             \n12 B,22                                                               \n13 B,23.7                                                             \n14 B,20.9                                                             \n15 B,21.7                                                             \n\n\n説明があるので、読み込みに失敗しました。 読み込んだデータのクラス (class) は 15 行 1 列の tibble になっています。 2 列あるはずです。 この場合、read_csv() に説明を無視させないといけない。\nskip = 3 を read_csv() に渡せば、最初の 3 行をスキップできます。\n\nfilename = \"./_data/sargassum_t-test_dataset.csv\"\ndset = read_csv(filename, skip = 3)\n\nRows: 12 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): site\ndbl (1): size\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ndset\n\n# A tibble: 12 × 2\n   site   size\n   <chr> <dbl>\n 1 A      19.9\n 2 A      20.6\n 3 A      20.3\n 4 A      20.4\n 5 A      20.9\n 6 A      18.1\n 7 B      22.3\n 8 B      22.9\n 9 B      22  \n10 B      23.7\n11 B      20.9\n12 B      21.7\n\n\n12 行 2 列の tibble になりました。 列1の列名は site 列２の列名は size です。"
  },
  {
    "objectID": "ttest.html#まずデータの平均値や標準偏差などをもとめる",
    "href": "ttest.html#まずデータの平均値や標準偏差などをもとめる",
    "title": "2群の比較：t 検定",
    "section": "まずデータの平均値や標準偏差などをもとめる",
    "text": "まずデータの平均値や標準偏差などをもとめる\nsite ごとの size の平均値、標準偏差、サンプル数、標準誤差は tidyverse パッケージの解析システムをつかいます。\n\ndset |> \n  group_by(site) |> \n  summarise(across(size, list(mean = mean, sd = sd, n = length))) |> \n  mutate(size_se = size_sd / sqrt(size_n))\n\n# A tibble: 2 × 5\n  site  size_mean size_sd size_n size_se\n  <chr>     <dbl>   <dbl>  <int>   <dbl>\n1 A          20.0   1.00       6   0.410\n2 B          22.2   0.971      6   0.396"
  },
  {
    "objectID": "ttest.html#t-検定-1",
    "href": "ttest.html#t-検定-1",
    "title": "2群の比較：t 検定",
    "section": "t 検定",
    "text": "t 検定\nt検定は t.test() でやります。\n\nt.test(size ~ site, data = dset)\n\n\n    Welch Two Sample t-test\n\ndata:  size by site\nt = -3.8886, df = 9.9893, p-value = 0.003022\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -3.486976 -0.946357\nsample estimates:\nmean in group A mean in group B \n       20.03333        22.25000 \n\n\nt.test() の結果をオブジェクトに書き込んだら、t値 (t value)、p値 (p value)、自由度 (degrees of freedom) を抽出できます。\n\ndset_test = t.test(size ~ site, data = dset)\ndset_test$statistic  # t value\n\n        t \n-3.888623 \n\ndset_test$parameter  # degrees of freedom \n\n      df \n9.989348 \n\ndset_test$p.value 　 # p value\n\n[1] 0.003022351"
  },
  {
    "objectID": "ttest.html#結果",
    "href": "ttest.html#結果",
    "title": "2群の比較：t 検定",
    "section": "結果",
    "text": "結果\n\nノコギリモクの幼体において、サイト A から採取した幼体の幅（平均値±標準誤差）は 20.03 ± 0.41 mm でしたが、 サイト B から採取した幼体の幅は 22.25 ± 0.40 mm でした。 ｔ検定の結果、両地点で幼体幅間に有意な差がみられた (t(9.99) = -3.889; P = 0.0030)。\n\n有意水準より低いP値は「P < 0.05」のように書くことも有ります。 つまり、「ｔ検定の結果、両地点で幼体幅間に有意な差がみられた (t(9.99) = -3.889; P < 0.05)」。\nt検定の結果を記述することが重要です。この 3 つの情報を必ず記述しましょう。\n\nt(9.99): 検定に使用した自由度（サンプル数の目安）\n-3.889: t検定の統計量\nP = 0.0030: 結果のP値"
  },
  {
    "objectID": "ttest.html#付録-a-等分散性と正規性の検定",
    "href": "ttest.html#付録-a-等分散性と正規性の検定",
    "title": "2群の比較：t 検定",
    "section": "付録 A: 等分散性と正規性の検定",
    "text": "付録 A: 等分散性と正規性の検定\nデータの正規性と等分散性の検証も必要であれば Levene Test と Shapiro-Wilk Normality Test があります。 Levene Test は car パッケージの leveneTest() 関数でできますが、Shapiro-Wilk Normality Test はベースR に あるので、 パッケージの読み込みは必要ないです。\n等分散性の検定\nLevene Test (ルビーン検定) は2群以上の分散の均質性 (homogeneity) を検定するための検定です。 ルビーン検定の帰無仮説は「各群の分散は等しい」です。 有意水準より低いP値を求めたら、帰無仮説を棄却します。 棄却した場合、各群は均一な分散ではありません。\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nleveneTest(size ~ site, data = dset)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  1   0.079 0.7844\n      10               \n\n\n\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nルビーン検定の統計量は F 値です。 データの等分散性を確認したところ、 F(1,10) = 0.08、 P値は P = 0.7844です。 有意水準より大きいので、帰無仮説を棄却しません。 つまり、等分散性ではないといえません。\n正規性の検定\nShapiro-Wilk Normality Test (シャピロ–ウィルク検定) の帰無仮説は「サンプルが正規分布に従う母集団からとれた」です。 つまりシャピロウィルク検定から得たP値はサンプルの正規性を評価する指標です。 帰無仮説検定論の場合、有意水準より低いP値は帰無仮説を棄却することになり、センプルは正規分布に従わない母集団から得たものだと考えられるようになる。\n\nshapiro.test(dset$size)\n\n\n    Shapiro-Wilk normality test\n\ndata:  dset$size\nW = 0.97575, p-value = 0.9608\n\n\n\n\n\nシャピロウィルク検定の統計量は W値です。 W =0.98、 P = ので、 帰無仮説を棄却しません。 正規性に従わないといえません。\n一般的な手順のコード\n\nlibrary(tidyverse)\nlibrary(car)\nfilename = \"./_data/sargassum_t-test_dataset.csv\"\ndset = read_csv(filename, skip = 3)\n\n# (1) データの可視化\nggplot(dset) + \n  geom_point(aes(x = site, y = size)) +\n  labs(y = \"Width (mm)\",\n       x = \"Site\")\n\n# (2) 等分散性の確認\nleveneTest(size ~ site, data = dset)\n\n# (3) 正規性の確認\nshapiro.test(dset$size)\n\n# (4) t検定\nt.test(size ~ site, data = dset)"
  },
  {
    "objectID": "ttest.html#付録-b-本資料のデータ作成",
    "href": "ttest.html#付録-b-本資料のデータ作成",
    "title": "2群の比較：t 検定",
    "section": "付録 B: 本資料のデータ作成",
    "text": "付録 B: 本資料のデータ作成\n資料に使ったデータは次のコードでつくれます。\n\nlibrary(tidyverse)\nset.seed(2021)\nnA = 6\nnB = 6\nmeanA = 20\nmeanB = 22\nsigmaA = 1\nsigmaB = 1\ngroupA = rnorm(nA, meanA, sigmaA) |>  round(digits = 1)\ngroupB = rnorm(nB, meanB, sigmaB) |>  round(digits = 1)\ndset   = tibble(site = c(\"A\", \"B\"), size = list(groupA, groupB)) |>  unnest(size)\nL1 = \"# 6 Sargassum macrocarpum individuals from 2 sites were measured.\"\nL2 = \"# site: is the collection site (A, B).\"\nL3 = \"# size: is the width of the individual in mm.\"\nfname = \"sargassum_t-test_dataset.csv\"\nwrite_lines(file = fname, list(L1, L2, L3))\nwrite_csv(dset, file = fname, append = TRUE, col_names = TRUE)"
  },
  {
    "objectID": "anova.html#ノコギリモク-sargassum-macrocarpum-の疑似データ",
    "href": "anova.html#ノコギリモク-sargassum-macrocarpum-の疑似データ",
    "title": "一元配置分散分析",
    "section": "ノコギリモク (Sargassum macrocarpum) の疑似データ",
    "text": "ノコギリモク (Sargassum macrocarpum) の疑似データ\n\n\n\n\n\n\n\nThe width of the rope is 6 mm, so the juvenile is about 20 mm in width.\n\n\n\n\n\n\n\n\n\n\nTable 1:  Size (mm) of juvenile Sargassum macrocarpum（ノコギリモク）. \n \n  \n    Sample \n    Site A \n    Site B \n    Site C \n  \n \n\n  \n    1 \n    19.9 \n    22.3 \n    18.2 \n  \n  \n    2 \n    20.6 \n    22.9 \n    19.5 \n  \n  \n    3 \n    20.3 \n    22.0 \n    19.6 \n  \n  \n    4 \n    20.4 \n    23.7 \n    16.2 \n  \n  \n    5 \n    20.9 \n    20.9 \n    19.6 \n  \n  \n    6 \n    18.1 \n    21.7 \n    18.1 \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nノコギモクの大きさは Table 1 に示しています。 サンプルは 3 箇所（3群）から採取しました。 各サンプルに個体数番号もふっています。\nデータは 上のノートに紹介したコードで発生しました。"
  },
  {
    "objectID": "anova.html#データの可視化",
    "href": "anova.html#データの可視化",
    "title": "一元配置分散分析",
    "section": "データの可視化",
    "text": "データの可視化\n\nggplot(dset) + \n  geom_point(aes(x = g, y = data, color = g),\n             size = 2,\n             position = position_jitter(0.1)) +\n  scale_color_manual(\"\", values = viridis::viridis(4)) +\n  labs(y = \"Width (mm)\", x = \"Site\") +\n  theme(legend.position = \"top\")\n\n\n\n\n各サイトの平均値 (\\overline{x}), 標準偏差 (s), と標準誤差 (s.e.) は、\n\n\\overline{x}_A= 20; s_A= 1; s.e. = 0.41\n\\overline{x}_B= 22.2; s_B= 0.97; s.e. = 0.4\n\\overline{x}_C= 18.5; s_C= 1.34; s.e. = 0.55"
  },
  {
    "objectID": "anova.html#hypothesis",
    "href": "anova.html#hypothesis",
    "title": "一元配置分散分析",
    "section": "仮説を決める",
    "text": "仮説を決める\n\n\n\n\n\n\nImportant\n\n\n\n解析する前に作業仮説、帰無仮説、対立仮説を設定する必要があります。\n\n\n\n\n\n\n\n\nWorking hypothesis\n\n\n\n作業仮設: ノコギリモクの大きさは採取した場所によって異なる。\n\n\n\n記述統計量によって、平均値以外の統計量（標準偏差と標準誤差）は似ています。\n\\overline{x}_A= 20; s= 1; s.e. = 0.41\n\\overline{x}_B= 22.2; s= 0.97; s.e. = 0.4\n\\overline{x}_C= 18.5; s= 1.34; s.e. = 0.55\n\n\n帰無仮説と対立仮説\n統計学的に解析するための帰無仮説と対立仮説を決めます。\n\nH_0 (null hypothesis 帰無仮説・ヌル仮説): ノコギリモクの大きさは場所によって異ならない\nH_A (alternative hypothesis 対立仮設): ノコギリモクの大きさは場所によって異なる"
  },
  {
    "objectID": "anova.html#ナイーブな-ペア毎の-t-検定",
    "href": "anova.html#ナイーブな-ペア毎の-t-検定",
    "title": "一元配置分散分析",
    "section": "ナイーブな ペア毎の t 検定",
    "text": "ナイーブな ペア毎の t 検定\nとりあえず、場所のペア毎の t 検定を実施します。 このとき、3 つの帰無仮説が必要なので、(hypothesis?) と違います。\n\nH0,A-B: Site A と Site B の大きさは同じ\nH0,A-C: Site A と Site C の大きさは同じ\nH0,B-C: Site B と Site C の大きさは同じ\n\nでは、それぞれの t 検定を実施します。\n\nSite A and B の t 検定\n\nresultAB = dset |> filter(!str_detect(g, \"C\"))\nresultAB = t.test(data ~ g, data = resultAB)\nresultAB\n\n\n    Welch Two Sample t-test\n\ndata:  data by g\nt = -3.8886, df = 9.9893, p-value = 0.003022\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -3.486976 -0.946357\nsample estimates:\nmean in group A mean in group B \n       20.03333        22.25000 \n\n\nt値は -3.889、P値は 0.003 です。 0.003 \\le 0.05 なので、帰無仮説は棄却できます。\n\n\nSite A and C の t 検定\n\nresultAC = dset |> filter(!str_detect(g, \"B\"))\nresultAC = t.test(data ~ g, data = resultAC)\nresultAC\n\n\n    Welch Two Sample t-test\n\ndata:  data by g\nt = 2.1968, df = 9.2717, p-value = 0.05477\nalternative hypothesis: true difference in means between group A and group C is not equal to 0\n95 percent confidence interval:\n -0.03773888  3.03773888\nsample estimates:\nmean in group A mean in group C \n       20.03333        18.53333 \n\n\nt値は 2.197、P値は 0.0548 です。 0.0548 \\ge 0.05 なので、帰無仮説は棄却できません。\n\n\nSite B and C の t 検定\n\nresultBC = dset |> filter(!str_detect(g, \"A\"))\nresultBC = t.test(data ~ g, data = resultBC)\nresultBC\n\n\n    Welch Two Sample t-test\n\ndata:  data by g\nt = 5.5063, df = 9.1228, p-value = 0.0003595\nalternative hypothesis: true difference in means between group B and group C is not equal to 0\n95 percent confidence interval:\n 2.192862 5.240472\nsample estimates:\nmean in group B mean in group C \n       22.25000        18.53333 \n\n\nt値は 5.506、P値は 0.0004 です。 0.0004 \\le 0.05 なので、帰無仮説は棄却できます。\nt検定の結果をまとめました。\n\n\n\n\nTable 2:  Summary of three t-tests. \n \n  \n    Pair \n    Difference \n    t-value \n    P-value \n    d.f. \n    95% CI \n    Is P ≤ 0.05? \n  \n \n\n  \n    B-A \n    -2.216667 \n    -3.888623 \n    0.0030224 \n    9.989348 \n    -3.487 to -0.95 \n    Yes \n  \n  \n    C-A \n    1.500000 \n    2.196821 \n    0.0547748 \n    9.271711 \n    -0.038 to  3.04 \n    No \n  \n  \n    C-B \n    3.716667 \n    5.506257 \n    0.0003595 \n    9.122821 \n    2.193 to  5.24 \n    Yes \n  \n\n\n\n\n\n\nd.f. は Welch-Satterthwaite 式で求めた自由度1、95% CI は 95% 信頼区間です。1 degrees-of-freedom"
  },
  {
    "objectID": "anova.html#第１種の誤り-accepting-a-false-h0",
    "href": "anova.html#第１種の誤り-accepting-a-false-h0",
    "title": "一元配置分散分析",
    "section": "第１種の誤り (accepting a false H0)",
    "text": "第１種の誤り (accepting a false H0)\n\n\n\n\n\n\n第１種の誤り\n\n\n\nH0 が FALSE のときに帰無仮説を棄却できなかった誤りです。\n\n\nt 検定を 1 回実施したときの誤りは\n\n\\text{Type I error rate} = \\alpha = 0.05\n\nt 検定を 2 回実施したときの誤りは\n\n1 - (1 - \\alpha) \\times (1 - \\alpha) = 1 - (1-\\alpha)^2=0.0975\n\nt 検定を 3 回実施したときの誤りは\n\n1 - (1 - \\alpha) \\times (1 - \\alpha) \\times (1 - \\alpha)= 1 - (1-\\alpha)^3=0.142625\n\nt 検定を h 回実施したとき、第１種の誤りは 1 - (1-\\alpha)^h です。\n\n群が増えると大変なことなる\nn 群のサンプルの全ペア毎の比較がしたい場合、 h のペア (k = 2) が存在します。\n\nh = \\binom{n}{k}=\\frac{n!}{k!(n-k!)}\n\nペア毎の h の数を求める式は次のようになります。\n\nh = \\binom{n}{2}=\\frac{n!}{2!(n-2!)} = \\frac{n(n-1)}{2}\n\n例えば、5 site の場合、10 のペアが存在します。 ペア毎の t 検定をしたら、第１種の誤りは\n\n1 - (1-0.05)^{10}=0.4012631\n\n\n\n\n\n\n\nR での求め方\n\n\n\n\nalpha = 0.05       # 有意水準\nk = 2              # ペアだから 2\nn = 5              # 比較する群・場所・グループの数\nh = choose(n, k)   # ペアの数\n1 - (1 - alpha)^h  # 第１種の誤り\n\n[1] 0.4012631\n\n\n\n\n\n\n\n\n\n比較する群が増えると、t 検定を繰り返して実施すると、第１種の誤りを起こしやすい。"
  },
  {
    "objectID": "anova.html#one-way-anova-一元配置分散分析",
    "href": "anova.html#one-way-anova-一元配置分散分析",
    "title": "一元配置分散分析",
    "section": "One-Way ANOVA (一元配置分散分析)",
    "text": "One-Way ANOVA (一元配置分散分析)\n複数群（因子の水準）の解析は 一元配置分散分析)2 用います。2 One-Way ANOVA (One-Way Analysis of Variance\n\n因子・要因3：説明変数、一般的には離散型な変数\n水準4：説明変数における値、レベル、要素\n\n3 factor4 level, factor level分散分析の帰無仮説は、\n\n\\mu_1 = \\mu_2 = \\cdots = \\mu_i\n\nつまり、一つの検定で複数群の平均値を同時に解析するから、第１種の誤りは 0.05 に抑えられる。\n分散分析のモデル式は次のように表せます。\n\nx_{ij} = \\mu_i + \\epsilon_{ij}\n\n水準 i とサンプル j の値は x_{ij}です。 水準 i の平均値は \\mu_i です。 モデルの残渣[residual]または誤差項[error term]は \\epsilon_{ij} です。"
  },
  {
    "objectID": "anova.html#一元配置分散分析表",
    "href": "anova.html#一元配置分散分析表",
    "title": "一元配置分散分析",
    "section": "一元配置分散分析表",
    "text": "一元配置分散分析表\n\n\n\n\n \n  \n    Factor \n    Degrees-of-freedom (df) \n    Sum-of-Squares (SS) \n    Mean-square (MS) \n    F-value \n    P-value \n  \n \n\n  \n    A \n    $df_A = I-1$ \n    $SS_A$ \n    $MS_A = SS_A / df_A$ \n    $MS_A / MS_R$ \n    $qf(1-α, df_A, df_R)$ \n  \n  \n    e \n    $df_R = I(J-1)$ \n    $SS_R$ \n    $MS_R = SS_R / df_R$ \n     \n     \n  \n  \n     \n    $df_T =IJ-1$ \n    $SS_T$ \n     \n     \n     \n  \n\n\n\n\n\n\n因子は A5\n残渣は e6\n水準数は I7\nサンプル数は　J8\n水準間平方和は SS_A9\n残渣平方和は SS_R10\n総平方和は SS_T11\n水準間平均平方は MS_A12\n残渣平均平方は MS_R13\nF値14は MS の比です。\n\n5 factor6 residual7 number of levels8 number of samples9 among levels sum-of-squares (SS)10 residual SS11 total SS12 among levels mean square (MS)13 residual mean square (MS)14 F-value"
  },
  {
    "objectID": "anova.html#平方和の方程式",
    "href": "anova.html#平方和の方程式",
    "title": "一元配置分散分析",
    "section": "平方和の方程式",
    "text": "平方和の方程式\n\n\\underbrace{\\sum_{i=1}^I\\sum_{j=1}^J(x_{ij} - \\overline{\\overline{x}})^2 }_{\\text{総平方和}\\;(SS_T)} =\n\\overbrace{J\\sum_{i=1}^I(\\overline{x}_{i}-\\overline{\\overline{x}})^2}^{\\text{水準間平方和}\\;SS_A} +\n\\underbrace{\\sum_{i=1}^I\\sum_{j=1}^J(x_{ij} - \\overline{x}_i)^2}_{\\text{残渣平方和}\\;SS_R}\n\n標本平均は \\bar{x}_i、総平均は \\bar{\\bar{x}} です。"
  },
  {
    "objectID": "anova.html#decomposing-the-sum-of-squares",
    "href": "anova.html#decomposing-the-sum-of-squares",
    "title": "一元配置分散分析",
    "section": "Decomposing the sum-of-squares",
    "text": "Decomposing the sum-of-squares\n\n\n\n\n\n\n\n\n\n残渣平方和：各サンプルの値は点、グループ毎の平均値は黒線で示しています。黒線から点の縦線は残渣を表しています。\n\n\n\n\n\n\n\n水準間平方和：各グループの平均値は点、総平均は黒線で示しています。黒線から点の縦線はグループ毎の平均値と総平均の違いを表しています。"
  },
  {
    "objectID": "anova.html#分散分析の統計量",
    "href": "anova.html#分散分析の統計量",
    "title": "一元配置分散分析",
    "section": "分散分析の統計量",
    "text": "分散分析の統計量\n\nF = \\left . \\frac{SS_A}{I-1} \\right / \\frac{SS_R}{I(J-1)}  = \\frac{SS_A}{SS_R} \\frac{I(J-1)}{I-1} = \\frac{MS_A}{MS_R}\n\nF値は 自由度 \\text{df}_1 = I-1, \\text{df}_2 = I(J-1) のF分布に従います。 水準の数は I、水準ごとのサンプルの数は J です。\n\n\n\n\n\n\nNote\n\n\n\n\nF値の分子15 が大きとき、または分母16が小さいとき、F値は大きいです。\nF値とP値は反比例します。\n\n15 numerator16 denominator"
  },
  {
    "objectID": "anova.html#f値の確率密度関数",
    "href": "anova.html#f値の確率密度関数",
    "title": "一元配置分散分析",
    "section": "F値の確率密度関数",
    "text": "F値の確率密度関数\n\nP(x|\\text{df}_1, \\text{df}_2) = \\frac{1}{\\mathrm{B}\\left(\\frac{\\text{df}_1}{2}, \\frac{\\text{df}_2}{2}\\right)}\\left(\\frac{\\text{df}_1}{\\text{df}_2}\\right)^{\\left(\\frac{\\text{df}_1}{2}\\right)}x^{\\left(\\frac{\\text{df}_1}{2}-1\\right)}\\left(1+\\frac{\\text{df}_1}{\\text{df}_2}x\\right)^{\\left(-\\frac{\\text{df}_1+\\text{df}_2}{2}\\right)}\n \\mathrm{B}(\\text{df}_1, \\text{df}_2)=\\int_0^1t^{x-1}(1-t)^{y-1}dt は ベータ関数17 といいます。 \\text{df}_1 と \\text{df}_2 は自由度、x は確率変数です。17 Beta function\n\n\n\n\n\n自由度が変わるとF分布の形が変わります。"
  },
  {
    "objectID": "anova.html#一元配置分散分析表の仮定",
    "href": "anova.html#一元配置分散分析表の仮定",
    "title": "一元配置分散分析",
    "section": "一元配置分散分析表の仮定",
    "text": "一元配置分散分析表の仮定\n分散分析するときに注意する仮定\n\n水準毎の母分散は等しい\n残渣は正規分布に従う\n観測値はお互いに独立であり、同一分布に従う\n観測変数は連続変数18\n説明変数は離散変数19\n\n18 continuous19 discrete"
  },
  {
    "objectID": "anova.html#rにおける解析",
    "href": "anova.html#rにおける解析",
    "title": "一元配置分散分析",
    "section": "Rにおける解析",
    "text": "Rにおける解析\n解析例に使うデータは thedata.csv に保存したので、まずは読み込みます。\n\n\n\n\n# Read data from a csv file\ndset = read_csv(\"thedata.csv\")\n\nデータをグループ化したあと、最初の 2 行を表示する。\n\ndset |> group_by(site) |> slice(1:2)\n\n# A tibble: 6 × 2\n# Groups:   site [3]\n  site    obs\n  <fct> <dbl>\n1 A      19.9\n2 A      20.6\n3 B      22.3\n4 B      22.9\n5 C      18.2\n6 C      19.5\n\n\n帰無仮説を当てはめる。\n\nnullmodel = lm(obs ~ 1, data = dset) # 帰無モデル、ヌルモデル\n\nフルモデル（対立仮説）を当てはめる。\n\nfullmodel = lm(obs ~ site, data = dset) # 対立モデル、フルモデル"
  },
  {
    "objectID": "anova.html#分散分析の結果",
    "href": "anova.html#分散分析の結果",
    "title": "一元配置分散分析",
    "section": "分散分析の結果",
    "text": "分散分析の結果\n帰無仮説と対立仮説のモデル結果を用いた方法。\n\nanova(nullmodel, fullmodel, test = \"F\")\n\nAnalysis of Variance Table\n\nModel 1: obs ~ 1\nModel 2: obs ~ site\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1     17 60.656                                  \n2     15 18.702  2    41.954 16.825 0.0001471 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nF値は 16.825、自由度は df1 = 2 と df2 = 15 です。 よって、P値は 0.000147 です。 有意水準が \\alpha = 0.05 、自由度が　(2, 15) のときのF値は 3.682.\nフルモデルの結果でけ用いた解析\n\nanova(fullmodel)\n\nAnalysis of Variance Table\n\nResponse: obs\n          Df Sum Sq Mean Sq F value    Pr(>F)    \nsite       2 41.954 20.9772  16.825 0.0001471 ***\nResiduals 15 18.702  1.2468                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\naov() 関数を用いた方法\nこのとき、lm() は不要です。\n\naovout = aov(obs ~ site, data = dset)\nsummary(aovout)\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nsite         2  41.95  20.977   16.82 0.000147 ***\nResiduals   15  18.70   1.247                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nImportant\n\n\n\n分散分析の帰無仮説は \\mu_0 = \\mu_1 = \\cdots \\mu_i なので、 ペア間の検定ではないです。"
  },
  {
    "objectID": "anova.html#多重比較-1",
    "href": "anova.html#多重比較-1",
    "title": "一元配置分散分析",
    "section": "多重比較",
    "text": "多重比較\n分散分析の帰無仮説を棄却したら、ペア毎の比較がしたくなります。 第１種の誤りを抑える多重比較の検定は豊富に存在します。\n\nBonferroni Procedure (ボンフェロニ法)\nHolm-Bonferroni Method (ホルム = ボンフェロニ法)\nTukey’s Honest Signiﬁcant Difference Test (テューキーの HSD 検定)\nTukey-Kramer method, Tukey’s test\nScheffe’s Method (シェッフェの方法)\nDunnett’s Test (ダネットの検定)\nFisher’s Least Signiﬁcant Difference (フィッシャーの最小有意差法)\nDuncan’s new multiple range test (ダンカンの新多重範囲検定)\n\n1 から 4 はペア毎の比較です。 ダネットの検定は水準に対する比較です。 フィッシャーとダンカンの検定の第１種の誤りは高いので、使用しないでください。"
  },
  {
    "objectID": "anova.html#多重比較用-r-パッケージ",
    "href": "anova.html#多重比較用-r-パッケージ",
    "title": "一元配置分散分析",
    "section": "多重比較用 R パッケージ",
    "text": "多重比較用 R パッケージ\n多重比較用の関数は次のパッケージにあります。\n\nmultcomp\nemmeans\n\nここでは、emmeans を紹介します。\n\nlibrary(emmeans) # 多重比較用パッケージ\nlibrary(nlme)    # gls() 関数はこのパッケージにある\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse"
  },
  {
    "objectID": "anova.html#繰り返しウェルチの-t-検定",
    "href": "anova.html#繰り返しウェルチの-t-検定",
    "title": "一元配置分散分析",
    "section": "繰り返しウェルチの t 検定",
    "text": "繰り返しウェルチの t 検定\n説明のために紹介しています。実際の解析には使わないでください。\n\nglsmodel = gls(obs ~ site, data = dset, \n               weights = varIdent(form = ~ 1|site))\nemout = emmeans(glsmodel, specs = pairwise ~ site, adjust = \"none\")\nemout$contrasts |> summary(infer =T)\n\n contrast estimate    SE   df lower.CL upper.CL t.ratio p.value\n A - B       -2.22 0.570 9.99  -3.4870   -0.946  -3.889  0.0030\n A - C        1.50 0.683 9.26  -0.0381    3.038   2.197  0.0548\n B - C        3.72 0.675 9.11   2.1925    5.241   5.506  0.0004\n\nDegrees-of-freedom method: satterthwaite \nConfidence level used: 0.95 \n\n\n第１種の誤りを調整していません。"
  },
  {
    "objectID": "anova.html#繰り返し-t-検定",
    "href": "anova.html#繰り返し-t-検定",
    "title": "一元配置分散分析",
    "section": "繰り返し t 検定",
    "text": "繰り返し t 検定\n説明のために紹介しています。実際の解析には使わないでください。\n\nemout = emmeans(fullmodel, specs = pairwise ~ site, data = dset, adjust = \"none\")\nemout$contrasts |> summary(infer =T)\n\n contrast estimate    SE df lower.CL upper.CL t.ratio p.value\n A - B       -2.22 0.645 15   -3.591   -0.843  -3.438  0.0037\n A - C        1.50 0.645 15    0.126    2.874   2.327  0.0344\n B - C        3.72 0.645 15    2.343    5.091   5.765  <.0001\n\nConfidence level used: 0.95 \n\n\n第１種の誤りを調整していません。"
  },
  {
    "objectID": "anova.html#ボンフェロニ法",
    "href": "anova.html#ボンフェロニ法",
    "title": "一元配置分散分析",
    "section": "ボンフェロニ法",
    "text": "ボンフェロニ法\n\nemout = emmeans(fullmodel, specs = pairwise ~ site, data=dset, adjust = \"bonferroni\")\nemout$contrasts |> summary(infer =T)\n\n contrast estimate    SE df lower.CL upper.CL t.ratio p.value\n A - B       -2.22 0.645 15   -3.953    -0.48  -3.438  0.0110\n A - C        1.50 0.645 15   -0.237     3.24   2.327  0.1032\n B - C        3.72 0.645 15    1.980     5.45   5.765  0.0001\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 3 estimates \nP value adjustment: bonferroni method for 3 tests \n\n\nP値は p_{adj} =m\\times p によって求められました."
  },
  {
    "objectID": "anova.html#ホルムボンフェロニ法",
    "href": "anova.html#ホルムボンフェロニ法",
    "title": "一元配置分散分析",
    "section": "ホルム=ボンフェロニ法",
    "text": "ホルム=ボンフェロニ法\n\nemout = emmeans(fullmodel, specs = pairwise ~ site, data=dset, adjust = \"holm\")\nemout$contrasts |> summary(infer =T)\n\n contrast estimate    SE df lower.CL upper.CL t.ratio p.value\n A - B       -2.22 0.645 15   -3.953    -0.48  -3.438  0.0073\n A - C        1.50 0.645 15   -0.237     3.24   2.327  0.0344\n B - C        3.72 0.645 15    1.980     5.45   5.765  0.0001\n\nConfidence level used: 0.95 \nConf-level adjustment: bonferroni method for 3 estimates \nP value adjustment: holm method for 3 tests \n\n\nP値は低い値から高い値へ並べ替えてから、p_{adj} = (m+1-k)\\times p によって求めます。 m は比較の数、 k は比較の指数です。\n\n\n\n\n\n\nNote\n\n\n\nボンフェロニ法とホルム=ボンフェロニ法の P値は次のように求められます。\n\nemout = emmeans(fullmodel, specs = pairwise ~ site, adjust = \"none\")\nx = emout$contrasts  |>  as_tibble()\nx  |>  arrange(p.value)  |> \n  mutate(k = 1:3)  |> mutate(m = n())  |> \n  mutate(p.bonferroni = p.value * m,\n         p.holm = p.value * (m + 1 - k))  |> \n  select(contrast, m, k, starts_with(\"p\"))\n\n# A tibble: 3 × 6\n  contrast     m     k   p.value p.bonferroni   p.holm\n  <chr>    <int> <int>     <dbl>        <dbl>    <dbl>\n1 B - C        3     1 0.0000373     0.000112 0.000112\n2 A - B        3     2 0.00366       0.0110   0.00731 \n3 A - C        3     3 0.0344        0.103    0.0344"
  },
  {
    "objectID": "anova.html#テューキーのhsd法",
    "href": "anova.html#テューキーのhsd法",
    "title": "一元配置分散分析",
    "section": "テューキーのHSD法",
    "text": "テューキーのHSD法\n\nemout = emmeans(fullmodel, specs = pairwise ~ site, data=dset, adjust = \"tukey\")\nemout$contrasts |> summary(infer =T)\n\n contrast estimate    SE df lower.CL upper.CL t.ratio p.value\n A - B       -2.22 0.645 15   -3.891   -0.542  -3.438  0.0096\n A - C        1.50 0.645 15   -0.174    3.174   2.327  0.0826\n B - C        3.72 0.645 15    2.042    5.391   5.765  0.0001\n\nConfidence level used: 0.95 \nConf-level adjustment: tukey method for comparing a family of 3 estimates \nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nP値はステュデント化範囲の分布21に従います。21 Studentized range distribution"
  },
  {
    "objectID": "anova.html#シェッフェの方法",
    "href": "anova.html#シェッフェの方法",
    "title": "一元配置分散分析",
    "section": "シェッフェの方法",
    "text": "シェッフェの方法\n\nemout = emmeans(fullmodel, specs = pairwise ~ site, data=dset, adjust = \"scheffe\")\nemout$contrasts |> summary(infer =T)\n\n contrast estimate    SE df lower.CL upper.CL t.ratio p.value\n A - B       -2.22 0.645 15   -3.966   -0.467  -3.438  0.0128\n A - C        1.50 0.645 15   -0.249    3.249   2.327  0.0991\n B - C        3.72 0.645 15    1.967    5.466   5.765  0.0002\n\nConfidence level used: 0.95 \nConf-level adjustment: scheffe method with rank 2 \nP value adjustment: scheffe method with rank 2 \n\n\nP値はF分布に従います。"
  },
  {
    "objectID": "anova.html#ダネットの検定",
    "href": "anova.html#ダネットの検定",
    "title": "一元配置分散分析",
    "section": "ダネットの検定",
    "text": "ダネットの検定\n\nemout = emmeans(fullmodel, specs = trt.vs.ctrl ~ site, ref = 2)\nemout$contrasts |> summary(infer =T)\n\n contrast estimate    SE df lower.CL upper.CL t.ratio p.value\n A - B       -2.22 0.645 15     -3.8   -0.633  -3.438  0.0070\n C - B       -3.72 0.645 15     -5.3   -2.133  -5.765  0.0001\n\nConfidence level used: 0.95 \nConf-level adjustment: dunnettx method for 2 estimates \nP value adjustment: dunnettx method for 2 tests \n\n\nダネットの検定は、各水準は標準水準と比較する方法です。 P値は多変量 t 分布に従います。"
  },
  {
    "objectID": "anova.html#多重比較のおすすめ",
    "href": "anova.html#多重比較のおすすめ",
    "title": "一元配置分散分析",
    "section": "多重比較のおすすめ",
    "text": "多重比較のおすすめ\n\n比較: A – B, A – C, B – C  テューキーのHSD法\n比較: A – B, A – C, A – D  ダネット法"
  },
  {
    "objectID": "anova2.html",
    "href": "anova2.html",
    "title": "多数群の比較：二元配置分散分析",
    "section": "",
    "text": "二元配置分散分析 (Two-Way ANOVA) 2 種類の因子（要因）を同時に比較するときに使用する。\n二元配置分散分析の帰無仮説\nフルモデルは:\nx_{ijk} = \\mu_{Ai}+\\mu_{Bj} + \\mu_{ABij} + \\epsilon_{ijk}\n水準 i, j, とサンプル k の値は x_{ijk}。 因子 A の水準 i ごとの平均値は \\mu_{Ai}。 因子 B の水準 i ごとの平均値は \\mu_{Bi}。\n交互作用 AB の i,j 効果の平均値は \\mu_{ABij}。 残渣（誤差項）は \\epsilon_{ijk}。"
  },
  {
    "objectID": "anova2.html#two-way-anova-table-二元配置分散分析表",
    "href": "anova2.html#two-way-anova-table-二元配置分散分析表",
    "title": "多数群の比較：二元配置分散分析",
    "section": "Two-Way ANOVA Table (二元配置分散分析表)",
    "text": "Two-Way ANOVA Table (二元配置分散分析表)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFactor\nDegrees-of-freedom (df)\nSum-of-Squares (SS)\nMean-square (MS)\nF-value\nP-value\n\n\n\n\nA\ndf_A = I-1\nSS_A\nMS_A = SS_A / df_A\nMS_A / MS_R\nqf(1-\\alpha, df_A, df_R)\n\n\nB\ndf_B = J-1\nSS_B\nMS_B = SS_B / df_B\nMS_B / MS_R\nqf(1-\\alpha, df_B, df_R)\n\n\nAB\ndf_{AB} = (I-1)(J-1)\nSS_{AB}\nMS_{AB} = SS_{AB} / df_{AB}\nMS_{AB} / MS_R\nqf(1-\\alpha, df_{AB}, df_R)\n\n\ne\ndf_R = IJ(K-1)\nSS_R\nMS_R = SS_R / df_R\n\n\n\n\n\ndf_T = IJK-1\nSS_T\n\n\n\n\n\n\n\n\n\nA と B は主効果、 e は残渣、 I と J は各因子の水準、 K はサンプル数です。 SS_A と SS_B は水準間平方和、 SS_{AB} は相互作用平方和、 SS_R は残渣平方和、 SS_T は総平方和です。 MS_A と MS_B は水準間平均平方、 MS_{AB} は相互作用平均平方、 MS_R は残渣平均平方です。 平均平方の比率はF値です。\n\n\n\n\n\n\n平方和は他にもある\n\n\n\n上述した分散分析表は Type I 平方和 (SS) を求めています。 このとき、SS(A), SS(B|A), SS(AB|A,B) です。 分散分析の結果は因子に順序とに依存し、非釣り合い型データに合わない。\nType II 平方和は、SS(A|B) と SS(B|A) のみです。相互作用はありません。 分散分析の結果は因子に順序とに依存しないが、非釣り合い型データに合わない。\nType III 平方和は、SS(A|B, AB), SS(B|A, AB), SS(AB|A,B) です。 分散分析の結果は因子に順序とに依存しない、非釣り合い型データにも使えるが、 必ずcontr.sum を設定しなければならない。\n\n\n平方和の非釣り合い型データの問題については、Hector, Felten, and Schmid (2010), Langsrud (2003) を参考にしてください。\n\nHector, Andy, Stefanie von Felten, and Bernhard Schmid. 2010. “Analysis of variance with unbalanced data: An update for ecology & evolution.” Journal of Animal Ecology 79: 308–16. https://doi.org/10.1111/j.1365-2656.2009.01634.x.\n\nLangsrud, Øyvind. 2003. “ANOVA for unbalanced data: Use Type II instead.” Statistics and Computing 13: 163–67. https://doi.org/10.1023/A:1023260610025."
  },
  {
    "objectID": "anova2.html#type-i-平方和の方程式",
    "href": "anova2.html#type-i-平方和の方程式",
    "title": "多数群の比較：二元配置分散分析",
    "section": "Type I 平方和の方程式",
    "text": "Type I 平方和の方程式\n\n\\begin{split}\n\\overbrace{\\sum_{i=1}^I\\sum_{j=1}^J\\sum_{k=1}^K(x_{ijk} - \\overline{\\overline{x}})^2 }^{\\text{総平方和}\\;(SS_T)} =\n\\overbrace{JK\\sum_{i=1}^I(\\overline{x}_{i}-\\overline{\\overline{x}})^2}^{\\text{水準間平方和}\\;SS_A} +\n\\overbrace{IK\\sum_{j=1}^J(\\overline{x}_{j}-\\overline{\\overline{x}})^2}^{\\text{水準間平方和}\\;SS_B} \\\\ +\n\\underbrace{K\\sum_{i=1}^I\\sum_{j=1}^J(\\overline{x}_{ij} + \\overline{\\overline{x}})^2}_{\\text{相互作用平方和}\\;SS_{AB}} +\n\\underbrace{\\sum_{i=1}^I\\sum_{j=1}^J\\sum_{k=1}^K(x_{ijk} - \\overline{x}_{ij})^2}_{\\text{残渣平方和}\\;SS_R}\n\\end{split}\n\n\\bar{x}_i is the sample mean (標本平均) and \\bar{\\bar{x}} is the global mean (総平均)."
  },
  {
    "objectID": "anova2.html#二元配置分散分析",
    "href": "anova2.html#二元配置分散分析",
    "title": "多数群の比較：二元配置分散分析",
    "section": "二元配置分散分析",
    "text": "二元配置分散分析\n分散分析の結果。\n\nfullmodel_treatment = lm(Weight ~ pH + Calluna + pH:Calluna, data = festuca)\nanova(fullmodel_treatment)\n\nAnalysis of Variance Table\n\nResponse: Weight\n           Df  Sum Sq Mean Sq F value     Pr(>F)    \npH          1 19.9800 19.9800 28.1792 0.00007065 ***\nCalluna     1 10.2102 10.2102 14.4001    0.00159 ** \npH:Calluna  1  5.3976  5.3976  7.6126    0.01397 *  \nResiduals  16 11.3446  0.7090                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nモデル係数の結果。\n\nsummary(fullmodel_treatment)\n\n\nCall:\nlm(formula = Weight ~ pH + Calluna + pH:Calluna, data = festuca)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.156 -0.603 -0.138  0.732  1.272 \n\nCoefficients:\n                       Estimate Std. Error t value    Pr(>|t|)    \n(Intercept)              3.3680     0.3766   8.944 0.000000127 ***\npHpH5.5                  3.0380     0.5326   5.705 0.000032562 ***\nCallunaPresent          -0.3900     0.5326  -0.732       0.475    \npHpH5.5:CallunaPresent  -2.0780     0.7531  -2.759       0.014 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.842 on 16 degrees of freedom\nMultiple R-squared:  0.7583,    Adjusted R-squared:  0.713 \nF-statistic: 16.73 on 3 and 16 DF,  p-value: 0.00003447\n\n\n因子ごとの比較は emmeans パッケージの emmeans() 関数でします。 object 引数に、処理するモデルを渡します。 formula には、A因子 と B因子をモデル式として、渡します。\n\nemmip(object = fullmodel_treatment, \n      formula = pH ~ Calluna)\n\n\n\n\n図で確認したと、ペアごとの比較をして、t値を求めます。 object 引数に、処理するモデルを渡します。\nB因子の水準内のペアごとの比較をしたい場合は、specs に pairwise ~ A因子 | B因子 を渡します。\n\nemmeans(object = fullmodel_treatment, specs = pairwise ~ pH | Calluna)\n\n$emmeans\nCalluna = Absent:\n pH    emmean    SE df lower.CL upper.CL\n pH3.5   3.37 0.377 16     2.57     4.17\n pH5.5   6.41 0.377 16     5.61     7.20\n\nCalluna = Present:\n pH    emmean    SE df lower.CL upper.CL\n pH3.5   2.98 0.377 16     2.18     3.78\n pH5.5   3.94 0.377 16     3.14     4.74\n\nConfidence level used: 0.95 \n\n$contrasts\nCalluna = Absent:\n contrast      estimate    SE df t.ratio p.value\n pH3.5 - pH5.5    -3.04 0.533 16  -5.705  <.0001\n\nCalluna = Present:\n contrast      estimate    SE df t.ratio p.value\n pH3.5 - pH5.5    -0.96 0.533 16  -1.803  0.0903\n\n\n全ペア毎の比較は、次の通りです。\n\nemmeans(object = fullmodel_treatment, specs = pairwise ~ pH : Calluna, adjust = \"tukey\") |> \n  summary(infer = T)\n\n$emmeans\n pH    Calluna emmean    SE df lower.CL upper.CL t.ratio p.value\n pH3.5 Absent    3.37 0.377 16     2.57     4.17   8.944  <.0001\n pH5.5 Absent    6.41 0.377 16     5.61     7.20  17.011  <.0001\n pH3.5 Present   2.98 0.377 16     2.18     3.78   7.908  <.0001\n pH5.5 Present   3.94 0.377 16     3.14     4.74  10.457  <.0001\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                      estimate    SE df lower.CL upper.CL t.ratio\n pH3.5 Absent - pH5.5 Absent      -3.04 0.533 16   -4.562   -1.514  -5.705\n pH3.5 Absent - pH3.5 Present      0.39 0.533 16   -1.134    1.914   0.732\n pH3.5 Absent - pH5.5 Present     -0.57 0.533 16   -2.094    0.954  -1.070\n pH5.5 Absent - pH3.5 Present      3.43 0.533 16    1.904    4.952   6.437\n pH5.5 Absent - pH5.5 Present      2.47 0.533 16    0.944    3.992   4.634\n pH3.5 Present - pH5.5 Present    -0.96 0.533 16   -2.484    0.564  -1.803\n p.value\n  0.0002\n  0.8827\n  0.7118\n  <.0001\n  0.0014\n  0.3080\n\nConfidence level used: 0.95 \nConf-level adjustment: tukey method for comparing a family of 4 estimates \nP value adjustment: tukey method for comparing a family of 4 estimates"
  },
  {
    "objectID": "anova2.html#釣り合い型データと直交性について",
    "href": "anova2.html#釣り合い型データと直交性について",
    "title": "多数群の比較：二元配置分散分析",
    "section": "釣り合い型データと直交性について",
    "text": "釣り合い型データと直交性について\n\n\n\n\n\n\n釣り合い型データ (balanced data)\n\n\n\n各因子水準のデータ数は同じであること\n\n\n\n\n\n\n\n\n直行性 (orthogonality)\n\n\n\n説明変数同士の内積 (inner product) はゼロと意味します。 ベクトルとした場合、ベクトル間の角度が 90°であること。 つまり、説明変数がお互いに相関していないこと。\n\n# Example of calculating the inner product of two 3d vectors.\n#| echo: false\n#| eval: false\nk  = c(rnorm(3))\nx1 = c(rnorm(2),0)\nx1 = x1 - as.vector(x1 %*% k) * k / sqrt(sum(k^2))^2\nx2 = pracma::cross(k,x1)\n\n\nggplot() +\n  geom_segment(aes(x = 0, y = 0, xend = 1, yend = 1)) +\n  geom_segment(aes(x = 0, y = 0, xend = -1, yend = 1)) +\n  geom_text(aes(x = 1, y =1, label = \"(1,1)\"), vjust = 0) +\n  geom_text(aes(x = -1, y =1, label = \"(-1,1)\"), vjust = 0) +\n  geom_label(aes(x = 0, y = sqrt(2*(0.5^2)), label = \"90°\")) +\n  geom_curve(aes(x = 0.5, y = 0.5, xend = -0.5, yend = 0.5),\n             arrow = arrow(ends = \"both\", type = \"closed\")) +\n  scale_x_continuous(expand = expansion(add = 0.2)) +\n  scale_y_continuous(expand = expansion(add = 0.2)) +\n  coord_equal()\n\n\n\n\n著効性のあるベクトルのペア\n\n\n\n\n説明変数の係数 x_1 と x_2 の内積がゼロになること。\n\n\\begin{aligned}\nx_1 &= \\begin{bmatrix}\n-1 & 1\n\\end{bmatrix} \\\\\nx_2 &= \\begin{bmatrix}\n1 & 1\n\\end{bmatrix}\n\\end{aligned}\n\n\nx_1^T \\cdot x_2 = \\begin{bmatrix}\n-1 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n1 \\\\ 1\n\\end{bmatrix} = (-1 \\times 1) + (1 \\times 1) = 0\n\n\n\n\n\n`summarise()` has grouped output by 'pH'. You can override using the `.groups`\nargument.\n`summarise()` has grouped output by 'name'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n釣り合い型データではないとき、Type-I 平方和を用いたとき,　第2種の誤りを起こすことが高くなります。つまり、誤って帰無仮説を採択することが増えます。\n\n\n\n\nつまり、因子の水準毎のデータ数が異なるとき、係数のデフォルト比 (contr.treatment) と デフォルトの平方和 (Type-I) の解析は誤りです。\n\n\n`summarise()` has grouped output by 'pH'. You can override using the `.groups`\nargument.\n\n\n解析用のデータ数の内訳。\n\ndset |> \n  group_by(pH, Calluna) |> \n  summarise(N = length(Weight))\n\n`summarise()` has grouped output by 'pH'. You can override using the `.groups`\nargument.\n\n\n# A tibble: 4 × 3\n# Groups:   pH [2]\n  pH    Calluna     N\n  <fct> <fct>   <int>\n1 pH3.5 Absent      7\n2 pH3.5 Present     8\n3 pH5.5 Absent     12\n4 pH5.5 Present    14\n\n\nデフォルトの平方和と比較の場合、モデルに入れる因子の順序によってF値が変わります。\n\ncontrasts(dset$pH) = contr.treatment      # Required\ncontrasts(dset$Calluna) = contr.treatment\n# pH first, Calluna second.\nf1 = lm(Weight ~ pH * Calluna, data = dset)\n# Calluna first, pH second.\nf2 = lm(Weight ~ Calluna*pH, data = dset)\nanova(f1)\n\nAnalysis of Variance Table\n\nResponse: Weight\n           Df Sum Sq Mean Sq F value    Pr(>F)    \npH          1 39.924  39.924 102.720 3.178e-12 ***\nCalluna     1 26.948  26.948  69.334 5.252e-10 ***\npH:Calluna  1  8.193   8.193  21.081 4.947e-05 ***\nResiduals  37 14.381   0.389                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(f2)\n\nAnalysis of Variance Table\n\nResponse: Weight\n           Df Sum Sq Mean Sq F value    Pr(>F)    \nCalluna     1 26.623  26.623  68.498 6.089e-10 ***\npH          1 40.248  40.248 103.555 2.844e-12 ***\nCalluna:pH  1  8.193   8.193  21.081 4.947e-05 ***\nResiduals  37 14.381   0.389                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nデータ数が異なるとき、因子は直交ではないときは、Type-III 平方を使いましょう。"
  },
  {
    "objectID": "anova2.html#type-iiii-平方和の分散分析",
    "href": "anova2.html#type-iiii-平方和の分散分析",
    "title": "多数群の比較：二元配置分散分析",
    "section": "Type-IIII 平方和の分散分析",
    "text": "Type-IIII 平方和の分散分析\nType-I 以外の平方を使うとき、car パッケージが必要です。\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nさらに、比較は 必ず contr.sum にすること。\n\ncontrasts(dset$pH) = contr.sum      # Required\ncontrasts(dset$Calluna) = contr.sum # Required\n\n\nfullmodel_1 = lm(Weight ~ pH * Calluna, data = dset)\nfullmodel_2 = lm(Weight ~ Calluna * pH, data = dset)\n\n\nAnova(fullmodel_1, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: Weight\n            Sum Sq Df  F value    Pr(>F)    \n(Intercept) 683.36  1 1758.218 < 2.2e-16 ***\npH           42.64  1  109.711 1.278e-12 ***\nCalluna      17.94  1   46.163 5.336e-08 ***\npH:Calluna    8.19  1   21.081 4.947e-05 ***\nResiduals    14.38 37                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAnova(fullmodel_2, type = \"III\")\n\nAnova Table (Type III tests)\n\nResponse: Weight\n            Sum Sq Df  F value    Pr(>F)    \n(Intercept) 683.36  1 1758.218 < 2.2e-16 ***\nCalluna      17.94  1   46.163 5.336e-08 ***\npH           42.64  1  109.711 1.278e-12 ***\nCalluna:pH    8.19  1   21.081 4.947e-05 ***\nResiduals    14.38 37                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nモデルに入れる因子の順序が変わっても、結果は同じです。"
  },
  {
    "objectID": "glm.html",
    "href": "glm.html",
    "title": "線形モデル",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.4.0 \n✔ readr   2.1.2      ✔ forcats 0.5.1 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(car)\n\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nlibrary(patchwork)"
  },
  {
    "objectID": "glm.html#線形モデルとは",
    "href": "glm.html#線形モデルとは",
    "title": "線形モデル",
    "section": "線形モデルとは？",
    "text": "線形モデルとは？\n\n線形モデルの 線型 (linear) は直線 (straight line) と全く関係ないです。\n線型モデルの線型は 線型結合 (linear combination) を意味しています。\n\n\ny = b_1 x_1 + b_2 x_2 + b_3 x_3 + \\cdots + b_n x_n\n b_i は係数，x_i はベクトル（変数, 説明変数）です。y が x_i の線型結合です。"
  },
  {
    "objectID": "glm.html#線型モデルは直線じゃなくてもいい",
    "href": "glm.html#線型モデルは直線じゃなくてもいい",
    "title": "線形モデル",
    "section": "線型モデルは直線じゃなくてもいい",
    "text": "線型モデルは直線じゃなくてもいい\n線形モデル\n\n\\mu = \\beta_0 + \\beta_1 x_1\n\\mu = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2^2\n\\mu = \\beta_0 + \\beta_1\\exp(x_1)\n\n非線形モデル\n\n\\mu = \\frac{\\beta_1 x_1}{\\beta_2 + x_1}\n\\mu = \\beta_1 \\left(1-\\exp(\\beta_2 x_1)\\right) + \\beta_3\n\\mu = \\beta_0 + \\beta_1\\exp(\\beta_2 x_1)\n\n線形モデルは直線じゃなくてもいい。 ただし，変数 (x_i) は線型結合であることが重要なポイントです。 では、検討しているモデルは線形モデルかどうかを確認したいなら、パラメータに対してモデルの偏微分方程式を求めてください。\nたとえば、モデルパラメータ (\\beta_i) に対して，\\muを微分します。\n\n\\mu = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2^2\n\nパラメータの数ほど微分方程式があります。\n\n\\frac{\\partial \\mu}{\\partial \\beta_0}  = 1 \\qquad\n\\frac{\\partial \\mu}{\\partial \\beta_1}  = x_1 \\qquad\n\\frac{\\partial \\mu}{\\partial \\beta_2}  = x_2\n\n\\beta_i はどの微分方程式に残らないので，このモデルは線形モデルでしょう。\n次のモデルは非線形モデルです。\n\n\\mu = \\frac{\\beta_1 x_1}{\\beta_2 + x_1}\n\nモデルパラメータは \\beta_1, \\beta_2 ですね。 ではパラメータに対する偏微分方程式は次の通りです。\n\n\\begin{aligned}\n\\frac{\\partial \\mu}{\\partial \\beta_1}  &= \\frac{x_1}{\\beta_2 x_1} \\\\\n\\frac{\\partial \\mu}{\\partial \\beta_2}  &= -\\frac{x_1}{(\\beta_2 x_2)^2}\n\\end{aligned}\n\nパラメータはお互いの方程式に残るので，このモデルは線形モデルではないですね。"
  },
  {
    "objectID": "glm.html#線形モデルの仮定",
    "href": "glm.html#線形モデルの仮定",
    "title": "線形モデル",
    "section": "線形モデルの仮定",
    "text": "線形モデルの仮定\n線形モデルを用いるときに守るべき仮定は分散分析と同じです。\n\n残差が正規分布に従うこと。\n残差またはデータはお互いに独立し，同一分布から発生していること。\n\nモデルに対する注意する点\n\n説明変数もお互いに関係性（相関関係）が低いこと（関係ないほうがいい）。\n\n関係性が高いとき，モデルパラメータの推定量の精度 (precision) と正確度 (accuracy) が落ちます。\nこの現象は多重共線性 (multicollinearity) とよびます。\n\n説明変数はお互いに関係がなければ，直交性 (orthogonal) のあるモデルとなり，変数の推定量はお互いとの相関性がないです。\n\n野外データは上の条件を満たすことは珍しいが、すべての条件を満たさなくても解析はできます。 ただし、結果の解釈と考察には気をつけましょう。"
  },
  {
    "objectID": "glm.html#一般線形モデルの例-general-linear-model",
    "href": "glm.html#一般線形モデルの例-general-linear-model",
    "title": "線形モデル",
    "section": "一般線形モデルの例 (General Linear Model)",
    "text": "一般線形モデルの例 (General Linear Model)\n解析例にはアヤメのデータをつかっています。 ちなみに、一般線形モデルは一般化線形モデル (Generalized Linear Model, GLM) の特例です。\n\niris = iris |> as_tibble()\niris |> print(n = 3)\n\n# A tibble: 150 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n1          5.1         3.5          1.4         0.2 setosa \n2          4.9         3            1.4         0.2 setosa \n3          4.7         3.2          1.3         0.2 setosa \n# … with 147 more rows\n\n\n検討する線型モデルは次の通りです。\n\nE(\\text{Petal Length}) = b_0 + b_1\\text{Petal Width} + b_2\\text{Sepal Length} + b_3\\text{Sepal Width}\n\nE(\\text{Petal Length}) は花びらの長さの期待値 (expected value) といいます。\n線形モデルなので，lm() 関数で解析できます。 さらに、分散分析表も存在します。\n\nm1 = lm(Petal.Length ~ Petal.Width + Sepal.Length + Sepal.Width, data = iris)\n\n分散分析表は summary.aov() でだします。\n\nm1 |> summary.aov() |> print(signif.stars = F)\n\n              Df Sum Sq Mean Sq F value Pr(>F)\nPetal.Width    1  430.5   430.5 4231.49 <2e-16\nSepal.Length   1    9.9     9.9   97.74 <2e-16\nSepal.Width    1    9.0     9.0   88.95 <2e-16\nResiduals    146   14.9     0.1               \n\n\nモデルに入れた全ての説明変数のP値は < 0.0001 ですね。\nでは、線形モデルの係数表を出してみましょう。\n\nm1 |> summary() |> print(signif.stars = F)\n\n\nCall:\nlm(formula = Petal.Length ~ Petal.Width + Sepal.Length + Sepal.Width, \n    data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.99333 -0.17656 -0.01004  0.18558  1.06909 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)  -0.26271    0.29741  -0.883    0.379\nPetal.Width   1.44679    0.06761  21.399   <2e-16\nSepal.Length  0.72914    0.05832  12.502   <2e-16\nSepal.Width  -0.64601    0.06850  -9.431   <2e-16\n\nResidual standard error: 0.319 on 146 degrees of freedom\nMultiple R-squared:  0.968, Adjusted R-squared:  0.9674 \nF-statistic:  1473 on 3 and 146 DF,  p-value: < 2.2e-16\n\n\nPetal.Length に対して，Petal.Width と Sepal.Length は正の効果があり，Sepal.Width とは負の効果がありました。 つまり， Petal.Width と Sepal.Length が上昇すると， Petal.Length も上昇します。\n分散分析表の場合は、それぞれの変数に対するF値がでたが、係数表の場合は t値 がでました。 当然それぞれの値も異なった。\n何が違うのか？\nまず、分散分析の平方和って、さまざま求め方がるあることに気づきましょう。\n\nType-I Sum-of-squares\n\nSS(A), SS(B|A), SS(AB|B, A)\n\nType-II Sum-of-squares\n\nSS(A|B), SS(B|A)\n\nType-III Sum-of-squares\n\nSS(A|B), SS(B|A), SS(AB|B,A)\n\n\n他にもありますが、この 3 つが一般的にです。\nType-I の場合、結果は変数の順序に依存します。 Type-II の場合、順序に依存しないが、相互作用はない。 Type-III の場合、順序に依存しないが、相互作用の影響も計算されます。\nType-I はRのデフォルトなので、Type-II または Type-II を使いたいなら、 car パッケージの Anova() 関数が必要です。\nといくおとで、説明変数の順序をかえるた、次のように異なる結果が返ってきます。\n\nm1a  = lm(Petal.Length ~ Petal.Width + Sepal.Length + Sepal.Width, data = iris)\nm1b = lm(Petal.Length ~ Petal.Width + Sepal.Width + Sepal.Length, data = iris)\nm1c = lm(Petal.Length ~ Sepal.Width + Petal.Width + Sepal.Length, data = iris)\nm1a |> summary.aov()\n\n              Df Sum Sq Mean Sq F value Pr(>F)    \nPetal.Width    1  430.5   430.5 4231.49 <2e-16 ***\nSepal.Length   1    9.9     9.9   97.74 <2e-16 ***\nSepal.Width    1    9.0     9.0   88.95 <2e-16 ***\nResiduals    146   14.9     0.1                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm1b |> summary.aov()\n\n              Df Sum Sq Mean Sq F value   Pr(>F)    \nPetal.Width    1  430.5   430.5 4231.49  < 2e-16 ***\nSepal.Width    1    3.1     3.1   30.37 1.57e-07 ***\nSepal.Length   1   15.9    15.9  156.31  < 2e-16 ***\nResiduals    146   14.9     0.1                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm1c |> summary.aov()\n\n              Df Sum Sq Mean Sq F value Pr(>F)    \nSepal.Width    1   85.2    85.2   837.8 <2e-16 ***\nPetal.Width    1  348.3   348.3  3424.1 <2e-16 ***\nSepal.Length   1   15.9    15.9   156.3 <2e-16 ***\nResiduals    146   14.9     0.1                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nこのとき、car パッケージの Anova() 関数を使って平方和の求め方を指定します。 相互作用なしのモデルなので、Type-II を指定しました。 相互作用も調べたいなら、type=\"III\" を渡してください。\n\nm1a |> Anova(type = \"II\")\n\nAnova Table (Type II tests)\n\nResponse: Petal.Length\n             Sum Sq  Df F value    Pr(>F)    \nPetal.Width  46.584   1 457.905 < 2.2e-16 ***\nSepal.Length 15.902   1 156.312 < 2.2e-16 ***\nSepal.Width   9.049   1  88.947 < 2.2e-16 ***\nResiduals    14.853 146                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm1b |> Anova(type = \"II\")\n\nAnova Table (Type II tests)\n\nResponse: Petal.Length\n             Sum Sq  Df F value    Pr(>F)    \nPetal.Width  46.584   1 457.905 < 2.2e-16 ***\nSepal.Width   9.049   1  88.947 < 2.2e-16 ***\nSepal.Length 15.902   1 156.312 < 2.2e-16 ***\nResiduals    14.853 146                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nm1c |> Anova(type = \"II\")\n\nAnova Table (Type II tests)\n\nResponse: Petal.Length\n             Sum Sq  Df F value    Pr(>F)    \nSepal.Width   9.049   1  88.947 < 2.2e-16 ***\nPetal.Width  46.584   1 457.905 < 2.2e-16 ***\nSepal.Length 15.902   1 156.312 < 2.2e-16 ***\nResiduals    14.853 146                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n係数表は t値の結果は Wald’s test と呼びます。\n\nm1 |> summary()\n\n\nCall:\nlm(formula = Petal.Length ~ Petal.Width + Sepal.Length + Sepal.Width, \n    data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.99333 -0.17656 -0.01004  0.18558  1.06909 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -0.26271    0.29741  -0.883    0.379    \nPetal.Width   1.44679    0.06761  21.399   <2e-16 ***\nSepal.Length  0.72914    0.05832  12.502   <2e-16 ***\nSepal.Width  -0.64601    0.06850  -9.431   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.319 on 146 degrees of freedom\nMultiple R-squared:  0.968, Adjusted R-squared:  0.9674 \nF-statistic:  1473 on 3 and 146 DF,  p-value: < 2.2e-16\n\n\nこのとき、\\beta = 0 の帰無仮説を検証しています。 つまり、係数の値は 0 か 0 じゃないかですね。\nWald’s test と F-test のこちらもモデルの解析に使えます。 ところが、一般的には、モデル選択に F-test を使いますが、 係数が 0 か 0 じゃないかの検証には Wald’s test を使います。 ちなみに、このモデルの場合、(t_\\nu)^2 = F_{(1,\\nu)} なので、 係数表のt値の2乗は Type-II 分散分析のF値と同じです。"
  },
  {
    "objectID": "glm.html#モデル診断",
    "href": "glm.html#モデル診断",
    "title": "線形モデル",
    "section": "モデル診断",
    "text": "モデル診断\nモデルを当てはめたあと、最も重要な解析はモデル残渣の確認です。 モデル残渣に異常があると、モデルとデータの相性が悪いです。 モデルとデータが合わない場合、求めた係数を信用できません。\n一般線形モデルのとき、標準化残渣 (standardized residuals) をもとめて、モデルの良さを目で確認します。 つまり、標準化残渣を様々形の図を作ります。\nまず、残渣を持てめて、図に使うデータを準備します。\n\nfiris = fortify(m1) |> as_tibble()\nfiris\n\n# A tibble: 150 × 10\n   Petal…¹ Petal…² Sepal…³ Sepal…⁴   .hat .sigma .cooksd .fitted  .resid .stdr…⁵\n     <dbl>   <dbl>   <dbl>   <dbl>  <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1     1.4     0.2     5.1     3.5 0.0205  0.320 3.73e-4    1.48 -0.0842 -0.267 \n 2     1.4     0.2     4.9     3   0.0212  0.319 3.71e-3    1.66 -0.261  -0.828 \n 3     1.3     0.2     4.7     3.2 0.0201  0.320 3.84e-4    1.39 -0.0864 -0.274 \n 4     1.5     0.2     4.6     3.1 0.0221  0.320 8.46e-4    1.38  0.122   0.387 \n 5     1.4     0.2     5       3.6 0.0230  0.320 1.68e-4    1.35  0.0533  0.169 \n 6     1.7     0.4     5.4     3.9 0.0327  0.320 9.86e-5    1.73 -0.0339 -0.108 \n 7     1.4     0.3     4.6     3.4 0.0255  0.320 3.34e-4    1.33  0.0711  0.226 \n 8     1.5     0.2     5       3.4 0.0189  0.320 2.81e-5    1.48  0.0241  0.0763\n 9     1.4     0.2     4.4     2.9 0.0293  0.320 1.14e-4    1.36  0.0386  0.123 \n10     1.5     0.1     4.9     3.1 0.0225  0.320 1.32e-4    1.45  0.0479  0.152 \n# … with 140 more rows, and abbreviated variable names ¹​Petal.Length,\n#   ²​Petal.Width, ³​Sepal.Length, ⁴​Sepal.Width, ⁵​.stdresid\n\n\n\n残渣の正規性\nまずは残渣は正規分布に従うかを評価します。 正規性はヒストグラムとQQプロットでします。 残渣が正規分布に従うなら、ヒストグラムは正規分布に似ていて、 QQプロットでは残渣を示す点が赤色の直線上に並びます。\n\nggplot(firis) + \n  geom_histogram(aes(x = .stdresid, y = ..density..)) +\n  stat_function(fun = dnorm, color = \"red\") +\n  labs(x = \"Standardized residual\", y = \"\") \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n残渣のヒストグラムと正規分布\n\n\n\n\n\nggplot(firis) + \n  geom_qq(aes(sample = .stdresid)) +\n  geom_abline(color = \"red\") +\n  labs(x = \"Theoretical Quantile\", y = \"Standardized residual\") \n\n\n\n\nQQプロット\n\n\n\n\nどちらの図を確認しても、標準化残渣は正規分布に従っているように見えます。\n\n\n残渣のばらつき\n正規性に問題がなければ、次は標準化残渣のばらつきを確認したいです。 すべての変数に対して残渣の図をつくります。 また、求めた期待値に対しても残渣を確認します。 残渣になんかしらのパタンがあると、問題です。\n\nfiris |> \n  select(matches(\"Petal|Sepal\"), .stdresid) |> \n  pivot_longer(matches(\"Petal|Sepal\")) |> \n  ggplot() + \n  geom_point(aes(x = value, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  facet_wrap(vars(name), scales = \"free\")\n\n\n\n\n標準化残渣と応答変数、それぞれの説明変数との関係\n\n\n\n\n残渣は 0 をまたいで均等にばらついていることが確認できました。 さらに、ばらつきに明確なパタンがないので、この点についてモデルには問題なさそうです。 次は残渣と期待値の関係を確認します。\n\nfiris |> \n  select(.fitted, .stdresid) |> \n  ggplot() + \n  geom_point(aes(x = .fitted, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") \n\n\n\n\n標準化残渣と期待値の関係\n\n\n\n\n期待値と標準化残渣の関係を確認すると、特に問題はないですね。 場合によって、標準化残渣の絶対値の平方根で確認しやすい場合もあります。 この解析の場合は不必要ですが、コードと結果は次の通りです。\n\nfiris |> \n  select(.fitted, .stdresid) |> \n  ggplot() + \n  geom_point(aes(x = .fitted, y = sqrt(abs(.stdresid)))) \n\n\n\n\n標準化残渣の平方根と期待値の関係\n\n\n\n\n最後に、クックの距離を確認します。クックの距離 (Cook’s distance) はモデルの当てはめに強く影響する値を検出してくれます。 データ点のクックの距離が P(F_{(n, n-p)}=0.5) を超えた場合、ちょっと怪しいかもしれないです。\n\ndof = summary(m1) |> pluck(\"df\")\nthold = qf(0.5, dof[1], dof[2])\nfiris |> \n  mutate(n = 1:n()) |> \n  mutate(above = ifelse(.cooksd > thold, T, F)) |> \n  ggplot() + \n  geom_point(aes(x = n, y = .cooksd, color = above)) + \n  geom_hline(yintercept = thold, color = \"red\", linetype = \"dashed\")\n\n\n\n\n診断図から問題ないと判断できたら、説明変数の多重鏡線性を確認しましょう。 まず，説明変数がお互いに相関があるかを確認します。\n\niris |> select(-Species, - Petal.Length) |> cor()\n\n             Sepal.Length Sepal.Width Petal.Width\nSepal.Length    1.0000000  -0.1175698   0.8179411\nSepal.Width    -0.1175698   1.0000000  -0.3661259\nPetal.Width     0.8179411  -0.3661259   1.0000000\n\n\nPetal.Width と Sepal.Length の相関は高いですが，他は 0 に近いので相関関係は低いです。 多重共線性をしっかりと確認したければ，分散拡大係数 (Variance Inflation Factor; VIF) をもとめます。\nVIF は決定係数 (R_i^2) をつかって計算するので，方程式は次の通りです。\n\n\\text{VIF} = \\frac{1}{1-R_i^2}\n\nVIF を計算するには，説明変数 x_i を他の説明変数 x_{j \\neq i} との線形モデル組み立てて，決定係数を求める必要があります。\nつまり，先ほどの相関係数の結果をつかうと，\n\nx1 = lm(Petal.Width ~ Sepal.Length + Sepal.Width, \n        data = iris) |> summary() |> pluck(\"r.squared\")\nx2 = lm(Sepal.Length ~ Petal.Width + Sepal.Width, \n        data = iris) |> summary() |> pluck(\"r.squared\")\nx3 = lm(Sepal.Width ~ Sepal.Length + Petal.Width, \n        data = iris) |> summary() |> pluck(\"r.squared\")\n1 / (1-c(x1,x2,x3)) # VIF\n\n[1] 3.889961 3.415733 1.305515\n\n\ncar パッケージの vif() 関数の方が便利です。\n\ncar::vif(m1)\n\n Petal.Width Sepal.Length  Sepal.Width \n    3.889961     3.415733     1.305515 \n\n\nVIF(\\beta_i) > 10 であれば，多重共線性の問題は大きいと考えられます。 このときの決定係数は 1-1/10 = 0.90 です。"
  },
  {
    "objectID": "glm.html#ガラパゴス諸島における種数の解析",
    "href": "glm.html#ガラパゴス諸島における種数の解析",
    "title": "線形モデル",
    "section": "ガラパゴス諸島における種数の解析",
    "text": "ガラパゴス諸島における種数の解析\niris データの解析に大きな問題がなかったので、あまり参考にならなかったので、 ガラパゴス諸島のデータを解析してみましょう。\nfaraway パッケージの gala を解析します。 gala にはガラパゴス諸島の生態系に対しての 7 つの変数があります。Species は植物の種数，Endemics は植物の固有種， Area は島の平面積 (m2)，Elevation は島の最も高い場所 (m)，Nearest は最も近い島からの距離 (km)，Scruz はサンタクルス島からの距離 (km)，Adjacent は最も近い島の面積 (m2)です。\n\ndata(gala, package = \"faraway\")\ngala = gala |> as_tibble() # tibble に変換\ngala |> print(n = 3) # 最初の 3 行を表示\n\n# A tibble: 30 × 7\n  Species Endemics  Area Elevation Nearest Scruz Adjacent\n    <dbl>    <dbl> <dbl>     <dbl>   <dbl> <dbl>    <dbl>\n1      58       23 25.1        346     0.6   0.6     1.84\n2      31       21  1.24       109     0.6  26.3   572.  \n3       3        3  0.21       114     2.8  58.7     0.78\n# … with 27 more rows\n\n\n解析する前に、データの可視化をします。\n\n\n\n\n\n説明変数 (explanatory variable)、または予測子 (Predictor) に対する種数の変動は予測子によって変わることがわかります。\n\ngala_out = gala |> select(-Endemics) |> gather(Variable, Predictor, -Species)\nggplot(gala_out) + geom_point(aes(x=Predictor, y=Species)) + facet_wrap(\"Variable\", scales = \"free_x\")\n\nでは、モデルと帰無仮設を決めます。\n「種数の増減は予測子に依存する」を作業仮説にしたとき，モデルは次のようになります。\n\nE(\\text{Species}) = b_0 + b_1\\text{Area}+b_2\\text{Elevation}+b_3\\text{Nearest}+b_4\\text{Scruz}+b_5\\text{Adjacent}\n\nネイマン＝ピアソン (Neyman-Pearson) の枠組みの中で解析するなら，帰無仮設と対立仮設を建てなければなりません。\n\n帰無仮設： b_0 = b_1 = b_2 = b_3 = b_4 = b_5\n対立仮説： b_0 \\neq b_1 \\neq b_2 \\neq b_3 \\neq b_4 \\neq b_5\n\n解析は次の通りです。 この度、帰無仮説のモデルもわさわさくみました。\n\nH0 = lm(Species ~ 1, data = gala)  # これが帰無仮設のモデルです。\nHF = lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data = gala) # これが対立仮説のモデルです。\n\n相互作用を入れていないが、Type-III SS を求めます。\n\n# anova(H0, HF) # Type-I SS\nAnova(H0, HF, type = \"III\")   # Type=III SS\n\nAnova Table (Type III tests)\n\nResponse: Species\n            Sum Sq Df F value    Pr(>F)    \n(Intercept) 217942  1  58.618 6.805e-08 ***\nResiduals    89231 24                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nP値は < 0.0001 なので，0.05 より小さいです。帰無仮設は棄却できます。 帰無仮設を棄却したら，採択するモデル（モデルは採択できるが，仮説は採択できません）の 診断図を作図して，残差のばらつきや正規性などを確認します。\nでは、HF の診断図を確認しましょう。\n\nfgala = fortify(HF)\np1 = ggplot(fgala) + \n  geom_histogram(aes(x = .stdresid, y = ..density..)) +\n  stat_function(fun = dnorm, color = \"red\") +\n  labs(x = \"Standardized residual\", y = \"\") \np2 = ggplot(fgala) + \n  geom_qq(aes(sample = .stdresid)) +\n  geom_abline(color = \"red\") +\n  labs(x = \"Theoretical Quantile\", y = \"Standardized residual\") \np1 | p2\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nデータ数は少ないが，残差は 0 を中心にしていて左右対称です。 殆どの残差はQQプロットの直線に沿っています。 よって，残差の正規性に大きな問題はなさそうです。\n\nfgala |> \n  select(Species, Area, Elevation, Nearest, Scruz, Adjacent, .stdresid) |> \n  pivot_longer(-.stdresid) |> \n  ggplot() + \n  geom_point(aes(x = value, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  facet_wrap(vars(name), scales = \"free\")\n\n\n\n\n予測子に対する残差の分布を確認すると，均等に分布していないように見えます。 例えば Residual vs. Nearest の場合，波状を描いているように見えます。"
  },
  {
    "objectID": "glm.html#モデルの診断図期待値に対する残差のばらつき",
    "href": "glm.html#モデルの診断図期待値に対する残差のばらつき",
    "title": "線形モデル",
    "section": "モデルの診断図：期待値に対する残差のばらつき",
    "text": "モデルの診断図：期待値に対する残差のばらつき\n\np1 = fgala |> \n  select(.fitted, .stdresid) |> \n  ggplot() + \n  geom_point(aes(x = .fitted, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") \np2 = fgala |> \n  select(.fitted, .stdresid) |> \n  ggplot() + \n  geom_point(aes(x = .fitted, y = sqrt(abs(.stdresid)))) \np1 | p2\n\n\n\n\n残差のばらつきは期待値と関係性が有るように見えます (Residual vs. Fitted Values Plot)。 Scale–Location Plot では，その関係が明確です。\n次は異常値を探してみましょう。\n\ndof = summary(HF) |> pluck(\"df\")\nthold = qf(0.5, dof[1], dof[2])\nfgala |> \n  mutate(n = 1:n()) |> \n  mutate(above = ifelse(.cooksd > thold, T, F)) |> \n  ggplot() + \n  geom_point(aes(x = n, y = .cooksd, color = above)) + \n  geom_hline(yintercept = thold, color = \"red\", linetype = \"dashed\")\n\n\n\n\nクックの距離が P(F_{(p, n-p)}=0.5) を超えれば，影響力の高い点だと考えられます。 このとき，16番目のデータが明らかに超えています。\nP(F_{(p, n-p)}=0.5)は自由度 p (パラメータの数) と n-p (データ数からパラメータ数の差) のときのF値の中央値です。\n\nwhich(cooks.distance(HF) > thold)\n\n16 \n16 \n\n\nいろいろと問題があったので、モデルの改良は必要ですね。 ところが、モデルを改良するときの問題点を意識してください。\n帰無仮設は棄却できたが，診断図を確認すると多数の問題点がありました。 このようなとき，モデルを改良する必要があります。 ただし，ネイマン=ピアソンの帰無仮設検定法のとき，検討するモデルが増えれば増えるほど第１種の誤りを起こす確率も上がります。\n\n\n\n5 つの予測子（変数）があるので，交互作用ありの一次式のモデルの場合，パラメータの数は32 です。 検証できるパラメータ数はデータ数に制限されるので， パラメータ数とデータ数が等しいときのモデルは飽和モデルとよびます。 ちなみに，32 パラメータのときの第 1 種の誤りを起こす確率は 0.8063 です。 飽和モデルのとき，分散を推定することができないので，飽和モデルは理論上のモデルです。"
  },
  {
    "objectID": "glm.html#モデル構築の考え方",
    "href": "glm.html#モデル構築の考え方",
    "title": "線形モデル",
    "section": "モデル構築の考え方",
    "text": "モデル構築の考え方\nモデルを設計するときに最も大事なことは：\n\n統計解析の仮定を守る\n科学的・統計学的にありえる\n説明しやすい\nシンプル・単純である\n\nモデルの改良点：\n\n応答変数を変換する\n説明変数を変換する\n誤差項の分布を変える\nモデルパラメータを増やす（二次関数・三次関数・交互作用など）\n\n\n応答変数の変換\n残差に問題があるとき，応答変数を変換することが一般的に行われています。 応答変数の変換は，残差を正規分布に従わせるためにします。\nたとえば個体数はつねに y\\ge 0 です。負の個体数は存在しません。 このとき，正規分布を仮定したら，0 近辺の値の 95% 信頼区間は負の値をとることもあります。 実データとの整合性がとれなくなります。 または，0 から 1 の値しか取れないデータのときでも正規性に問題がでます。\n応答変数が個体数のような正の整数のとき，ログ変換することが多いです。log(x) または，0 から 1 の比率の様なデータのとき，アークサイン変換をします。 asin(x)"
  },
  {
    "objectID": "glm.html#応答変数変換した解析",
    "href": "glm.html#応答変数変換した解析",
    "title": "線形モデル",
    "section": "応答変数変換した解析",
    "text": "応答変数変換した解析\n応答変数をログ変換した解析と変換していない解析の結果はつぎの通りです。\nログ変換後の分散分析表\n\ngala = gala |> mutate(logSpecies = log(Species))\nlogHF = lm(logSpecies~ Area + Elevation + Nearest + Scruz + Adjacent, data = gala)\nlogHF |> Anova(type = \"III\") |> print(signif.stars = F)\n\nAnova Table (Type III tests)\n\nResponse: logSpecies\n            Sum Sq Df F value    Pr(>F)\n(Intercept) 53.956  1 48.7039 3.238e-07\nArea         3.998  1  3.6088   0.06955\nElevation   26.168  1 23.6211 5.927e-05\nNearest      0.918  1  0.8289   0.37164\nScruz        1.217  1  1.0984   0.30505\nAdjacent     7.597  1  6.8575   0.01505\nResiduals   26.588 24                  \n\n\n変換なしの分散分析表\n\nHF |> Anova(type = \"III\") |> print(signif.stars = F)\n\nAnova Table (Type III tests)\n\nResponse: Species\n            Sum Sq Df F value    Pr(>F)\n(Intercept)    506  1  0.1362 0.7153508\nArea          4238  1  1.1398 0.2963180\nElevation   131767  1 35.4404 3.823e-06\nNearest          0  1  0.0001 0.9931506\nScruz         4636  1  1.2469 0.2752082\nAdjacent     66406  1 17.8609 0.0002971\nResiduals    89231 24                  \n\n\n変数（パラメータ）に対するP値はが変わりました。 変換なしの解析に比べて，ログ変換後のElevation と Nearest のP値は下がりましたが，その他のP値は上がりました。\n\nfgala = fortify(logHF)\np1 = ggplot(fgala) + \n  geom_histogram(aes(x = .stdresid, y = ..density..)) +\n  stat_function(fun = dnorm, color = \"red\") +\n  labs(x = \"Standardized residual\", y = \"\") \np2 = ggplot(fgala) + \n  geom_qq(aes(sample = .stdresid)) +\n  geom_abline(color = \"red\") +\n  labs(x = \"Theoretical Quantile\", y = \"Standardized residual\") \np1 | p2\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nログ変換なしの結果よりちょっと良くなったと思います。\n\nfgala |> \n  select(logSpecies, Area, Elevation, Nearest, Scruz, Adjacent, .stdresid) |> \n  pivot_longer(-.stdresid) |> \n  ggplot() + \n  geom_point(aes(x = value, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  facet_wrap(vars(name), scales = \"free\")\n\n\n\n\n説明変数が上昇すると残差のばらつきが減少する傾向があるので，残差のばらつきに問題があります。\n\np1 = fgala |> \n  select(.fitted, .stdresid) |> \n  ggplot() + \n  geom_point(aes(x = .fitted, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") \np2 = fgala |> \n  select(.fitted, .stdresid) |> \n  ggplot() + \n  geom_point(aes(x = .fitted, y = sqrt(abs(.stdresid)))) \np1 | p2\n\n\n\n\n期待値に対しても，残差のばらつきの均一性が問題です。さらに Scale - Location Plot には明らか傾向があります。\n\ndof = summary(logHF) |> pluck(\"df\")\nthold = qf(0.5, dof[1], dof[2])\nfgala |> \n  mutate(n = 1:n()) |> \n  mutate(above = ifelse(.cooksd > thold, T, F)) |> \n  ggplot() + \n  geom_point(aes(x = n, y = .cooksd, color = above)) + \n  geom_hline(yintercept = thold, color = \"red\", linetype = \"dashed\")\n\n\n\n\n\nwhich(cooks.distance(logHF) > thold)\n\n16 \n16 \n\n\n16番目のデータに対して，クックの距離は下がったが，PF_{(n, n-p)}=0.5) 以上のままなので，課題としてのこります。\n\n応答変数以外の解決手法\n応答変数をログ変換しても問題は解決できませんでした。 次に検討することは説明変数の変換または、削除です。 まずは，VIF の高い変数から外してみみます。\n\ncar::vif(logHF)\n\n     Area Elevation   Nearest     Scruz  Adjacent \n 2.928145  3.992545  1.766099  1.675031  1.826403 \n\n\nElevation の値が一番高かったので，Elevation なしのモデルを再解析します。\n\nlogHF2 = lm(logSpecies~ Area + Nearest + Scruz + Adjacent, data = gala)\ncar::vif(logHF2)\n\n    Area  Nearest    Scruz Adjacent \n1.047496 1.669984 1.658583 1.073529 \n\n\n診断図を確認したら，結果は良くなかったので，Nearest か Scruz も外します。 種数は島と島の間の距離に依存するかもしれないが，一つの島（Santa Cruz島）との距離の影響は考えにくいので，Scruz を外します。"
  },
  {
    "objectID": "glm.html#単純化したモデル",
    "href": "glm.html#単純化したモデル",
    "title": "線形モデル",
    "section": "単純化したモデル",
    "text": "単純化したモデル\n結果として，つぎのモデルを解析することになりました。\n\nE(\\log(\\text{Species})) = b_0 + b_1\\text{Area}+b_2\\text{Nearest}+b_3\\text{Adjacent}\n\n\nlogSF = lm(logSpecies~ Area + Nearest + Adjacent, data = gala)\nlogSF |> Anova(type = \"III\") |> print(signif.stars = F)\n\nAnova Table (Type III tests)\n\nResponse: logSpecies\n             Sum Sq Df F value    Pr(>F)\n(Intercept) 159.281  1 74.7879 3.983e-09\nArea         13.201  1  6.1981   0.01951\nNearest       2.372  1  1.1139   0.30095\nAdjacent      0.180  1  0.0847   0.77335\nResiduals    55.374 26                  \n\n\nArea 以外の変数のP値は 0.05 より高いです。"
  },
  {
    "objectID": "glm.html#診断図",
    "href": "glm.html#診断図",
    "title": "線形モデル",
    "section": "診断図",
    "text": "診断図\n\nfgala = fortify(logSF)\np1 = ggplot(fgala) + \n  geom_histogram(aes(x = .stdresid, y = ..density..)) +\n  stat_function(fun = dnorm, color = \"red\") +\n  labs(title = \"Histogram\", x = \"Standardized residual\", y = \"\") \np2 = ggplot(fgala) + \n  geom_qq(aes(sample = .stdresid)) +\n  geom_abline(color = \"red\") +\n  labs(title = \"QQ plot\", x = \"Theoretical Quantile\", y = \"Standardized residual\") \np3 = fgala |> \n  select(logSpecies, Area, Nearest, Adjacent, .stdresid) |> \n  pivot_longer(-.stdresid) |> \n  ggplot() + \n  geom_point(aes(x = value, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Resid. vs. predictor\", x = \"Predictor value\", y = \"Standardized residual\") +\n  facet_wrap(vars(name), scales = \"free\")\np4 = fgala |> \n  select(.fitted, .stdresid) |> \n  ggplot() + \n  geom_point(aes(x = .fitted, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")  +\n  labs(title = \"Resid. vs. fit\", x = \"Fitted value\", y = \"Standardized residual\")\np5 = fgala |> \n  select(.fitted, .stdresid) |> \n  ggplot() + \n  geom_point(aes(x = .fitted, y = sqrt(abs(.stdresid))))  +\n  labs(title = \"Scale - location\", x = \"Fitted value\", y = \"Standardized residual\")\ndof = summary(logSF) |> pluck(\"df\")\nthold = qf(0.5, dof[1], dof[2])\np6 = fgala |> \n  mutate(n = 1:n()) |> \n  mutate(above = ifelse(.cooksd > thold, T, F)) |> \n  ggplot() + \n  geom_point(aes(x = n, y = .cooksd, color = above)) + \n  geom_hline(yintercept = thold, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Cook's distance\", x = \"Index\", y = \"Cook's distance\")\n\np1 + p2 + p3 + p4 + p5 + p6\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n残差のばらつきや正規性は良くなったが，クックの距離の問題が残っています。"
  },
  {
    "objectID": "glm.html#説明変数も変換する",
    "href": "glm.html#説明変数も変換する",
    "title": "線形モデル",
    "section": "説明変数も変換する",
    "text": "説明変数も変換する\nこんどは，説明変数も変換します。\n\nE(\\log(\\text{Species})) = b_0 + b_1\\log(\\text{Area})+b_2\\text{Nearest}+b_3\\log(\\text{Adjacent})\n\n\ngala = gala |> mutate(logSpecies = log(Species), logArea = log(Area), logAdjacent = log(Adjacent))\nlogSF2 = lm(logSpecies~ logArea + Nearest + logAdjacent, data = gala)\nlogSF2 |> Anova(type = \"III\") |> print(signif.stars = F)\n\nAnova Table (Type III tests)\n\nResponse: logSpecies\n             Sum Sq Df  F value    Pr(>F)\n(Intercept) 151.442  1 239.3574 1.247e-14\nlogArea      52.429  1  82.8653 1.445e-09\nNearest       0.618  1   0.9764    0.3322\nlogAdjacent   0.159  1   0.2512    0.6204\nResiduals    16.450 26                   \n\n\n\nfgala = fortify(logSF2)\np1 = ggplot(fgala) + \n  geom_histogram(aes(x = .stdresid, y = ..density..)) +\n  stat_function(fun = dnorm, color = \"red\") +\n  labs(title = \"Histogram\", x = \"Standardized residual\", y = \"\") \np2 = ggplot(fgala) + \n  geom_qq(aes(sample = .stdresid)) +\n  geom_abline(color = \"red\") +\n  labs(title = \"QQ plot\", x = \"Theoretical Quantile\", y = \"Standardized residual\") \np3 = fgala |> \n  select(logSpecies, logArea, Nearest, logAdjacent, .stdresid) |> \n  pivot_longer(-.stdresid) |> \n  ggplot() + \n  geom_point(aes(x = value, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") + \n  labs(title = \"Resid. vs. predictor\", x = \"Predictor value\", y = \"Standardized residual\") +\n  facet_wrap(vars(name), scales = \"free\")\np4 = fgala |> \n  select(.fitted, .stdresid) |> \n  ggplot() + \n  geom_point(aes(x = .fitted, y = .stdresid)) + \n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\")  +\n  labs(title = \"Resid. vs. fit\", x = \"Fitted value\", y = \"Standardized residual\")\np5 = fgala |> \n  select(.fitted, .stdresid) |> \n  ggplot() + \n  geom_point(aes(x = .fitted, y = sqrt(abs(.stdresid))))  +\n  labs(title = \"Scale - location\", x = \"Fitted value\", y = \"Standardized residual\")\ndof = summary(logSF) |> pluck(\"df\")\nthold = qf(0.5, dof[1], dof[2])\np6 = fgala |> \n  mutate(n = 1:n()) |> \n  mutate(above = ifelse(.cooksd > thold, T, F)) |> \n  ggplot() + \n  geom_point(aes(x = n, y = .cooksd, color = above)) + \n  geom_hline(yintercept = thold, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Cook's distance\", x = \"Index\", y = \"Cook's distance\")\n\np1 + p2 + p3 + p4 + p5 + p6\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n残差のばらつき，正規性，クックの距離の問題は解決できました。\n結果\n\nlogSF2 |> Anova(type = \"III\") |> print(signif.stars = F)\n\nAnova Table (Type III tests)\n\nResponse: logSpecies\n             Sum Sq Df  F value    Pr(>F)\n(Intercept) 151.442  1 239.3574 1.247e-14\nlogArea      52.429  1  82.8653 1.445e-09\nNearest       0.618  1   0.9764    0.3322\nlogAdjacent   0.159  1   0.2512    0.6204\nResiduals    16.450 26                   \n\n\n帰無仮設の有意性検定によると，logArea と Nearest の効果は有意ですが，logAdjacent の効果は有意じゃない。\nところが，この結果まで導くには，5 種類のモデルを検証しました。\\alpha_\\text{fwer} は 1 - (1-0.05)^5 = 0.2262 ですので，\\alpha_\\text{fwer}=0.05 にしたければ，\\alpha = 0.0102 に設定しなければなりません。このとき，logArea 以外の要因の効果は有意ではありません。"
  },
  {
    "objectID": "glm.html#一般化線形モデル",
    "href": "glm.html#一般化線形モデル",
    "title": "線形モデル",
    "section": "一般化線形モデル",
    "text": "一般化線形モデル\n一般化線形モデル (GLM) は今までの線型モデルと同じように，線型結合した変数から成り立っています。\nさらに，\n\n正規分布を仮定した線型モデルのとき，予測残差は Residuals (残差) とよびましたが，GLMの予測残差は Deviance (尤離度・逸脱度・デビアンス) とよびます。\n正規分布以外の指数型分布族にぞくする分布も使えます。\n一般化線形モデルに 3 つの成分が存在します。\n\n誤差構造またはランダム成分 (random component)\n線型予測子 (linear predictor)または系統成分(systematic component)\n連結関数またはリンク関数 (link function)"
  },
  {
    "objectID": "glm.html#指数型分布族誤差項",
    "href": "glm.html#指数型分布族誤差項",
    "title": "線形モデル",
    "section": "指数型分布族・誤差項",
    "text": "指数型分布族・誤差項\n\nf(y|\\theta, \\phi) = \\exp\\left(\\frac {y\\theta - b(\\theta)} {a(\\phi)} + c(y, \\phi)\\right)\n\n\\theta は正準パラメータ (canonical parameter)，\\phi はばらつきのパラメータ (dispersion parameter, 分散パラメータ)，a(\\cdot), b(\\cdot), c(\\cdot) は存知の関数です。さらに，平均値は \\mu = E(y) = b'(\\theta)，分散は var(y) = a(\\phi) b''(\\theta) です。つまり，平均値は \\theta の関数できまるが，分散は二つの関数の積です。a(\\phi) = \\phi/w であれば，v(y) = \\phi b''(\\theta)/w であり，分散関数とよびます。正規分布の場合，分散と平均値は独立しているのと，\\phi=1 なので，var(y) = 1/w です。w は存知の重みです。\n\n線形予測子または系統成分\n応答変数における予測子の影響は次のように書けます。\n\n\\eta = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_n x_n\n\nこの線形予測子は一般線形モデルと同じ用に構築します。\n\n\n連結関数 (リンク関数)\n応答変数の期待値と予測子の関係はリンク関数を通して関係づけます。\n\ng(\\mu) = \\eta\n\nリンク関数は単調な微分可能な関数です。\n\n\n一般化線形モデルの推定方法\n一般化線形モデルのパラメータ推定には最尤推定 (maximum likelihood estimation; maximum likelihood method; 最尤推定法) を用います。\ny_1, \\dots, y_n は n 数の独立な確率変数とします。さらに，y_i は同じパラメータの正規分布に従うとしたら，同時確立分布はつぎの通りです。\n\nP(y_1, \\dots, y_n | \\mu, \\sigma^2) = \\prod_{i=1}^{N} \\exp\\left(y_i\\frac{\\mu}{\\sigma^2} -\\frac{\\mu^2}{2\\sigma^2} - \\frac{1}{2}\\log(2\\pi\\sigma^2) -\\frac{y_i^2}{2\\sigma^2}\\right) = \\mathcal{L}(\\mu, \\sigma^2 | y_1, \\dots, y_n)\n\nここの \\mathcal{L}(\\cdots) が尤度関数 (log-likelihood function)。両側の自然対数を求めれば，対数尤度関数 (log-likelihood function) に導きます。\n\n\\log(\\mathcal{L}(\\mu,\\sigma^2)) = - \\frac{n}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^{N}(y_i - \\mu)^2\n\nこの対数尤度関数が最大または負の対数尤度関数 (negative log-likelihood function) が最小になるパラメータをさがすことが目的です。\n\n\n一般的な誤差項\n連続型確率分布\n\n正規分布\nガンマ分布\nベータ分布\n指数分布\n\n離散型確率分布\n\nポアソン分布\n二項分布\n負の二項分布\ncategorical 分布\n\n他にありますが，上記の分布が一般的につかわれています。"
  },
  {
    "objectID": "glm.html#ガラパゴス諸島のデータのglm解析例",
    "href": "glm.html#ガラパゴス諸島のデータのglm解析例",
    "title": "線形モデル",
    "section": "ガラパゴス諸島のデータのGLM解析例",
    "text": "ガラパゴス諸島のデータのGLM解析例\nまず，帰無仮設の気無モデル（ヌルモデル）を組み立てます。\n\n\\begin{aligned}\n\\text{Species} &\\sim Pois(\\mu)\\\\\nE(\\text{Species}) &= \\mu \\\\\nE(\\text{Species}) &= \\exp(\\eta) \\\\\n\\eta &= b_0  \\\\\n\\end{aligned}\n\n\\eta = b_0 は切片のみのモデルです。\n\nポアソンGLM解析の出力\n\nH0_poisson = glm(Species ~ 1, data = gala, family = poisson(link = \"log\"))\nH0_poisson |> summary() |> print(signif.stars = F)\n\n\nCall:\nglm(formula = Species ~ 1, family = poisson(link = \"log\"), data = gala)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-12.307   -9.782   -5.200    1.142   27.351  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)  4.44539    0.01978   224.8   <2e-16\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3510.7  on 29  degrees of freedom\nResidual deviance: 3510.7  on 29  degrees of freedom\nAIC: 3673.6\n\nNumber of Fisher Scoring iterations: 6\n\n\nパラメータの推定量の表の後に，(Dispersion parameter for poisson family taken to be 1) の記述があります。 これは，ポアソン分布のばらつきのパラメータ (\\phi) を 1 に設定したと意味します。 ヌルモデルを当てはめたので，ヌル逸脱度 (Null Deviance) と残渣逸脱度 (Residual Deviance) は同じです。\n\n\n対立モデル\n次は対立モデルです。\n\n\\begin{aligned}\n\\text{Species} &\\sim Pois(\\mu)\\\\\nE(\\text{Species}) &= \\mu \\\\\nE(\\text{Species}) &= \\exp(\\eta) \\\\\n\\eta &= b_0 + b_1\\text{Area}+b_2\\text{Elevation}+b_3\\text{Nearest}+b_4\\text{Scruz}+b_5\\text{Adjacent}\\\\\n\\end{aligned}\n\n\n\n対立モデルの出力\n\nHF_poisson = glm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, \n                 data = gala, family = poisson(link = \"log\"))\nHF_poisson |> summary() |> print(signif.stars = F)\n\n\nCall:\nglm(formula = Species ~ Area + Elevation + Nearest + Scruz + \n    Adjacent, family = poisson(link = \"log\"), data = gala)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-8.2752  -4.4966  -0.9443   1.9168  10.1849  \n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)\n(Intercept)  3.155e+00  5.175e-02  60.963  < 2e-16\nArea        -5.799e-04  2.627e-05 -22.074  < 2e-16\nElevation    3.541e-03  8.741e-05  40.507  < 2e-16\nNearest      8.826e-03  1.821e-03   4.846 1.26e-06\nScruz       -5.709e-03  6.256e-04  -9.126  < 2e-16\nAdjacent    -6.630e-04  2.933e-05 -22.608  < 2e-16\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3510.73  on 29  degrees of freedom\nResidual deviance:  716.85  on 24  degrees of freedom\nAIC: 889.68\n\nNumber of Fisher Scoring iterations: 5\n\n\n最初に当てはめた線形モデルの結果と全く違います。"
  },
  {
    "objectID": "glm.html#尤度比検定とaicにおけるモデル比較",
    "href": "glm.html#尤度比検定とaicにおけるモデル比較",
    "title": "線形モデル",
    "section": "尤度比検定とAICにおけるモデル比較",
    "text": "尤度比検定とAICにおけるモデル比較\n今までは，正規分布を仮定した解析手法をつかっていたので，比較はF検定で行いました。 ポアソンGLMののとき，尤度比検定で，帰無モデル（ヌルモデル）と対立モデルを比較します。\n\nanova(H0_poisson, HF_poisson, test = \"LRT\")\n\nAnalysis of Deviance Table\n\nModel 1: Species ~ 1\nModel 2: Species ~ Area + Elevation + Nearest + Scruz + Adjacent\n  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    \n1        29     3510.7                          \n2        24      716.8  5   2793.9 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAICでも比較できます。\n\nAIC(H0_poisson, HF_poisson)\n\n           df       AIC\nH0_poisson  1 3673.5596\nHF_poisson  6  889.6767\n\n\n尤度比検定のとき，NHSTの検証になるので，帰無仮設を棄却することになります。 AICのとき，モデル選択するので，AICの最も低いモデルを採択します。 正規分布やガンマ分布などの他の分布をつかったときでも，尤度比検定やAICを用いてもいいです。 もちろん，正規分布の場合，F検定でも問題はありません (このとき，test = \"F\")。\n\nポアソン分布のときに必ず確認する値\nポアソン分布のGLMを実施したあと，必ず確認しなければならい値は過分散 (over-dispersion)です。 過分散があるとき，パラメータの標準誤差，信頼区間，モデルの予測値は誤っています。 過分散をパラメータとして推定するモデル（正規分布やガンマ分布など）の場合は，過分散の推定を無視できます。 この点については https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html を参考にしてください。\n\noverdisp_fun = function(model) {\n    rdf = df.residual(model)\n    rp = residuals(model,type=\"pearson\")\n    Pearson.chisq = sum(rp^2)\n    prat = Pearson.chisq/rdf\n    pval = pchisq(Pearson.chisq, df=rdf, lower.tail=FALSE)\n    mu = mean(predict(model, type = \"response\"))\n    c(mu=mu, chisq=Pearson.chisq,ratio=prat,rdf=rdf,p=pval)\n}\noverdisp_fun(HF_poisson)\n\n           mu         chisq         ratio           rdf             p \n 8.523333e+01  7.619792e+02  3.174914e+01  2.400000e+01 2.187190e-145 \n\n\n感覚的に考えると，モデル結果の Residual Deviance の値が degrees of freedom の値と同じであれば，過分散は存在しません。 ただし，どの手法をつかっても，平均値は 5 以上じゃなければなりません。 上記のP値はとても小さいので，過分散が存在すると考えられます。 または，Residual Deviance (761.98) は自由度 (24) より大きいので，検定をしなくても過分散の問題は明らかです 過分散はデータがポアソン分布に従わないときとモデル変数が不十分なときに起こります。過小分散も存在しますが，過分散ほどの問題ではありません。\n\n\nモデルの改良\n過分散の問題があったので，診断図を確認するよりも，他のモデルを当てはめてみます。 ここでは説明変数のログ変換したモデルを解析します。\n\nH2_poisson = glm(Species ~ logArea + Nearest + logAdjacent, \n                 data = gala, family = poisson(link = \"log\"))\nH2_poisson |> summary() |> print(signif.stars = F)\n\n\nCall:\nglm(formula = Species ~ logArea + Nearest + logAdjacent, family = poisson(link = \"log\"), \n    data = gala)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-5.6281  -3.4817  -0.3344   2.7419   8.2056  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)\n(Intercept)  3.361898   0.046784  71.860  < 2e-16\nlogArea      0.371771   0.007961  46.698  < 2e-16\nNearest     -0.006244   0.001312  -4.759 1.94e-06\nlogAdjacent -0.097529   0.006106 -15.974  < 2e-16\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3510.73  on 29  degrees of freedom\nResidual deviance:  371.78  on 26  degrees of freedom\nAIC: 540.61\n\nNumber of Fisher Scoring iterations: 5\n\n\nResidual deviance と degrees of freedom を確認すると，371.78\\; /\\; 26 > 1 なので，過分散が残っています。 ガラパゴス諸島の種数のデータに対して，どのように変換を変換しても，過分散は >1 です。\n\n\n過分散をパラメータとして扱う\n何をしても過分散が残るとき，過分散をパラメータ化することができます。 ポアソン分布で説明できなかった分散を負の二項分布で説明できるかもしれません。\n【重要】MASS パッケージのselect() 関数は tidyverse の select() 関数と同じ名前なので、 あとに読み込んだパッケージの関すが優先されます。 つまり、MASS パッケージは tidyverse の先に読み込みましょう。 あるいは、不要になったらディタッチ (detach) しましょう。\n\ndetach(name = package:MASS)\n\n\nH2_negbinom = MASS::glm.nb(Species ~ logArea + Nearest + logAdjacent, data = gala, link = \"log\")\nH2_negbinom |> summary() |> print(signif.stars = F)\n\n\nCall:\nMASS::glm.nb(formula = Species ~ logArea + Nearest + logAdjacent, \n    data = gala, link = \"log\", init.theta = 2.83714993)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2174  -0.9672  -0.3006   0.5165   2.0350  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)\n(Intercept)  3.369739   0.157235  21.431   <2e-16\nlogArea      0.364309   0.035028  10.400   <2e-16\nNearest     -0.012389   0.008211  -1.509    0.131\nlogAdjacent -0.034399   0.036005  -0.955    0.339\n\n(Dispersion parameter for Negative Binomial(2.8371) family taken to be 1)\n\n    Null deviance: 144.45  on 29  degrees of freedom\nResidual deviance:  32.82  on 26  degrees of freedom\nAIC: 284.88\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  2.837 \n          Std. Err.:  0.830 \n\n 2 x log-likelihood:  -274.884 \n\n\n誤差項の分布は負の二項分布にしたので，分散は Variance = \\mu + \\frac{\\mu}{\\theta} として推定されます。 このモデルの\\theta は 2.8371 です。 このモデルの解析を進めたいので，次は診断図の確認です。\n\n\n負の二項分布GLMの診断図\n診断図はよくつくるので、診断図用の関数を定義します。\n\n\n診断図関数：残差のヒストグラム\n\n# 残差のヒストグラム\ngg_resid_hist = function(fitted.model) {\n  require(tidyvers)\n  require(statmod)\n  data = fortify(fitted.model) |> as_tibble()\n  if(class(fitted.model)[1] != \"lm\") {\n    data = data |> mutate(qresid = statmod::qresiduals(fitted.model))\n    data = data |> mutate(residual = qresid)\n    xlabel = \"Randomized Quantile Residuals\"\n  } else {\n    data = data |> mutate(residual = .resid)\n    xlabel = \"Standardized Residuals\"\n  }\n  ggplot(data) +\n    geom_histogram(aes(x = residual)) +\n    labs(x = xlabel) + ggtitle(\"Histogram of Residuals\")\n}\n\n\n\n診断図関数：残差のQQプロット\n\n# 残差のQQプロット\ngg_qq = function(fitted.model) {\n  require(tidyvers)\n  require(statmod)\n  data = fortify(fitted.model) |> as_tibble()\n   if(class(fitted.model)[1] != \"lm\") {\n    data = data |> mutate(qresid = statmod::qresiduals(fitted.model))\n    data = data |> mutate(residual = qresid)\n    ylabel = \"Randomized Quantile Residuals\"\n  } else {\n    data = data |> mutate(residual = .stdresid)\n    ylabel = \"Standardized Residuals\"\n  }\n  ggplot(data) +\n    geom_qq(aes(sample =residual)) +\n    geom_abline(color = \"red\") +\n    labs(x = \"Theoretical Quantile\", y = ylabel) +\n    ggtitle(\"Normal-QQ Plot\")\n}\n\n\n\n診断図関数：変数に対する残差のばらつき\n\n# 変数に対する残差のプロット\ngg_resX = function(fitted.model, ncol=NULL, ...) {\n  require(tidyvers)\n  require(statmod)\n  residlab = function(string) {\n    sprintf(\"Residuals vs. %s\", string)\n  }\n  data = fortify(fitted.model) |> as_tibble()\n  varnames = as.character(formula(fitted.model)) |> pluck(3)\n  varnames = str_split(varnames, \" \\\\+ \") |> pluck(1)\n  if(class(fitted.model)[1] != \"lm\") {\n    data = data |> mutate(qresid = statmod::qresiduals(fitted.model))\n    data = data |> mutate(residual = qresid)\n    ylabel = \"Randomized Quantile Residuals\"\n  } else {\n    data = data |> mutate(residual = .resid)\n    ylabel = \"Standardized Residuals\"\n  }\n  varnames = names(data)[names(data) %in% varnames]\n  data = data |> dplyr::select(varnames, residual) |> gather(var, value, varnames)\n  ggplot(data) + \n    geom_point(aes(x = value, y = residual)) +\n    geom_hline(yintercept=0, color = \"red\", linetype = \"dashed\") +\n    labs(x=\"Value\", y = ylabel) +\n    facet_wrap(\"var\", labeller=labeller(var=residlab), scales = \"free_x\",\n               ncol = ncol)\n}\n\n\n\n診断図関数：期待値に対する残差のばらつき\n\n# 期待値に対する残差のプロット\ngg_resfitted = function(fitted.model) {\n  require(tidyvers)\n  require(statmod)\n  data = fortify(fitted.model) |> as_tibble()\n  if(class(fitted.model)[1] != \"lm\") {\n    data = data |> mutate(qresid = statmod::qresiduals(fitted.model))\n    data = data |> mutate(residual = qresid)\n    ylabel = \"Randomized Quantile Residuals\"\n  } else {\n    data = data |> mutate(residual = .resid)\n    ylabel = \"Standardized Residuals\"\n  }\n  ggplot(data) + \n    geom_point(aes(x = .fitted, y = residual)) +\n    geom_hline(yintercept=0, color = \"red\", linetype = \"dashed\") +\n    labs(x=\"Fitted Values\", y = ylabel) +\n    ggtitle(\"Residuals vs. Fitted Values\")\n}\n\n\n\n診断図関数：スケール・ロケーションプロット\n\n# スケール・ロケーションプロット\ngg_scalelocation = function(fitted.model) {\n  require(tidyvers)\n  require(statmod)\n  data = fortify(fitted.model) |> as_tibble()\n  if(class(fitted.model)[1] != \"lm\") {\n    data = data |> mutate(qresid = statmod::qresiduals(fitted.model))\n    data = data |> mutate(residual = qresid)\n    ylabel = expression(sqrt(\"|RQR|\"))\n  } else {\n    data = data |> mutate(residual = .resid)\n    ylabel = expression(sqrt(\"|Standardized Residuals|\"))\n  }\n  ggplot(data) + \n    geom_point(aes(x = .fitted, y = sqrt(abs(residual)))) +\n    geom_smooth(aes(x = .fitted, y = sqrt(abs(residual))), \n                se = F, color = \"red\", linetype = \"dashed\") +\n    labs(x=\"Fitted Values\", y = ylabel) +\n    ggtitle(\"Scale - Location Plot\")\n}\n\n\n\n診断図関数：クックの距離\n\n# クックの距離\ngg_cooksd = function(fitted.model) {\n  require(tidyvers)\n  require(statmod)\n  data = fortify(fitted.model) |> as_tibble()\n  data = data |> mutate(n = seq_along(.cooksd))\n  dof = summary(fitted.model) |> pluck(\"df\")\n  thold = qf(0.5, dof[1], dof[2])\n  data2 = data |> mutate(above = ifelse(.cooksd > thold, T, F)) |> filter(above)\n  ggplot(data) + \n    geom_point(aes(x = n, y = .cooksd)) +\n    geom_segment(aes(x = n, y = 0, xend = n, yend = .cooksd)) +\n    geom_hline(yintercept=thold, color = \"red\", linetype = \"dashed\") +\n    geom_text(aes(x = n, y = .cooksd, label = n), data = data2, nudge_x=3, color = \"red\") +\n    labs(x=\"Sample\", y = \"Cook's Distance\") +\n    ggtitle(\"Cook's Distance Plot\")\n}\n\n残差の確認は線形モデルと同じようにしますが，解析に使う残差は ダン=スミス残差 (Dunn-Smyth Residuals; randomized quantile residuals) です。 とくに，ポアソンGLMと疑似ポアソンGLMのダン=スミス残差を求めます。 ダン=スミス残差は残差の累積分布関数を用いて残差のランダム化を行います。 ランダム化した残差は正規分布に近似するので，QQプロットで使用した分布のよさを評価できます。 ]ランダム化残差が直線に沿っていたら，分布に問題がないと示唆します。 左側はポアソン分布GLMのランダム化残差のQQプロットです。右側は負の二項分布GLMのランダム化残差のQQプロットです。\n\np1 = gg_qq(H2_poisson)\n\nLoading required package: tidyvers\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'tidyvers'\n\n\nLoading required package: statmod\n\np2 = gg_qq(H2_negbinom)\n\nLoading required package: tidyvers\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'tidyvers'\n\np1 | p2\n\n\n\n\n\n\nη に対するランダム化残差のばらつき\nランダム化残差と \\eta の間に明確な関係はありません。 \\sqrt{\\left(|\\text{Randomized Quantile Residuals}~\\right)} は若干山形な形をしています。 ところが，期待値の両端のデータが点線を引っ張っているように見えるので， 明確ではない。\n\np1 = gg_resfitted(H2_negbinom)\n\nLoading required package: tidyvers\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'tidyvers'\n\np2 = gg_scalelocation(H2_negbinom)\n\nLoading required package: tidyvers\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'tidyvers'\n\np1 | p2\n\n`geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n\n全てクックの距離はP(F_{(n, n-p)}=0.5) より低いので，モデルを引っ張る点はありません。\n\ngg_cooksd(H2_negbinom)"
  },
  {
    "objectID": "glm.html#結果",
    "href": "glm.html#結果",
    "title": "線形モデル",
    "section": "結果",
    "text": "結果\n診断図と過分散の確認から，次のモデルにたどり着きました。\n\n\\begin{aligned}\n\\text{Species} &\\sim NegBin(\\mu)\\\\\nE(\\text{Species}) &= \\mu \\\\\nE(\\text{Species}) &= \\exp(\\eta) \\\\\nVariance &= \\mu + \\frac{\\mu}{\\theta} \\\\\n\\eta &= b_0 + b_1\\log(\\text{Area})+b_3\\text{Nearest}+b_5\\log(\\text{Adjacent})\\\\\n\\end{aligned}\n\n\nAIC(H2_poisson, H2_negbinom)\n\n            df      AIC\nH2_poisson   4 540.6134\nH2_negbinom  5 284.8838\n\n\nポアソン分布GLMのAICが高いので，負の二項分布GLMを採択します。\nCross Validated の参考資料 (Why is the quasi-Poisson in GLM not treated as a special case of negative binomial?)[https://stats.stackexchange.com/questions/157575/why-is-the-quasi-poisson-in-glm-not-treated-as-a-special-case-of-negative-binomi]\n\n負の二項分布の結果（係数表）\n係数表だけ出力しました。\n\nH2_negbinom |> summary() |> coefficients() |> print(digits = 3)\n\n            Estimate Std. Error z value  Pr(>|z|)\n(Intercept)   3.3697    0.15723  21.431 6.83e-102\nlogArea       0.3643    0.03503  10.400  2.47e-25\nNearest      -0.0124    0.00821  -1.509  1.31e-01\nlogAdjacent  -0.0344    0.03600  -0.955  3.39e-01\n\n# H2_negbinom |> summary() # 全情報の出力\n\n切片と logArea の効果ははっきりしています (P < 0.0001)。ところが，Nearest と logAdjacent のP値は >0.05 でした。 さらに，変数をへらしてもAICは大きく変わりません。\n\n\nAIC\n\nH2_negbinom2 = MASS::glm.nb(Species ~ logArea + Nearest , data = gala, link = \"log\")\nH2_negbinom3 = MASS::glm.nb(Species ~ logArea +  logAdjacent, data = gala, link = \"log\")\nH2_negbinom4 = MASS::glm.nb(Species ~ logArea , data = gala, link = \"log\")\nAIC(H2_negbinom, H2_negbinom2, H2_negbinom3, H2_negbinom4) |> as_tibble(rownames = \"model\") |> arrange(AIC)\n\n# A tibble: 4 × 3\n  model           df   AIC\n  <chr>        <dbl> <dbl>\n1 H2_negbinom2     4  284.\n2 H2_negbinom4     3  284.\n3 H2_negbinom      5  285.\n4 H2_negbinom3     4  285.\n\n\n最も小さいAICは H2_negbinom2 になりましたがAICがの差が 0 ~ 7 の範囲に入るので，どのモデルでもいいです。 このとき，尤度比検定もした方がいい。\n\n\n尤度比検定\n\nanova(H2_negbinom, H2_negbinom2, H2_negbinom3, H2_negbinom4, test = \"LRT\")\n\nWarning in anova.negbin(H2_negbinom, H2_negbinom2, H2_negbinom3, H2_negbinom4, :\nonly Chi-squared LR tests are implemented\n\n\nLikelihood ratio tests of Negative Binomial Models\n\nResponse: Species\n                            Model    theta Resid. df    2 x log-lik.   Test\n1                         logArea 2.533343        28       -277.7721       \n2               logArea + Nearest 2.730433        27       -275.7711 1 vs 2\n3           logArea + logAdjacent 2.619569        27       -276.9859 2 vs 3\n4 logArea + Nearest + logAdjacent 2.837150        26       -274.8838 3 vs 4\n     df  LR stat.   Pr(Chi)\n1                          \n2     1  2.000994 0.1571961\n3     0 -1.214826 1.0000000\n4     1  2.102117 0.1470954\n\n\n自由度の高い順からペア毎の尤度比検定を行っています。E(\\text{Species})\\sim b_0 + b_1\\text{logArea} のモデルでいいかもしれないです。\n\n\nモデル診断図\n\np1 = gg_qq(H2_negbinom4)\n\nLoading required package: tidyvers\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'tidyvers'\n\np2 = gg_resX(H2_negbinom4, ncol = 2)\n\nLoading required package: tidyvers\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'tidyvers'\n\n\nNote: Using an external vector in selections is ambiguous.\nℹ Use `all_of(varnames)` instead of `varnames` to silence this message.\nℹ See <https://tidyselect.r-lib.org/reference/faq-external-vector.html>.\nThis message is displayed once per session.\n\np3 = gg_resfitted(H2_negbinom4)\n\nLoading required package: tidyvers\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'tidyvers'\n\np4 = gg_cooksd(H2_negbinom4)\n\nLoading required package: tidyvers\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'tidyvers'\n\np1 + p2 + p3 + p4\n\n\n\n\nモデル診断図を確認したら，残差の問題はないですので，E(\\text{Species})\\sim b_0 + b_1\\text{logArea} のモデルを採択することになりました。\n\n\n採択したモデル\n\nH2_negbinom4 |> summary()\n\n\nCall:\nMASS::glm.nb(formula = Species ~ logArea, data = gala, link = \"log\", \n    init.theta = 2.533342912)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.9737  -0.9216  -0.2155   0.5056   1.8969  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  3.22480    0.13669  23.592   <2e-16 ***\nlogArea      0.34989    0.03543   9.877   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(2.5333) family taken to be 1)\n\n    Null deviance: 130.161  on 29  degrees of freedom\nResidual deviance:  32.604  on 28  degrees of freedom\nAIC: 283.77\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  2.533 \n          Std. Err.:  0.719 \n\n 2 x log-likelihood:  -277.772 \n\n\n\nndata = gala |> expand(logArea = seq(min(logArea), max(logArea), length = 21)) \nndata = bind_cols(ndata, predict(H2_negbinom4, newdata = ndata, type = \"link\", se = T) |> as_tibble())\nndata = ndata |> \n  mutate(expect = exp(fit),\n         lower = exp(fit - se.fit),\n         upper = exp(fit + se.fit))\n\n\ngala |> \n  ggplot(aes(x = logArea)) +\n  geom_ribbon(aes(ymin = lower, ymax = upper), data = ndata, alpha = 0.5) +\n  geom_point(aes(y = Species)) +\n  geom_line(aes(y = expect), data = ndata)"
  },
  {
    "objectID": "nonlinear.html",
    "href": "nonlinear.html",
    "title": "非線形モデル",
    "section": "",
    "text": "ここで紹介する非線形モデルは、海洋生物科学実験III用に準備しました。 光合成光曲線の解析を紹介します。"
  },
  {
    "objectID": "nonlinear.html#必要なパッケージ",
    "href": "nonlinear.html#必要なパッケージ",
    "title": "非線形モデル",
    "section": "必要なパッケージ",
    "text": "必要なパッケージ\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.4.0 \n✔ readr   2.1.2      ✔ forcats 0.5.1 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n'nlstools' has been loaded.\n\n\nIMPORTANT NOTICE: Most nonlinear regression models and data set examples\n\nrelated to predictive microbiolgy have been moved to the package 'nlsMicrobio'\n\n\n\n# データ処理用パッケージ\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(googlesheets4)\n# 作図用パッケージ\nlibrary(ggcorrplot)\nlibrary(patchwork)\n#非線形モデル用補助パッケージ\nlibrary(minpack.lm)\nlibrary(nlstools)"
  },
  {
    "objectID": "nonlinear.html#データの読み込み",
    "href": "nonlinear.html#データの読み込み",
    "title": "非線形モデル",
    "section": "データの読み込み",
    "text": "データの読み込み\nデータは Google Drive に共有しています。 googlesheets4 のパッケージをつかて、データをクラウドからダウンロードできます。 リンクをクリックしたら、ブラウザからもダウンロードできます。 データは共有制限なしで公開したので、authentication なしでアクセスできます。 このときは gs4_deauth() を実行します。\n\ngs4_deauth()\n\nGoogle Drive で共有したデータは次のURLの通りです。 リンクは実験日１〜３の google sheets に飛びます。\n\n実験日 1\n実験日 2\n実験日 3\n\n\nspreadsheet1 = \"https://docs.google.com/spreadsheets/d/1CsY7ILKZRFlwQEIzSgu1veMQ964IPVegIJOo04lIGVE/edit#gid=1846404397\"\nspreadsheet2 = \"https://docs.google.com/spreadsheets/d/1yeC-rJdxdiVa_icoNHZ1xrt4HWHyGCeQMnUt1r2_hnk/edit#gid=540001236\" \nspreadsheet3 = \"https://docs.google.com/spreadsheets/d/1Im8Qg-ukk8uh_3z4H6IwirTc4nhxPqKrDWrjhK4gZ0o/edit#gid=2099964525\"\n\n公開した google sheets にはつぎのスプレッドシートが入っています。\n\n光合成データ\n海藻資料データ\n光環境データ\n\nデータの読み込みは read_sheet() で行います。 各シートの構造は同じにしたので、map() をつかって一度にデータを読み込みます。 fnames 変数は解析に使わないので、select(-fnames) で外します。\n\nmgldata = tibble(fnames = c(spreadsheet1, spreadsheet2, spreadsheet3), day = 1:3) |> \n  mutate(data = map(fnames, read_sheet, sheet = \"光合成データ\")) |> \n  select(-fnames)\n\n✔ Reading from \"Photosynthesis_Lab_Day_1_2018\".\n\n\n✔ Range ''光合成データ''.\n\n\n✔ Reading from \"Photosynthesis_Lab_Day_2_2018\".\n\n\n✔ Range ''光合成データ''.\n\n\n✔ Reading from \"Photosynthesis_Lab_Day_3_2018\".\n\n\n✔ Range ''光合成データ''.\n\nseaweed = tibble(fnames = c(spreadsheet1, spreadsheet2, spreadsheet3), day = 1:3) |> \n  mutate(data = map(fnames, read_sheet, sheet = \"海藻資料データ\"))|> \n  select(-fnames)\n\n✔ Reading from \"Photosynthesis_Lab_Day_1_2018\".\n\n\n✔ Range ''海藻資料データ''.\n\n\n✔ Reading from \"Photosynthesis_Lab_Day_2_2018\".\n\n\n✔ Range ''海藻資料データ''.\n\n\n✔ Reading from \"Photosynthesis_Lab_Day_3_2018\".\n\n\n✔ Range ''海藻資料データ''.\n\nlightdata = tibble(fnames = c(spreadsheet1, spreadsheet2, spreadsheet3), day = 1:3) |> \n  mutate(data = map(fnames, read_sheet, sheet = \"光環境データ\"))|> \n  select(-fnames)\n\n✔ Reading from \"Photosynthesis_Lab_Day_1_2018\".\n\n\n✔ Range ''光環境データ''.\n\n\n✔ Reading from \"Photosynthesis_Lab_Day_2_2018\".\n\n\n✔ Range ''光環境データ''.\n\n\n✔ Reading from \"Photosynthesis_Lab_Day_3_2018\".\n\n\n✔ Range ''光環境データ''.\n\n\n\n光環境データの処理\n光条件毎の (light) 光量子量 (ppfd) の平均値を求めます。 アルミホイル の光条件のときの光量子量は測っていないが、ppfd は 0 とします。 アルミホイル のときのデータはコードとして定義して、追加します。\n\nlightdata = lightdata |> \n  unnest(data) |> \n  select(day,\n         han = \"班\",\n         light = \"光環境\",\n         sample = matches(\"サンプル\"),\n         ppfd = matches(\"光量子\")) |> \n  group_by(day, light) |> \n  summarise(ppfd = mean(ppfd))\n\n`summarise()` has grouped output by 'day'. You can override using the `.groups`\nargument.\n\n\nアルミホイルのデータを定義し、tmp に入れます。\n\ntmp = tibble(light = rep(\"アルミホイル\",3), \n             ppfd =  rep(0, 3),\n             day = 1:3)\n\n観測したデータと tmp を縦に結合します。\n\nlightdata = bind_rows(lightdata, tmp) |> \n  mutate(light = factor(light), day = factor(day))\n\n\n\n全データを結合\n光合成データ mgldata と海藻資料データ seaweed を結合します。 matches() は select() の selection helper function です。 matches() にわたした正規表現 (regular expression) とマッチ (match) した変数（列名）が返ってきます。\n\nmgldata = mgldata |> unnest(data) |> \n  select(day,\n         han = \"班\",\n         sample = matches(\"サンプル\"),\n         min = matches(\"時間\"),\n         mgl = matches(\"酸素\"),\n         temperature = matches(\"水温\"),\n         light = matches(\"光環境\"),\n         seaweed = matches(\"海藻\"))\n\nseaweed = seaweed |> unnest(data) |> \n  select(day,\n         seaweed = matches(\"海藻\"),\n         han = \"班\",\n         sample = matches(\"サンプル\"),\n         vol = matches(\"容量\"),\n         gww = matches(\"湿重量\"))\n\nmgldata = full_join(mgldata, seaweed, by = c(\"han\", \"sample\", \"day\"))\n\nhan, sample, day は as.factor() を通して因子に変換します。 across() は複数変数に同じ関数を適応したいときにつかいます。\n\nmgldata = mgldata |> mutate(across(c(han, sample, day), as.factor))\n\nacross() を使わずに変換するなら、次通りです。\n\nmgldata = mgldata |>\n  mutate(han = as.factor(han), sample = as.factor(sample), day = as.factor(day))\n\n結合したら，溶存酸素濃度の時間変動を可視化します。\n\nggplot(mgldata) +\n  geom_point(aes(x = min, y = mgl, color = han)) +\n  facet_grid(rows = vars(light),\n             cols = vars(seaweed))\n\n\n\n\n光の調整は編み袋 (ネット) でやっています。 ネットの枚数が増えると光量子量が下がります。 アルミホイルで光量子量が 0 の条件を作っています。 ネットがないとき (最も明るいとき) 溶存酸素濃度が顕著に増加しましたが、アルミホイルのときは緩やかに減少しました。\n\n\n光合成速度を求める\n複数データ群から光合成速度を計算したいので、map() 関数を通して行います。 map() に渡す光合成速度用関数を定義します。 fit_model() は線形モデルを溶存酸素濃度時系列データに当てはめ用です。 get_rate() は 2 つのモデル係数 (y = b_0+ b_1 x) から傾き (b_1) を抽出します。\n\nfit_model = function(df) {\n  lm(mgl ~ min, data = df)\n}\nget_rate = function(m) {\n  coefficients(m)[2] \n}\n\nここでデータをグループ化して、グループ毎の傾きを求めます。\n\nmgldata = \n  mgldata |> \n  group_nest(day, han, sample, light, gww, vol, seaweed) |> \n  mutate(model = map(data, fit_model)) |> \n  mutate(rate = map_dbl(model, get_rate)) |> \n  mutate(stats = map(model, glance)) |> \n  unnest(stats) \n\n求めた係数と環境データを結合します。\n\nalldata = full_join(mgldata, lightdata, by = c(\"day\", \"light\"))\n\n係数は湿重量を実験容器の容積で割って、湿重量あたりの純光合成速度を求めます。 ここで、係数の単位は mg O2 l-1 min-1 から mg O2 gww{-1}min-1^ に変わります。\n単位の求め方：\n\n\\overbrace{\\frac{mg\\;\\text{O}_2}{l}}^{\\text{酸素濃度}} \\times \\underbrace{\\frac{1}{g_{ww}}}_{\\text{湿重量}} \\times \\overbrace{ml}^{\\text{容積}} \\times \\frac{1 \\;l}{1000\\; ml} \\times \\frac{1000\\;\\mu g\\;\\text{O}_2}{1\\;mg\\;\\text{O}_2}\n R コード：\n\nalldata = \n  alldata |> \n  mutate(normalized_rate = rate / gww * vol)\n\n解析をする前に、光量子量、種、班ごとの平均値を求めます。\n\ndataset = \n  alldata |> \n  group_by(ppfd, seaweed, han) |> \n  summarise(np = mean(normalized_rate))\n\n`summarise()` has grouped output by 'ppfd', 'seaweed'. You can override using\nthe `.groups` argument.\n\n\n標準化した光合成速度は次の通りです。\n\nxlabel = expression(paste(\"PPFD\"~(mu*mol~m^{-2}~s^{-1})))\nylabel = '\"Net photosynthesis rate\"~(mu*g~O[2]~g[ww]^{-1}~min^{-1})'\nylabel = as.expression(parse(text = ylabel))\ndataset |> \n  ggplot() + \n  geom_point(aes(x = ppfd, y = np, color = han)) +\n  scale_color_viridis_d(\"\", end = 0.8) +\n  scale_x_continuous(xlabel) +\n  scale_y_continuous(ylabel) +\n  facet_grid(col = vars(seaweed)) \n\n\n\n\nウミトラノオ、コブクロモク、ホンダワラ属海藻の幼体の光合成速度。記号の色はそれぞれの班を示しています。光合成速度のばらつきは光量子量があると大きくなるのが明らかです。"
  },
  {
    "objectID": "nonlinear.html#モデルの当てはめ",
    "href": "nonlinear.html#モデルの当てはめ",
    "title": "非線形モデル",
    "section": "モデルの当てはめ",
    "text": "モデルの当てはめ\n非線形モデルの当てはめに便利な nlstools パッケージを使います。 当てはめたいモデルは次のとおりです。\n\n\\overbrace{P_{net}}^{\\text{純光合成速度}} = \\underbrace{P_{max}\\left(1 - \\exp\\left(-\\frac{\\alpha}{P_{max}}I\\right)\\right)}_{\\text{総光合成速度}} - \\overbrace{R_d}^{\\text{暗呼吸速度}}\n\n\nP_{net} は純光合成速度 (normalized_rate)\nI は光量子量 (ppfd)\nP_{max} は光合成飽和速度 (pmax)\n\\alpha は初期勾配 (alpha)\nR_d は暗呼吸速度 (rd)\n\nこのモデルをR関数に書き換えると次のようになります。\n\npecurve = function(ppfd, pmax, rd, alpha) {\n  pmax * (1-exp(-alpha / pmax * ppfd)) - rd\n}\n\nnlstools の preview() 関数をつかって，モデル当てはめ用の関数 (nls()) に必要なの初期値を探します。 variable 引数に ppfd 変数の位置情報を渡してください。 この位置情報は tibble 変数（列）の順位です。 このデータの場合、ppfd は 1 列目なので、variable = 1 を preview() に渡しています。 str_detect() も matches() と同じように正規表現をつかって、 seaweed 変数からマッチしたものを返します。\n\nSTART = list(pmax = 10, rd = 3, alpha = 0.3)\n\ncrispifolium = dataset |> filter(str_detect(seaweed, \"コブクロ\"))\nthunbergii   = dataset |> filter(str_detect(seaweed, \"ウミトラノオ\"))\njuvenile     = dataset |> filter(str_detect(seaweed, \"幼体\"))\n\n\npreview(np ~ pecurve(ppfd, pmax, rd, alpha), \n        data = crispifolium, \n        variable = 1,\n        start = START)\n\n\n\n\n\nRSS:  167 \n\n\n\npreview(np ~ pecurve(ppfd, pmax, rd, alpha), \n        data = thunbergii, \n        variable = 1,\n        start = START)\n\n\n\n\n\nRSS:  501 \n\n\n\npreview(np ~ pecurve(ppfd, pmax, rd, alpha), \n        data = juvenile, \n        variable = 1,\n        start = START)\n\n\n\n\n\nRSS:  246 \n\n\n+ 記号がデータの中心に通る用になったら，そのときの初期値を nls() または、minpack.lm パッケージの nlsLM() 関数に渡してモデルの当てはめをします。 nls() 関数は Gauss-Newton アルゴリズムによってパラメータ推定をしますが、nlsLM() は Levenberg-Marquardt アルゴリズムを用います。 Levenberg-Marquardt法のほうが優秀ですが、モデルの組み方によって使えないときがあります。\nここではデータを海藻毎に当てはめるので、解析関数をつくります。\n\nfit_nls = function(df) {\n  START = list(pmax = 14, rd = 3, alpha = 0.3)\n  # nls(np ~ pecurve(ppfd, pmax, rd, alpha),  data = df, start = START)\n  nlsLM(np ~ pecurve(ppfd, pmax, rd, alpha),  data = df, start = START)\n}\n\ndataset = dataset |> ungroup() |> \n  group_nest(seaweed) |>\n  mutate(model = map(data, fit_nls))\ndataset\n\n# A tibble: 3 × 3\n  seaweed                    data model \n  <chr>        <list<tibble[,3]>> <list>\n1 ウミトラノオ           [24 × 3] <nls> \n2 コブクロモク           [24 × 3] <nls> \n3 幼体                   [24 × 3] <nls> \n\n\n当てはめたモデルの結果は次の通りです。\n\ndataset = dataset |> \n  mutate(summary =map(model, glance)) |> \n  unnest(summary)\ndataset\n\n# A tibble: 3 × 12\n  seaweed     data model sigma isConv  finTol logLik   AIC   BIC devia…¹ df.re…²\n  <chr>   <list<t> <lis> <dbl> <lgl>    <dbl>  <dbl> <dbl> <dbl>   <dbl>   <int>\n1 ウミト… [24 × 3] <nls>  2.51 TRUE   1.49e-8  -54.5 117.  122.    132.       21\n2 コブク… [24 × 3] <nls>  2.21 TRUE   1.49e-8  -51.5 111.  116.    103.       21\n3 幼体    [24 × 3] <nls>  1.31 TRUE   1.49e-8  -39.0  86.0  90.7    36.2      21\n# … with 1 more variable: nobs <int>, and abbreviated variable names ¹​deviance,\n#   ²​df.residual\n\n\n次はモデルの期待値を求めます。 この関数は期待値を擬似データから計算します。 擬似データは tibble() で作っています。\n\ncalc_fitted = function(data, model) {\n  N = 21 # 擬似データの長さ\n  ndata = tibble(ppfd = seq(min(data$ppfd), max(data$ppfd),length = N))\n  tmp = predict(model, newdata = ndata) |> as_tibble()\n  bind_cols(ndata,tmp)\n}\n\ndataset = dataset |> mutate(fitted = map2(data, model, calc_fitted))\n\n\nggplot() +\n  geom_point(aes(x = ppfd, y = np), data = unnest(dataset, data))+\n  geom_line(aes(x = ppfd, y = value), data = unnest(dataset, fitted)) +\n  facet_grid(rows = vars(seaweed))\n\n\n\n\n観測とモデル期待値。"
  },
  {
    "objectID": "nonlinear.html#診断図",
    "href": "nonlinear.html#診断図",
    "title": "非線形モデル",
    "section": "診断図",
    "text": "診断図\n線形モデルと同様に、モデルを当てはめたら、残渣の診断図も確認します。 ここでは残渣 (residuals) とモデル期待値 (fitted) を求めます。\n\nshindan = dataset |> \n  select(seaweed, data, model) |> \n  mutate(residuals = map(model, residuals)) |> \n  mutate(fitted = map(model, fitted)) |> \n  select(seaweed, data, residuals, fitted) |> \n  unnest(everything())\n\n\np1 = ggplot(shindan) +\n  geom_point(aes(x = fitted, y = residuals,\n                 color = seaweed),\n             size = 3) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  scale_color_viridis_d(end = 0.8)\n\np2 = ggplot(shindan) +\n  geom_point(aes(x = fitted, y = sqrt(abs(residuals)),\n                 color = seaweed),\n             size = 3) +\n  scale_color_viridis_d(end = 0.8)\np1 + p2 + plot_layout(nrow = 2, guides = \"collect\")\n\n\n\n\nモデル残渣と残渣の絶対値の平方根の図で、モデルの当てはめの良さが分かります。（左）はモデル残渣対期待値です。期待値が増加するとモデル残渣の散らばりが大きくなるのがはっきりしています。点線 (0) の周りを均一にばらつくのが理想です。（右）は残渣の絶対値の平方根です。期待値が上がると残渣が増加しています。これらの図を確認すると、残渣のばらつきの均一性に問題があります。"
  },
  {
    "objectID": "nonlinear.html#パラメータの集計",
    "href": "nonlinear.html#パラメータの集計",
    "title": "非線形モデル",
    "section": "パラメータの集計",
    "text": "パラメータの集計\n残渣プロットの結果はひどかったが、とりあえず、光飽和点 (I_k) と光補償点 (I_c) を求めましょう。\n\n光飽和点：I_k = P_{max} / \\alpha\n光補償点：I_c = \\frac{P_{max}}{\\alpha} \\ln\\left(\\frac{P_{max}}{P_{max} - R_d}\\right)\n\nまずは係数を抽出するための関数を定義します。 モデルの定義によって、cfs からパラメータを抽出すろときのコードが変わります。 pecurve() を定義したときに、pmax, alpha, rd がパラメータ名だったので、 抽出には \"pmax\", \"alpha\", \"rd\" の文字列を使いました。 文字列はパラメータ名と一致するようにしましょう。\n\nget_cfs = function(m) {\n  cfs = coef(m)\n  tibble(pmax = cfs[\"pmax\"],\n         alpha = cfs[\"alpha\"],\n         rd = cfs[\"rd\"])\n}\n\nここで光飽和点と光補償点を求めるための関数を定義します。\n\ncalc_ik = function(m) {\n  # 光飽和点\n  cfs = coef(m)\n  cfs[\"pmax\"] / cfs[\"alpha\"]*log(cfs[\"pmax\"]/(cfs[\"pmax\"] - cfs[\"rd\"]))\n}\ncalc_ic = function(m) {\n  # 光補償点\n  cfs = coef(m)\n  cfs[\"pmax\"] / cfs[\"alpha\"]\n}\n\nmap() を使って係数を求めます。\n\nmodelcfs = dataset |> \n  select(seaweed, model) |> \n  mutate(cfs = map(model, get_cfs)) |> \n  mutate(ik = map(model, calc_ik)) |> \n  mutate(ic = map(model, calc_ic)) |> \n  unnest(c(ik, ic, cfs))\nmodelcfs\n\n# A tibble: 3 × 7\n  seaweed      model   pmax  alpha    rd    ik    ic\n  <chr>        <list> <dbl>  <dbl> <dbl> <dbl> <dbl>\n1 ウミトラノオ <nls>  23.3  0.0858 2.08  25.4  271. \n2 コブクロモク <nls>   5.24 0.165  0.431  2.74  31.8\n3 幼体         <nls>   4.50 0.0718 0.827 12.7   62.7\n\n\n\n\n\n\n \n  \n    海藻類 \n    P~max~ \n    α \n    R~d~ \n    I~k~ \n    I~c~ \n  \n \n\n  \n    ウミトラノオ \n    23.3 \n    0.086 \n    2.1 \n    25.4 \n    271.4 \n  \n  \n    コブクロモク \n    5.2 \n    0.165 \n    0.4 \n    2.7 \n    31.8 \n  \n  \n    幼体 \n    4.5 \n    0.072 \n    0.8 \n    12.7 \n    62.7 \n  \n\n\n\n\n\n\nモデル係数表について\nモデル係数の統計量は summary() または nlstools の overview() で見れます。 Estimate はモデル係数の期待値です。 Std. Error は係数の標準誤差です。 t value と Pr(>|t|) は 0 に対して、モデル係数の t 値と t 値の P 値です。 この係数結果に出力されたものが Wald’s test の結果です。 すべての海藻類に対して、P_{max} = 0 の帰無仮説を棄却できますが、 R_d = 0 の帰無仮説は棄却できません。 コブクロモクと幼体の \\alpha = 0 の帰無仮説は棄却できないが、ウミトラノオの場合は棄却できました。\n\ndataset |> \n  mutate(summary = map(model, summary)) |> \n  pull(summary, name = seaweed)\n\n$ウミトラノオ\n\nFormula: np ~ pecurve(ppfd, pmax, rd, alpha)\n\nParameters:\n      Estimate Std. Error t value Pr(>|t|)    \npmax  23.28655    5.72475   4.068 0.000553 ***\nrd     2.07666    1.27193   1.633 0.117440    \nalpha  0.08580    0.02415   3.553 0.001883 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.506 on 21 degrees of freedom\n\nNumber of iterations to convergence: 9 \nAchieved convergence tolerance: 1.49e-08\n\n\n$コブクロモク\n\nFormula: np ~ pecurve(ppfd, pmax, rd, alpha)\n\nParameters:\n      Estimate Std. Error t value Pr(>|t|)   \npmax    5.2410     1.3996   3.745  0.00119 **\nrd      0.4315     1.2211   0.353  0.72736   \nalpha   0.1646     0.1069   1.539  0.13862   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.212 on 21 degrees of freedom\n\nNumber of iterations to convergence: 7 \nAchieved convergence tolerance: 1.49e-08\n\n\n$幼体\n\nFormula: np ~ pecurve(ppfd, pmax, rd, alpha)\n\nParameters:\n      Estimate Std. Error t value Pr(>|t|)    \npmax   4.50104    0.92297   4.877 8.02e-05 ***\nrd     0.82731    0.69152   1.196   0.2449    \nalpha  0.07176    0.03480   2.062   0.0518 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.314 on 21 degrees of freedom\n\nNumber of iterations to convergence: 10 \nAchieved convergence tolerance: 1.49e-08"
  },
  {
    "objectID": "nonlinear.html#多重比較",
    "href": "nonlinear.html#多重比較",
    "title": "非線形モデル",
    "section": "多重比較",
    "text": "多重比較\n数種類のデータ群に、一つのモデルを当てはめたら、群毎に推定したパラメータの違いが気になります。 複数群をお互いに比較することは多重比較といいます。 一般的には、 群毎にパラメータを推定する full model から群毎のデータをまとめて、一つのパラメータを推定する pooled model まで考えられます。 このとき、nlsLM() は使用できないので、nls() 関数を使います。 nls() の収束を助けるために、まずは pooled model の nlsLM() の結果を full model のパラメータ初期値にします。\nでは、データをすこし整理します。\n\ndataset = dataset |> select(seaweed, data) |> unnest(data)\ndataset = dataset |> mutate(seaweed = factor(seaweed))\n\nフルモデル (full model) の場合は群毎に、P_{max}、\\alpha、R_d を推定します。 プールモデル (pooled model) の場合は、群の区別をせず、1 セットのパラメータを推定します。\n\n# フルモデルの初期値を求める\npoolmodel = nlsLM(np ~ pecurve(ppfd, pmax, rd, alpha), \n                  start = list(pmax = 10, rd = 3, alpha = 0.3),\n                  data = dataset, \n                  lower = c(pmax = 0, rd = 0, alpha = 0))\n\nnlsLM() は次のようにインデックスされた係数のモデルに対応していないので、nls() を使います。\n\nSTART = lapply(coef(poolmodel), rep, 3)\nfullmodel = nls(np ~ pecurve(ppfd, pmax[seaweed], rd[seaweed], alpha[seaweed]), \n                start = START, \n                data = dataset)\n\nAIC を確認すると、フルモデルの AIC が最も低いです。\n\nAIC(poolmodel, fullmodel) |> as_tibble(rownames = \"model\") |> arrange(AIC)\n\n# A tibble: 2 × 3\n  model        df   AIC\n  <chr>     <dbl> <dbl>\n1 fullmodel    10  320.\n2 poolmodel     4  353.\n\n\nすべてのモデルを当てはめて、AIC で比較することはできます。 3 変数・3 群のモデルの組み方は (3(3-1)/2)^3=125 とおりも考えられます。 125とおりのモデルを調べたくないので、Wald’s 検定でパラメータ比較をします。\n\n\n\n\n\n\nImportant\n\n\n\n【重要】：ここで評価したフルモデルと海藻毎に当てはめたモデルのモデル係数に期待値は同じじゃない！\n\nsummary(fullmodel)\n\n\nFormula: np ~ pecurve(ppfd, pmax[seaweed], rd[seaweed], alpha[seaweed])\n\nParameters:\n       Estimate Std. Error t value Pr(>|t|)    \npmax1  23.28613    4.73717   4.916 6.64e-06 ***\npmax2   5.24099    1.31169   3.996 0.000172 ***\npmax3   4.50104    1.45692   3.089 0.002983 ** \nrd1     2.07673    1.05259   1.973 0.052890 .  \nrd2     0.43151    1.14443   0.377 0.707404    \nrd3     0.82730    1.09158   0.758 0.451339    \nalpha1  0.08580    0.01999   4.293 6.20e-05 ***\nalpha2  0.16462    0.10022   1.643 0.105444    \nalpha3  0.07176    0.05493   1.306 0.196200    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.073 on 63 degrees of freedom\n\nNumber of iterations to convergence: 14 \nAchieved convergence tolerance: 5.833e-06\n\n\n推定した係数の違いはモデルの構造と関係しています。 それぞれの海藻に当てはめたとき、それぞれの海藻のモデルごとに独立した誤差項が存在するが、 上のフルモデルの場合 1 つの誤差項しかないです。\nつまり、それぞれの海藻にはつぎのモデルを当てはめたので、\\sigma 3 つ存在します。 これで、係数の期待値と標準誤差がきまります。\n\n\\begin{aligned}\n\\mu &= P_{max} \\left(1 - \\exp\\left(-\\frac{\\alpha}{P_{max}}I\\right)\\right)-R_d  \\\\\nP_{net} & \\sim N(\\mu, \\sigma)\n\\end{aligned}\n フルモデルは次のようになります。\n\n\\begin{aligned}\n\\mu_i &= P_{max,i} \\left(1 - \\exp\\left(-\\frac{\\alpha_{i}}{P_{max,i}}I\\right)\\right)-R_{d,i}  \\\\\nP_{net,i} & \\sim N(\\mu_i, \\sigma)\n\\end{aligned}\n i は海藻を区別するためのインデックス。 このモデルには 1 つの \\sigma しかないです。\nこの微妙な違いで、係数の期待値と標準誤差が変わります。\n\n\nパラメータの多重比較は aomisc のパッケージが有ると楽です。 インストール方法は remotes パッケージをつかって、github からインストールします。\n\nremotes::install_github(\"onofriAndreaPG/aomisc\")\n\naomisc パッケージの多重比較は Holm法を使います。\n\nHolm 法は p 値を小さい順になれべてから実施します。 最も小さい P 値の有意水準は \\alpha / N です。 N は比較する回数です。 ここで P \\leq \\alpha / N なら、 次の P 値を \\alpha / (N-1) で評価します。 P > \\alpha / (N-1) なら、ここで検定が終わります。 帰無仮説を棄却できないまで、N-k の補正で P 値を評価続けます。\n\n\nlibrary(aomisc)\n\nLoading required package: drc\n\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\n\n'drc' has been loaded.\n\n\nPlease cite R and 'drc' if used for a publication,\n\n\nfor references type 'citation()' and 'citation('drc')'.\n\n\n\nAttaching package: 'drc'\n\n\nThe following objects are masked from 'package:stats':\n\n    gaussian, getInitial\n\n\nLoading required package: plyr\n\n\n------------------------------------------------------------------------------\n\n\nYou have loaded plyr after dplyr - this is likely to cause problems.\nIf you need functions from both plyr and dplyr, please load plyr first, then dplyr:\nlibrary(plyr); library(dplyr)\n\n\n------------------------------------------------------------------------------\n\n\n\nAttaching package: 'plyr'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\n\nThe following object is masked from 'package:purrr':\n\n    compact\n\n\nLoading required package: car\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nLoading required package: multcompView\n\ncfs = summary(fullmodel)$coef\ndf = summary(fullmodel)$df\n\npmax の多重比較は次のとおりです。 3つ目の比較まで評価しました\n\nrows = 1:3\npairComp(cfs[rows,1], cfs[rows,2],dfr = df[2], adjust = \"holm\")\n\n$pairs\n\n     Simultaneous Tests for General Linear Hypotheses\n\nLinear Hypotheses:\n                 Estimate Std. Error t value Pr(>|t|)   \npmax1-pmax2 == 0   18.045      4.915   3.671  0.00102 **\npmax1-pmax3 == 0   18.785      4.956   3.790  0.00102 **\npmax2-pmax3 == 0    0.740      1.960   0.377  0.70711   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- holm method)\n\n\n$Letters\n           Mean       SE CLD\npmax1 23.286128 4.737174   a\npmax2  5.240990 1.311692   b\npmax3  4.501039 1.456917   b\n\n\nalphaとrdの場合、P > \\alpha/N ので、3つ目比較までしません。\n\nrows = 4:6\npairComp(cfs[rows,1], cfs[rows,2],dfr = df[2], adjust = \"holm\")\n\n$pairs\n\n     Simultaneous Tests for General Linear Hypotheses\n\nLinear Hypotheses:\n             Estimate Std. Error t value Pr(>|t|)\nrd1-rd2 == 0   1.6452     1.5549   1.058    0.882\nrd1-rd3 == 0   1.2494     1.5164   0.824    0.882\nrd2-rd3 == 0  -0.3958     1.5815  -0.250    0.882\n(Adjusted p values reported -- holm method)\n\n\n$Letters\n         Mean       SE CLD\nrd1 2.0767329 1.052591   a\nrd2 0.4315057 1.144426   a\nrd3 0.8273007 1.091576   a\n\n\n\nrows = 7:9\npairComp(cfs[rows,1], cfs[rows,2],dfr = df[2], adjust = \"holm\")\n\n$pairs\n\n     Simultaneous Tests for General Linear Hypotheses\n\nLinear Hypotheses:\n                   Estimate Std. Error t value Pr(>|t|)\nalpha1-alpha2 == 0 -0.07882    0.10219  -0.771        1\nalpha1-alpha3 == 0  0.01404    0.05846   0.240        1\nalpha2-alpha3 == 0  0.09286    0.11428   0.813        1\n(Adjusted p values reported -- holm method)\n\n\n$Letters\n             Mean         SE CLD\nalpha1 0.08579966 0.01998607   a\nalpha2 0.16461744 0.10021636   a\nalpha3 0.07175792 0.05493227   a\n\n\naomisc パッケージを読み込むと、drc と MASS パッケージも同時に読み込まれます。 tidyverse を読み込んだあとに、MASS パッケージを読み込むと MASS の select() 関数が定義され、 tidyverse の select() が使えなくなります。 aomisc は不要になったので、次の 3 つのパッケージをディタッチ (detach, unload) します。\n\ndetach(package:aomisc)\ndetach(package:drc)\ndetach(package:MASS)"
  },
  {
    "objectID": "nonlinear.html#モデル係数の相関関係",
    "href": "nonlinear.html#モデル係数の相関関係",
    "title": "非線形モデル",
    "section": "モデル係数の相関関係",
    "text": "モデル係数の相関関係\n上で多重比較をしましたが、非線形モデルのパラメータはお互いとの相関関係が強いことが多いです。 このとき、多重比較にバイアス (bias) が入り、フェアな比較はできません。 光合成光曲線の解析から推定したパラメータの相関関係は次の図の通りです。\n\nfm_cov  = vcov(fullmodel) # モデルパラメータの分散共分散行列\nfm_corr = cov2cor(fm_cov) # モデルパラメータの相関行列\n\n\nvname1 = str_c(c(\"pmax\", \"alpha\", \"rd\"), \"1\")\nvname2 = str_c(c(\"pmax\", \"alpha\", \"rd\"), \"2\")\nvname3 = str_c(c(\"pmax\", \"alpha\", \"rd\"), \"3\")\np1 = ggcorrplot(fm_corr[vname1, vname1],     type = \"upper\", show.diag = T, lab =T)\np2 = ggcorrplot(fm_corr[vname2, vname2], type = \"upper\", show.diag = T, lab = T)\np3 = ggcorrplot(fm_corr[vname3, vname3], type = \"upper\", show.diag = T, lab = T)\np1 + p2 + p3 + plot_layout(ncol = 1)\n\n\n\n\n光合成光曲線の解析からもとめたパラメータの相関関係を示しています。種ごとにまとめて、3つの相関プロットに示した。（右）ウミトラノオ、（中）コブクロモク、（左）幼体。相関係数は -1 から　1 をとります。ウミトラノオ以外の場合では、パラメータごとの関係に正の相関があります。\n\n\n\n\n\nvname1 = str_c(c(\"pmax\", \"alpha\", \"rd\"), \"1\")\nvname2 = str_c(c(\"pmax\", \"alpha\", \"rd\"), \"2\")\nvname3 = str_c(c(\"pmax\", \"alpha\", \"rd\"), \"3\")\np1 = ggcorrplot(fm_cov[vname1, vname1],     type = \"upper\", show.diag = T, lab =T, legend.title = \"Cov\")\np2 = ggcorrplot(fm_cov[vname2, vname2], type = \"upper\", show.diag = T, lab = T, legend.title = \"Cov\")\np3 = ggcorrplot(fm_cov[vname3, vname3], type = \"upper\", show.diag = T, lab = T, legend.title = \"Cov\")\np1 + p2 + p3 + plot_layout(ncol = 1)\n\n\n\n\n光合成光曲線の解析からもとめたパラメータの共分散を示しています。種ごとにまとめて、3つの共分散プロットに示した。（右）ウミトラノオ、（中）コブクロモク、（左）幼体。分散は正の値しか取れませんが、共分散は負の値をとっても大丈夫です。ウミトラノオの場合は負の値になっています。\n\n\n\n\nモデル係数はお互いに独立していないことが明確ですね。 これにより、Wald’s 検定の結果はそのまま受け入れないほうがいいです。"
  },
  {
    "objectID": "nonlinear.html#ガウス誤差伝播法",
    "href": "nonlinear.html#ガウス誤差伝播法",
    "title": "非線形モデル",
    "section": "ガウス誤差伝播法",
    "text": "ガウス誤差伝播法\nガウス誤差伝播法 (Gaussian error propagation, GEP) は直接推定したパラメータ（係数）から 間接的に求めたパラメータの誤差を推定するための手法です。 GEP においては、誤差が正規分布に従うことと、パラメータ推定にバイアスがないことが条件です。 ところがほとんどのモデルは、この条件を満たしていません。 それにしても、推定した誤差がパラメータ期待値の 10% を下回れば、信頼できる結果と考えられます。\nGEPを実施するには、つぎの式を応用します。 \\sigma_f^2 は間接的に求めたパラメータ f の分散です。 g は間接的に求めたパラメータ f の偏微分方程式の行列です。 V は直接推定したパラメータの分散共分散行列です。\n\n\\sigma_f^2 = \\mathbf{g}^T\\mathbf{V}\\mathbf{g}\n \n\\mathbf{g} =\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial \\beta_x}\\\\\n\\frac{\\partial f}{\\partial \\beta_y}\\\\\n\\frac{\\partial f}{\\partial \\beta_z}\\\\\n\\end{bmatrix}\n\n\n\\mathbf{V} =\n\\begin{bmatrix}\n\\sigma_{xx}^2 & \\sigma_{xy}^2 & \\sigma_{xz}^2 \\\\\n\\sigma_{xy}^2 & \\sigma_{yy}^2 & \\sigma_{yz}^2 \\\\\n\\sigma_{xz}^2 & \\sigma_{yz}^2 & \\sigma_{zz}^2 \\\\\n\\end{bmatrix}\n\n\n\\sigma_f^2 =\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial \\beta_x}&\n\\frac{\\partial f}{\\partial \\beta_y}&\n\\frac{\\partial f}{\\partial \\beta_z}\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\sigma_{xx}^2 & \\sigma_{xy}^2 & \\sigma_{xz}^2 \\\\\n\\sigma_{xy}^2 & \\sigma_{yy}^2 & \\sigma_{yz}^2 \\\\\n\\sigma_{xz}^2 & \\sigma_{yz}^2 & \\sigma_{zz}^2 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial \\beta_x}\\\\\n\\frac{\\partial f}{\\partial \\beta_y}\\\\\n\\frac{\\partial f}{\\partial \\beta_z}\\\\\n\\end{bmatrix}\n\n\n\\begin{aligned}\n\\sigma_f^2 &=\n\\frac{\\partial f}{\\partial \\beta_x}\\left(\\frac{\\partial f}{\\partial \\beta_x}\\sigma_{xx}^2 +\\frac{\\partial f}{\\partial \\beta_y}\\sigma_{xy}^2 +\\frac{\\partial f}{\\partial \\beta_z}\\sigma_{xz}^2\\right) \\\\\n{} &+\n\\frac{\\partial f}{\\partial \\beta_y}\\left(\\frac{\\partial f}{\\partial \\beta_x}\\sigma_{xy}^2 +\\frac{\\partial f}{\\partial \\beta_y}\\sigma_{yy}^2 +\\frac{\\partial f}{\\partial \\beta_z}\\sigma_{yz}^2\\right) \\\\\n{} &+\n\\frac{\\partial f}{\\partial \\beta_z}\\left(\\frac{\\partial f}{\\partial \\beta_x}\\sigma_{xz}^2 +\\frac{\\partial f}{\\partial \\beta_y}\\sigma_{yz}^2 +\\frac{\\partial f}{\\partial \\beta_z}\\sigma_{zz}^2\\right)\n\\end{aligned}\n\n\n\\begin{aligned}\n\\sigma_f^2 &=\n\\left(\\frac{\\partial f}{\\partial \\beta_x}\\right)^2\\sigma_{xx}^2 +\n\\frac{\\partial f}{\\partial \\beta_x}\\frac{\\partial f}{\\partial \\beta_y}\\sigma_{xy}^2 +\n\\frac{\\partial f}{\\partial \\beta_x}\\frac{\\partial f}{\\partial \\beta_z}\\sigma_{xz}^2 \\\\\n{} & +\n\\left(\\frac{\\partial f}{\\partial \\beta_y}\\right)^2\\sigma_{yy}^2 +\n\\frac{\\partial f}{\\partial \\beta_y}\\frac{\\partial f}{\\partial \\beta_x}\\sigma_{xy}^2 +\n\\frac{\\partial f}{\\partial \\beta_y}\\frac{\\partial f}{\\partial \\beta_z}\\sigma_{yz}^2 \\\\\n{} &+\n\\left(\\frac{\\partial f}{\\partial \\beta_z}\\right)^2\\sigma_{zz}^2 +\n\\frac{\\partial f}{\\partial \\beta_z}\\frac{\\partial f}{\\partial \\beta_x}\\sigma_{xz}^2 +\n\\frac{\\partial f}{\\partial \\beta_z}\\frac{\\partial f}{\\partial \\beta_y}\\sigma_{yz}^2\n\\end{aligned}\n\n\n\\begin{aligned}\n\\sigma_f^2 &=\n\\left(\\frac{\\partial f}{\\partial \\beta_x}\\right)^2\\sigma_{xx}^2 +\n\\left(\\frac{\\partial f}{\\partial \\beta_y}\\right)^2\\sigma_{yy}^2 +\n\\left(\\frac{\\partial f}{\\partial \\beta_z}\\right)^2\\sigma_{zz}^2 \\\\\n{} &+\n2\\left(\\frac{\\partial f}{\\partial \\beta_x}\\frac{\\partial f}{\\partial \\beta_y}\\sigma_{xy}^2 +\n\\frac{\\partial f}{\\partial \\beta_x}\\frac{\\partial f}{\\partial \\beta_z}\\sigma_{xz}^2 +\n\\frac{\\partial f}{\\partial \\beta_y}\\frac{\\partial f}{\\partial \\beta_z}\\sigma_{yz}^2 \\right)\n\\end{aligned}\n\n参考文献：\n\nTellinghuisen J. 2000. Statistical error propagation. Journal of Physical Chemistry A 104: 2834 - 2844.\nTellinghuisen J. 2001. Statistical error propagation. Journal of Physical Chemistry A 105: 3917 - 3921.\nLo E. 2005. Gaussian error propagation applied to ecological data: Post-ice-storm-downed woody biomass. Ecological Monographs 75: 451-466.\n\n\n\\sigma_f^2=\\sum_{i = 1}^n \\left(\\frac{\\partial q}{\\partial x_i}\\sigma_{x_i}\\right)^2 + 2\\sum_{i = 1}^n\\sum_{j = 1,j\\neq i}^n \\left(\\frac{\\partial q}{\\partial x_i}\\frac{\\partial q}{\\partial x_j}\\rho_{x_i x_j}\\sigma_{x_i}\\sigma_{x_j}\\right)\n\n共分散と相関の関係は次の通りです。\n\n\\overbrace{\\rho_{xy}}^\\text{相関}= \\overbrace{\\sigma_{xy}}^\\text{共分散} / \\underbrace{(\\sigma_x\\sigma_y)}_{x,y\\;\\text{の標準偏差}}\n\nでは、Rで解析に使ったモデルの偏微分方程式を求めます。 光飽和点 (I_k) と光補償点 (I_c) の formula は次のように定義します。\n\n\\begin{aligned}\nI_k & = f_1 = P_{max} / \\alpha \\\\\nI_c & = f_2 = \\frac{P_{max}}{\\alpha} \\ln\\left(\\frac{P_{max}}{P_{max} - R_d}\\right) \\\\\n\\end{aligned}\n\n\nfic = ic ~ pmax/alpha\nfik = ik ~ pmax/alpha * log(pmax/(pmax - rd))\n\nfic と fik は 3つの要素で組み立てられた formula です。 たとえば、ficの場合は次のようになっています。\n\nclass(fic)\n\n[1] \"formula\"\n\nlength(fic)\n\n[1] 3\n\nfic[[1]]\n\n`~`\n\nfic[[2]]\n\nic\n\nfic[[3]]\n\npmax/alpha\n\n\nそれぞれの偏微分方程式は次のとおりになります。\n光補償点 (f_1) の場合、 P_{max} と \\alpha に対する式は次のとおりです。\n\n\\begin{aligned}\n\\frac{\\partial f_1}{\\partial \\alpha} &= \\frac{-P_{max}}{\\alpha^2} \\\\\n\\frac{\\partial f_1}{\\partial P_{max}} &= \\frac{1}{\\alpha} \\\\\n\\end{aligned}\n\n光飽和点 (f_2) の場合、R_d　もあるので式は次のとおりです。\n\n\\begin{aligned}\n\\frac{\\partial f_2}{\\partial \\alpha} &= -\\frac{P_{max}}{\\alpha^2} \\log\\left(\\frac{P_{max}}{P_{max} - R_d}\\right) \\\\\n\\frac{\\partial f_2}{\\partial P_{max}} &= \\frac{1}{\\alpha} \\log\\left(\\frac{P_{max}}{P_{max} - R_d}\\right)\n+ \\left(\\frac{1}{\\alpha} - \\frac{P_{max}}{P_{max}-R_d}\\right) \\\\\n\\frac{\\partial f_2}{\\partial R_{d}} &= \\frac{P_{max}}{\\alpha\\,(P_{max}-R_d)} \\\\\n\\end{aligned}\n Rでは、次の関数をつかって、光合成光曲線の光補償点と光飽和点の誤差を求めます。 関数に rlang パッケージの関数をつかています。\n\npropagate_error = function(model, dmodel, parameters) {\n  require(rlang)  \n  cfs = coefficients(model)    # モデル係数の期待値\n  vars = all.vars(dmodel[[3]]) # 偏微分方程式の対象となる変数\n  V = vcov(model)              # 当てはめたモデルの分散共分散行列\n  \n  V = V[parameters, parameters] # 必要な分散共分散の抽出\n  expectation = cfs[parameters] # 必要な期待値の抽出\n  \n  # 偏微分方程式はここで求めています。\n  gradient_fn = deriv(dmodel[[3]], vars, function.arg = T)\n  ff = expr(gradient_fn(!!!syms(vars)))\n  tmp =  exprs(!!!expectation)\n  tmp = set_names(tmp, vars)\n    for(i in 1:length(vars)) {\n    call2(\"=\", expr(!!vars[i]), expectation[i]) |> eval()\n  }\n  G = eval(ff)\n  G = attributes(G)$gradient |> matrix(ncol = 1)\n  \n  # g^T V g\n  sqrt((t(G) %*% V) %*% G)\n}\n\n直接推定したモデル係数を抽出して、期待値と標準誤差だけ残します。\n\ncfsout = summary(fullmodel)$coef |> as_tibble(rownames = \"parameter\") |> \n  select(parameter, est=Estimate, se = `Std. Error`) |> \n  mutate(id = str_extract(parameter, \"[0-9]\"),\n         parameter = str_extract(parameter, \"[A-z]+\")) |> \n  pivot_wider(names_from = parameter,\n              values_from = c(est, se),\n              names_glue = \"{.value}_{parameter}\")\n\n間接的に推定したパラメータを追加します。\n\ncnames = c(\"Seaweed\", \"Parameter\", \"Estimate\", \"SE\")\ncfsout = cfsout |> \n  mutate(est_ic = est_pmax / est_alpha,\n         est_ik = est_pmax / est_alpha * log(est_pmax / (est_pmax - est_rd))) |> \n  mutate(pmax = str_c(\"pmax\", id),\n         alpha = str_c(\"alpha\", id),\n         rd = str_c(\"rd\", id),\n         seaweed = c(\"ウミトラノオ\", \"コブクロモク\", \"幼体\"))\n\npropagate_error() をそれぞれの海藻に適応します。\n\napply_propagate_error_fic = function(pmax, alpha, rd) {\n    fic = ic ~ pmax / alpha\n    propagate_error(fullmodel, fic, c(pmax, alpha))\n}\n\napply_propagate_error_fik = function(pmax, alpha, rd) {\n  fik = ik ~ pmax / alpha * log(pmax / (pmax - rd))\n    propagate_error(fullmodel, fik, c(pmax, alpha, rd))\n}\n\ncfsout = cfsout |> \n  mutate(se_ic = pmap_dbl(list(pmax, alpha, rd), apply_propagate_error_fic)) |> \n  mutate(se_ik = pmap_dbl(list(pmax, alpha, rd), apply_propagate_error_fik)) |> \n           select(-alpha, -pmax, -rd) |> \n  pivot_longer(cols = matches(\"est_|se_\"),\n               names_to = c(\"stat\", \"par\"),\n               names_pattern = \"(est|se)_(.*)\") |> \n  pivot_wider(names_from = stat, values_from = value) |> \n  arrange(par, id) |> \n  select(-id)\n\nLoading required package: rlang\n\n\n\nAttaching package: 'rlang'\n\n\nThe following objects are masked from 'package:purrr':\n\n    %@%, as_function, flatten, flatten_chr, flatten_dbl, flatten_int,\n    flatten_lgl, flatten_raw, invoke, splice\n\n\n\ncfsout |> \n  kableExtra::kbl(digits = c(0,0,3,2),\n                  col.names = cnames)\n\n\n\n \n  \n    Seaweed \n    Parameter \n    Estimate \n    SE \n  \n \n\n  \n    ウミトラノオ \n    alpha \n    0.086 \n    0.02 \n  \n  \n    コブクロモク \n    alpha \n    0.165 \n    0.10 \n  \n  \n    幼体 \n    alpha \n    0.072 \n    0.05 \n  \n  \n    ウミトラノオ \n    ic \n    271.401 \n    109.63 \n  \n  \n    コブクロモク \n    ic \n    31.837 \n    17.59 \n  \n  \n    幼体 \n    ic \n    62.725 \n    49.15 \n  \n  \n    ウミトラノオ \n    ik \n    25.353 \n    9.00 \n  \n  \n    コブクロモク \n    ik \n    2.735 \n    6.34 \n  \n  \n    幼体 \n    ik \n    12.739 \n    11.54 \n  \n  \n    ウミトラノオ \n    pmax \n    23.286 \n    4.74 \n  \n  \n    コブクロモク \n    pmax \n    5.241 \n    1.31 \n  \n  \n    幼体 \n    pmax \n    4.501 \n    1.46 \n  \n  \n    ウミトラノオ \n    rd \n    2.077 \n    1.05 \n  \n  \n    コブクロモク \n    rd \n    0.432 \n    1.14 \n  \n  \n    幼体 \n    rd \n    0.827 \n    1.09"
  },
  {
    "objectID": "maps.html#必要なパッケージ",
    "href": "maps.html#必要なパッケージ",
    "title": "地図の作り方",
    "section": "必要なパッケージ",
    "text": "必要なパッケージ\n\nlibrary(tidyverse)　# Essential package\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.4.0 \n✔ readr   2.1.2      ✔ forcats 0.5.1 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(ggpubr)     # Publication-oriented figures\nlibrary(kableExtra) # Tables\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(magick)     # Imagemagick R API\n\nLinking to ImageMagick 6.9.11.60\nEnabled features: fontconfig, freetype, fftw, heic, lcms, pango, webp, x11\nDisabled features: cairo, ghostscript, raw, rsvg\nUsing 32 threads\n\nlibrary(patchwork)  # Simplified figure tiling\nlibrary(showtext)   # I want to use google fonts in the figures\n\nLoading required package: sysfonts\nLoading required package: showtextdb\n\n\n次の2つは地図専用のパッケージです。\n\nlibrary(ggspatial)  # Essential for map-making with ggplot\nlibrary(sf)         # Essential for map data manipulation\n\nLinking to GEOS 3.9.0, GDAL 3.2.2, PROJ 7.2.1; sf_use_s2() is TRUE\n\n\nNoto Sans のフォントが好きなので、ここで Google Fonts からアクセスします。\n\nfont_add_google(\"Noto Sans\",\"notosans\")\n\nggplot のデフォルトテーマも設定し、フォント埋め込みも可能にします。 ここでデフォルトを設定すると、毎回 theme_pubr() を ggplotのチェインにたさなくていい。\n\ntheme_pubr(base_size = 10, base_family = \"notosans\") |> theme_set()\nshowtext_auto() # Automatically embed the Noto Sans fonts into the ggplots."
  },
  {
    "objectID": "maps.html#シェープファイルの読み込み",
    "href": "maps.html#シェープファイルの読み込み",
    "title": "地図の作り方",
    "section": "シェープファイルの読み込み",
    "text": "シェープファイルの読み込み\nシェープファイル (shapefile) は地図データのことです。 基本的の拡張子は shp, shx, dbf　ですが、その他に prj と xml もあります。\n研究室用にダウンロードした 国土交通省・国土数値情報ダウンロードサービス のシェープファイルは ~/Lab_Data/Japan_map_data/Japan に入っています。\n\nmlit = read_sf(\"~/Lab_Data/Japan_map_data/Japan/N03-20210101_GML/\")\n\nmlit に読み込んだシェープファイルはここへ。\nシェープファイルの 座標参照系 (CRS: Coordinate Reference System) を確認しましょう。\n\nst_crs(mlit)\n\nCoordinate Reference System:\n  User input: JGD2011 \n  wkt:\nGEOGCRS[\"JGD2011\",\n    DATUM[\"Japanese Geodetic Datum 2011\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"Japan - onshore and offshore.\"],\n        BBOX[17.09,122.38,46.05,157.65]],\n    ID[\"EPSG\",6668]]\n\n\nCRSには 地理座標系 と 投影座標系 の2種類があります。 座標系にはEPSGコードもつけられています。\n\n# HTML 用テーブル\ntibble(`EPSG Code` = c(4326,6668,6677),\n       `CRS` = c(\"WGS84\", \"JGD2011\", \"JGD2011 / Japan Plane Rectangular CS IX\"),\n       `Units` = c(\"degrees\", \"degrees\", \"meters\")) |> \n  kbl() |> \n  kable_styling(bootstrap_options = c(\"hover\"))\n\n\n\n \n  \n    EPSG Code \n    CRS \n    Units \n  \n \n\n  \n    4326 \n    WGS84 \n    degrees \n  \n  \n    6668 \n    JGD2011 \n    degrees \n  \n  \n    6677 \n    JGD2011 / Japan Plane Rectangular CS IX \n    meters \n  \n\n\n\n\n\nこのデータは政策区域のデータなので、とても重いです。 まずは、都道府県ごとにまとめた RDS ファイルを作って保存します。 都道府県ごとに st_union() を使って polgyon データを結合します。 結合したデータを unnest して、simple feature に戻してかた保存します。 121158 features もあるので、数時間もかります。\n沿岸のデータだけなら軽いですので、C23 シリーズのファイルを読み込みます。\n\nmlit = tibble(folder = dir(\"~/Lab_Data/Japan_map_data/Coastline/\", full = TRUE)) |> \n  mutate(data = map(folder, read_sf)) |> select(data) |> \n  unnest(data) |> \n  st_as_sf(crs = st_crs(6668))\n\nでは、ここで地図の確認をします。\n\nmlit |> ggplot() + geom_sf()\n\n\n\n\nmlit のデータは細かい政策区域まで分けられているので、全国スケールの図には向いていません。 st_union() をつかって、都道府県ごとに polygon を結合したファイルは、~/Lab_Data/Japan_map_data/Japan/todofuken.rds に保存しています。 次のコードで、都道府県ごとにまとめましたが、並列処理でも５時間以上もかかったので、RDS ファイルを使いましょう。\n\n# Takes 5.5 hours to complete with 30 cores!\n# library(furrr)\n# plan(multisession, workers = 30)\n# Group by prefecture\n# mlit1 = mlit |> group_nest(N03_001) |> \n#   # mutate(data = future_map(data, st_union)) |> \n#   unnest(data) |> st_as_sf() \n# mlit1 |> write_rds(\"~/Lab_Data/Japan_map_data/Japan/todofuken.rds\")\n\n\nmlit1 = read_rds(\"~/Lab_Data/Japan_map_data/Japan/todofuken.rds\")\n\n\nmlit1 |> ggplot() + geom_sf()"
  },
  {
    "objectID": "maps.html#調査地点のデータを準備する",
    "href": "maps.html#調査地点のデータを準備する",
    "title": "地図の作り方",
    "section": "調査地点のデータを準備する",
    "text": "調査地点のデータを準備する\n形上湾アマモ場調査のステーションの GPS tibble を準備する。\n\nzostera = read_csv(\"~/Lab_Data/matsumuro/Katagami_Bay/longlat_info.csv\")\n\nRows: 105 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): eelgrass\ndbl  (4): Name, lat, long, coverage(%)\ndttm (1): datetime\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nzostera |> print(n = 3)\n\n# A tibble: 105 × 6\n   Name   lat  long datetime            eelgrass `coverage(%)`\n  <dbl> <dbl> <dbl> <dttm>              <chr>            <dbl>\n1     1  33.0  130. 2021-05-25 09:14:48 absent               0\n2     2  33.0  130. 2021-05-25 09:30:32 absent               0\n3     3  33.0  130. 2021-05-25 09:37:16 present              5\n# … with 102 more rows\n\n\nzostera に緯度経度を設定する。 CRS は mlit と同じにします。\n\nzostera = zostera |> st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(mlit))\nzostera |> print(n = 3)\n\nSimple feature collection with 105 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 129.7845 ymin: 32.90032 xmax: 129.806 ymax: 32.95375\nGeodetic CRS:  JGD2011\n# A tibble: 105 × 5\n   Name datetime            eelgrass `coverage(%)`            geometry\n* <dbl> <dttm>              <chr>            <dbl>         <POINT [°]>\n1     1 2021-05-25 09:14:48 absent               0 (129.7985 32.95375)\n2     2 2021-05-25 09:30:32 absent               0 (129.7987 32.95258)\n3     3 2021-05-25 09:37:16 present              5  (129.7963 32.9519)\n# … with 102 more rows"
  },
  {
    "objectID": "maps.html#九州データの抽出",
    "href": "maps.html#九州データの抽出",
    "title": "地図の作り方",
    "section": "九州データの抽出",
    "text": "九州データの抽出\n九州のデータと長崎のデータを抽出します。 重要：長崎の名前が誤っています。Nagasaki のはずが、Naoasaki として記録されています。\n\ntoget = \"長崎|福岡|大分|佐賀|熊本|鹿児島|宮崎\"\nkyushu = mlit1 |> filter(str_detect(N03_001, toget))\n\n海岸線のデータ (mlit) から長崎の情報を抽出したいが、このデータの位置情報はコードで記述されています。\n\nadmincode = readxl::read_xlsx(\"~/Lab_Data/Japan_map_data/AdminiBoundary_CD.xlsx\", skip = 2)\nadmincode = admincode |> select(code = matches(\"行政\"), N03_001 = matches(\"都道府県*.*漢字\"))\ncodes = admincode |> filter(str_detect(N03_001, \"長崎\")) |> pull(code)\n\n\nnagasaki = mlit |> filter(str_detect(C23_001, str_c(codes, collapse = \"|\"))) \n\n長崎の海岸線は次のようになります。\n\nggplot() + geom_sf(data = nagasaki)\n\n\n\n\n九州は mlit1 から抽出したので、都道府県政策区域として作図されます。\n\nggplot() + geom_sf(data = kyushu)\n\n\n\n\n長崎をハイライトしましょう。\n\nkyushu |> \n  mutate(fillme = str_detect(N03_001, \"長崎\")) |> \n  ggplot() + geom_sf(aes(fill = fillme), color = NA) +\n  guides(fill = \"none\") +\n  scale_fill_viridis_d() +\n  theme(panel.background = element_rect(fill = \"lightblue\", color = \"black\"),\n        axis.line = element_blank())\n\n\n\n\nこの図には、違和感を感じるので、山口、島根、愛媛、広島と高知も追加します。 そしれ、最初に作った kyushu の範囲を抽出しておきます。\n\nkbbox = kyushu |> st_bbox()\n\n\ntoget = \"長崎|福岡|大分|佐賀|熊本|鹿児島|宮崎|山口|島根|愛媛|高知|広島\"\nkyushu = mlit1 |> filter(str_detect(N03_001, toget))\n\n長崎、九州、その他の色分けをして、 kyushu をクロップします。 クロップ範囲は kbbox です。\n\nkyushu = kyushu |>\n  mutate(fillme = case_when(str_detect(N03_001, \"長崎\") ~ \"Nagasaki\",\n                            str_detect(N03_001, \"福岡|大分|佐賀|熊本|鹿児島|宮崎\") ~ \"Kyushu\",\n                            TRUE ~ \"Honshu\")) |> \n  st_crop(kbbox)\n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n\nこの地図は次のようになりました。\n\nggplot(kyushu) + \n  geom_sf(aes(fill = fillme), color = NA) +\n  guides(fill = \"none\") +\n  coord_sf(expand = FALSE) +\n  scale_fill_viridis_d() +\n  theme(panel.background = element_rect(fill = \"lightblue\", color = \"black\"),\n        axis.line = element_blank())"
  },
  {
    "objectID": "maps.html#調査地点の図",
    "href": "maps.html#調査地点の図",
    "title": "地図の作り方",
    "section": "調査地点の図",
    "text": "調査地点の図\n形上湾と大村湾の図を作ります。 形上湾の方には、調査地点と結果ものせます。 まずは形上湾と大村湾の範囲を決めます。 範囲は Google Map で選びました。\n\nkatagami = rbind(c(32.95809069048365, 129.7669185309373),\n                 c(32.89802000729197, 129.82832411747583)) |>\n  as_tibble(.name_repair = ~c(\"lat\", \"long\")) |> \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(kyushu))\n\nomurabay = rbind(rev(c(33.103196388120104, 129.67183787501082)),\n                 rev(c(32.817013859622804, 130.03298144413574))) |> \n  as_tibble(.name_repair = \\(x) c(\"long\", \"lat\")) |>\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(kyushu))\n\nここで、それぞれの湾のデータを kyushu からぬきます。\n\nomurabay_area = kyushu |> filter(str_detect(N03_001, \"長崎\")) |> st_crop(st_bbox(omurabay)) \n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\nkatagami_area = kyushu |> filter(str_detect(N03_001, \"長崎\")) |> st_crop(st_bbox(katagami)) \n\nWarning: attribute variables are assumed to be spatially constant throughout all\ngeometries\n\n\nアマモの被度データの simple features データを準備します。\n\nzostera = zostera |>\n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(kyushu)) |> \n  rename(coverage = matches(\"cover\")) |> \n  mutate(rank = cut(coverage, \n                    c(-Inf, 1, 10, 40, 70, Inf),\n                    labels = c(\"E\", \"D\", \"C\", \"B\", \"A\"))) |> \n  mutate(rank = factor(rank, \n                       levels = LETTERS[1:5],\n                       labels = LETTERS[1:5]))\n\n九州の図を先につくります。\n\n# The main plot of kyushu\npmain = ggplot(kyushu) + \n  geom_sf(aes(fill = fillme), color = NA) +\n  guides(fill = \"none\") +\n  coord_sf(expand = FALSE) +\n  scale_fill_viridis_d() +\n  theme(panel.grid = element_blank(),\n        panel.background = element_rect(fill = \"lightblue\", color =\"black\"),\n        panel.border  = element_rect(fill = NA, color =\"black\"),\n        plot.background =  element_rect(fill = NA, color =NA),\n        axis.title = element_blank(),\n        axis.line = element_blank())\n\n大村湾と形上湾の図を次に作りますが、先にラベルの tibble を準備します。 tibble の long と lat のデータは試行錯誤で来ました。 もっといい方法はあるはずです。\n\n# Build plots for Omura Bay and Katagami Bay.\ntmp1 = omurabay_area |> st_transform(crs = st_crs(6677)) |> st_bbox()\ntmp2 = katagami_area |> st_transform(crs = st_crs(6677)) |> st_bbox()\n# tibble for labeling figures. The long and lat are by trial-and-error.\n# Need to find a better method.\nlabel1 = tibble(long = tmp1[3] -2500,\n                lat = tmp1[2] +1700,\n                label = \"Omura Bay, Nagasaki, Japan\") |> \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(6677), agr = \"constant\") |> \n  st_transform(crs = st_crs(omurabay_area))\n\nlabel2 = tibble(long = tmp2[1] +800,\n                lat = tmp2[4] -150,\n                label = \"Katagami Bay, Nagasaki, Japan\") |> \n  st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(6677), agr = \"constant\") |> \n  st_transform(crs = st_crs(omurabay_area))\n\nでは、大村湾と形上湾の地図をつくります。\n\npomura = ggplot() +\n  geom_sf(fill = \"grey50\", data = omurabay_area, size = 0) +\n  geom_sf_text(aes(label = label), \n               data = label1,\n               color = \"white\",\n               family = \"notosans\", \n               fontface = \"bold\",\n               vjust = 1, hjust = 1,\n               size = 5)  + \n  coord_sf(expand = FALSE) +\n  annotation_north_arrow(style = north_arrow_minimal(text_family = \"notosans\", \n                                                     text_face = \"bold\",\n                                                     line_width = 2,\n                                                     text_size = 20),\n                         pad_y = unit(0.3, \"npc\")) + \n  theme(panel.background = element_rect(fill = \"lightblue\", color =\"black\"),\n        panel.border  = element_rect(fill = NA, color =\"black\"),\n        plot.background =  element_rect(fill = \"white\", color =NA),\n        axis.title = element_blank(),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank())\n\npkatagami = ggplot() +\n  geom_sf(fill = \"grey50\", data = katagami_area, size = 0) +\n  geom_sf(aes(fill = rank), data = zostera,\n          pch = 21, size = 3,\n          color = \"white\", stroke = 1) +\n  geom_sf_text(aes(label = label), \n               data = label2,\n               color = \"white\",\n               family = \"notosans\", \n               fontface = \"bold\",\n               vjust = 1.0, hjust = 0.0,\n               size = 5)  + \n  annotation_north_arrow(style = north_arrow_minimal(text_family = \"notosans\", \n                                                     text_face = \"bold\",\n                                                     line_width = 2,\n                                                     text_size = 20)) + \n  coord_sf(expand = FALSE, crs = st_crs(katagami_area)) +\n  scale_fill_viridis_d(end = 0.8) +\n  theme(panel.grid = element_blank(),\n        panel.background = element_rect(fill = \"lightblue\", color =\"black\"),\n        panel.border  = element_rect(fill = NA, color =\"black\"),\n        plot.background =  element_rect(fill = \"white\", color =NA),\n        axis.title = element_blank(),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank())\n\npatchwork のパッケージをつかって、図を組み立てます。 図は PDF に保存したら、magick を使って、PNGにも変換します。\n\npout = pmain + (pomura / pkatagami)\npdfname = \"Images/katagami-map-v1.pdf\"\npngname = str_replace(pdfname, \"pdf\", \"png\")\nggsave(pdfname, plot= pout, width = 300, height = 300, units = \"mm\")\nimage_read_pdf(pdfname, density = 600) |> image_trim() |> image_border(color = \"white\") |> image_write(pngname)\n\n\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data\n\nWarning in st_point_on_surface.sfc(sf::st_zm(x)): st_point_on_surface may not\ngive correct results for longitude/latitude data"
  },
  {
    "objectID": "japan-zenkoku.html",
    "href": "japan-zenkoku.html",
    "title": "全国の地図",
    "section": "",
    "text": "library(tidyverse)　# Essential package\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.4.0 \n✔ readr   2.1.2      ✔ forcats 0.5.1 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(ggpubr)     # Publication-oriented figures\nlibrary(kableExtra) # Tables\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(magick)     # Imagemagick R API\n\nLinking to ImageMagick 6.9.11.60\nEnabled features: fontconfig, freetype, fftw, heic, lcms, pango, webp, x11\nDisabled features: cairo, ghostscript, raw, rsvg\nUsing 32 threads\n\nlibrary(patchwork)  # Simplified figure tiling\nlibrary(ggspatial)  # Essential for map-making with ggplot\nlibrary(sf)         # Essential for map data manipulation\n\nLinking to GEOS 3.9.0, GDAL 3.2.2, PROJ 7.2.1; sf_use_s2() is TRUE\n\nlibrary(showtext)   # I want to use google fonts in the figures\n\nLoading required package: sysfonts\nLoading required package: showtextdb\n\nlibrary(mapdata)    # Rough maps\n\nLoading required package: maps\n\nAttaching package: 'maps'\n\nThe following object is masked from 'package:purrr':\n\n    map\n\nlibrary(ggsflabel)  # Map labelling functions\n\n\nAttaching package: 'ggsflabel'\n\nThe following objects are masked from 'package:ggplot2':\n\n    geom_sf_label, geom_sf_text, StatSfCoordinates\n\n\nNoto Sans のフォントが好きなので、ここで Google Fonts からアクセスします。\n\nfont_add_google(\"Noto Sans JP\", \"notosans-jp\") # Japanese sans-serif font\nfont_add_google(\"Noto Sans\", \"notosans\")       # English sans-serif font\n\nNoto Fonts 類のフォントは研究室のサーバにインストール済みなので、次のコードで準備する。\n\nfont_add(\"notosans-jp\", \n         regular = \"NotoSansCJKjp-Regular.otf\",\n         bold = \"NotoSansCJKjp-Bold.otf\")\nfont_add(\"notosans\", \n         regular = \"NotoSans-Regular.ttf\",\n         bold = \"NotoSans-Bold.ttf\",\n         bolditalic = \"NotoSans-BoldItalic.ttf\",\n         italic = \"NotoSans-Italic.ttf\")\n\nサーバにインストールされているフォント名の検索は次のコードでできます。\n\nfont_files() |> as_tibble() |> \n  select(file, family, face, ps_name) |> \n  filter(str_detect(ps_name, \"NotoSans-[Reg|Bold|Ital]|NotoSansCJKjp\")) |> \n  print(n = 50)\n\nggplot のデフォルトテーマも設定し、フォント埋め込みも可能にします。 ここでデフォルトを設定すると、毎回 theme_pubr() を ggplotのチェインにたさなくていい。\n\ntheme_pubr(base_size = 10, base_family = \"notosans-jp\") |> theme_set()\nshowtext_auto() # Automatically embed the Noto Sans fonts into the ggplots."
  },
  {
    "objectID": "japan-zenkoku.html#シェープファイルの読み込み",
    "href": "japan-zenkoku.html#シェープファイルの読み込み",
    "title": "全国の地図",
    "section": "シェープファイルの読み込み",
    "text": "シェープファイルの読み込み\nシェープファイル (shapefile) は地図データのことです。 基本的の拡張子は shp, shx, dbf　ですが、その他に prj と xml もあります。\n研究室用にダウンロードした 国土交通省・国土数値情報ダウンロードサービス のシェープファイルは ~/Lab_Data/Japan_map_data/Japan に入っています。\nところが、情報量が多くて全国の地図には適していません。 とてもおもいです。 ここでは、mapdata の地図データを用います。 まずはデータを SpatialPolygon に変換し、CRS を JGD2011 に設定します。\nCRSには 地理座標系 と 投影座標系 の2種類があります。 座標系にはEPSGコードもつけられています。\n\n# HTML 用テーブル\ntibble(`EPSG Code` = c(4326,6668,6677),\n       `CRS` = c(\"WGS84\", \"JGD2011\", \"JGD2011 / Japan Plane Rectangular CS IX\"),\n       `Units` = c(\"degrees\", \"degrees\", \"meters\")) |> \n  kbl() |> \n  kable_styling(bootstrap_options = c(\"hover\"))\n\n\n\n \n  \n    EPSG Code \n    CRS \n    Units \n  \n \n\n  \n    4326 \n    WGS84 \n    degrees \n  \n  \n    6668 \n    JGD2011 \n    degrees \n  \n  \n    6677 \n    JGD2011 / Japan Plane Rectangular CS IX \n    meters \n  \n\n\n\n\n\nmaps パッケージから地図を準備する。\n\njpn = map(\"japan\", fill = TRUE, plot = FALSE)\n\nデータを ポリゴンに変換し、CRSを適応する。\n\njpn = maptools::map2SpatialPolygons(jpn, IDs = jpn$names)\njpn = jpn |> st_as_sf() |> st_set_crs(6668)\n\nポリゴンの解像度を減らす。\n\njpn = jpn |> \n  rmapshaper::ms_simplify(keep = 0.04, keep_shapes = F) |>\n  st_union()\n\nRegistered S3 method overwritten by 'geojsonlint':\n  method         from \n  print.location dplyr\n\n\n地図データを確認する。\n\ncolor = RColorBrewer::brewer.pal(9, \"Blues\")[2]\njpn |> \n  ggplot() + geom_sf() + \n  theme(panel.background = element_rect(fill = color),\n        panel.grid.major = element_line(color = \"white\", size = 0.5))\n\n\n\n\n地図の座標を UTM (Universal Transverse Mercator) に変換する。\n\njpn |> \n  st_transform(\"+proj=utm +zone=54 +datum=WGS84 +units=km\") %>% \n  ggplot() + \n  geom_sf(color = NA, fill = \"black\") + \n  theme(panel.background = element_rect(fill = color),\n        panel.grid.major = element_line(color = \"white\", size = 0.5))"
  },
  {
    "objectID": "japan-zenkoku.html#公開用地図の作成",
    "href": "japan-zenkoku.html#公開用地図の作成",
    "title": "全国の地図",
    "section": "公開用地図の作成",
    "text": "公開用地図の作成\n\nfudai       = c(40.04152543512538, 141.90348502302353)\nhirota      = c(39.02402594131857, 141.78725806724896)\nmatsushima  = c(38.34549669653925, 141.0807915733725)\nmie         = c(34.50235994784464, 136.85048430773975)\nnaruto      = c(34.22374792321184, 134.60913287860734)\nkamigoto    = c(32.98827976845565, 129.11838896005543)\ntokunoshima = c(27.763718381600228, 128.97442879693742)\nkaturen     = c(26.297604704320968, 127.8515917134318)\nchinen      = c(26.175546599376673, 127.83566562706314)\nishigaki    = c(24.380846276132317, 124.17950044492075)\nokinawa     = c(26.297604704320968, 127.8515917134318)\nupper       = c(33.94130434786708, 130.16535814817098)\nlower       = c(28.354532974308754, 129.78718537187962)\n\nGPS データの tibble と 座標を準備する。\n\ngps_info = rbind(fudai, hirota, matsushima, mie, naruto, kamigoto, tokunoshima,\n                 okinawa, ishigaki) |> \n  as_tibble(.name_repair = ~c(\"lat\", \"long\")) |> \n  mutate(label = \n           factor(c(\"Fudai\", \"Hirota\", \"Matsushima\", \n                    \"Mie\", \"Naruto\", \"Kamigoto\", \n                    \"Tokunoshima\", \"Okinawa (Chinen and Katsuren)\", \"Ishigaki\")))\n\ngps_info = gps_info |> mutate(label2 = str_to_sentence(label)) \ngps_info = gps_info |> st_as_sf(coords = c(\"long\", \"lat\"), crs = st_crs(jpn))\n\nここで作図をします。\n\nggplot() +\n  geom_sf(fill = \"grey50\", data = jpn, size = 0) +\n  geom_sf_text_repel(aes(label = label), \n               data = gps_info,\n               color = \"black\",\n               family = \"notosans\", \n               fontface = \"bold\",\n               seed = 2020,\n               vjust   = c(1,1,1,\n                           1,1,0,\n                           1,1,1), \n               hjust   = c(0,0,0,\n                           0,0,1,\n                           0,0,0),\n               nudge_x = c(1, 1, 0.5,\n                           1, 1,-0.5,\n                           1, 1, 1),\n               nudge_y = c( 1, 1, -1,\n                           -1,-1, 1,\n                            1, 1, 1),\n               size = 5)  + \n  geom_sf(data = gps_info, size = 3) +\n  geom_sf(data = gps_info, size = 2, color = \"white\") +\n  coord_sf(crs = 6668) +\n  theme(panel.background = element_rect(fill = \"lightblue\", color =NA),\n        panel.border  = element_rect(fill = NA, color =NA),\n        plot.background =  element_rect(fill = \"lightblue\", color =NA),\n        axis.title = element_blank(),\n        axis.line = element_blank(),\n        axis.text = element_blank(),\n        axis.ticks = element_blank())\n\nWarning in st_point_on_surface.sfc(data$geometry): st_point_on_surface may not\ngive correct results for longitude/latitude data"
  },
  {
    "objectID": "windfetch.html",
    "href": "windfetch.html",
    "title": "送風距離の求め方",
    "section": "",
    "text": "海藻は沿岸域における波あたりの強さによって、種数と主構成が変わります(Nishihara, Terada, and Shimabukuro 2011)。 波あたりは外界に面した開放性の評価、生物の分布を用いた評価 (biological exposure scale, Burrows et al. (Burrows, Harvey, and Robb 2008)、 地図を用いて方位ごとの対岸距離 (fetch) を求める、観測機器を設置して波あたりの力や波高などの観測が主な評価方法です(大垣 2009)。\nここでは、fetch, とくに wind fetch （送風距離）の評価方法を消化します。 波あたりの強さを、生物の分布を用いて評価した場合、波あたりと生物の関係の説明は循環論になります。\n送風距離の関数は blasee/fetchR を参考にしました。"
  },
  {
    "objectID": "windfetch.html#必要なパッケージ",
    "href": "windfetch.html#必要なパッケージ",
    "title": "送風距離の求め方",
    "section": "必要なパッケージ",
    "text": "必要なパッケージ\n\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.3.6      ✔ purrr   0.3.4 \n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.0      ✔ stringr 1.4.0 \n✔ readr   2.1.2      ✔ forcats 0.5.1 \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(ggpubr)\nlibrary(sf)\n\nLinking to GEOS 3.9.0, GDAL 3.2.2, PROJ 7.2.1; sf_use_s2() is TRUE\n\nlibrary(magick)\n\nLinking to ImageMagick 6.9.11.60\nEnabled features: fontconfig, freetype, fftw, heic, lcms, pango, webp, x11\nDisabled features: cairo, ghostscript, raw, rsvg\nUsing 32 threads\n\nlibrary(ggrepel)\nlibrary(showtext)\n\nLoading required package: sysfonts\nLoading required package: showtextdb\n\nlibrary(patchwork)"
  },
  {
    "objectID": "windfetch.html#送風距離関数の定義",
    "href": "windfetch.html#送風距離関数の定義",
    "title": "送風距離の求め方",
    "section": "送風距離関数の定義",
    "text": "送風距離関数の定義\n詳細の原因はわかりませんが、fetchR の関数は国土交通省・国土数値情報 のシェープファイルと合わなかったので、ここで再定義しています。\n最大円は calc_circle() で求めます。 map_layer に起点の sf オブジェクトを渡します。 max_dist に最大円の半径を渡します。このときの他には km です。 n_vectors は象限あたりの方位の数です。\n\ncalc_circle = function(map_layer, max_dist = 30, n_vectors = 9) {\n  # Calculate the fetch limits.\n  # max_dist in kilometers\n  delta_theta =  360 / (n_vectors * 4)\n  theta = seq(0, 360, by = delta_theta)\n  n = length(theta)\n  theta = theta[-n]\n  max_dist = units::set_units(1000*max_dist, \"m\")\n  fetch_circle = st_buffer(map_layer, dist = max_dist, nQuadSegs = n_vectors) \n  fetch_limits = st_coordinates(fetch_circle)\n  fetch_limits = fetch_limits[-n, ]\n  list(fetch_circle = fetch_circle, fetch_limits = as_tibble(fetch_limits[order(theta), ]) )\n}\n\ncalc_intersection() は起点からフェッチの最大円まで直線を引きます。 最大円内に交差したポリゴンの交差点を特定し、起点に一番近い交差点を返します。 交差点が内場合は、最大円までの距離を返します。 これらの関数は tidyverse や sf が必要です。\n\ncalc_intersection = function(fetch_limit, origin, map_layer) {\n  X = rbind(st_coordinates(origin), fetch_limit) |> as.matrix()\n  fetch_vector = st_linestring(X) |> st_sfc(crs = st_crs(map_layer))\n  fetch_intersection = st_intersection(fetch_vector, map_layer) |> st_cast(\"POINT\")\n  if(length(fetch_intersection) > 0) {\n    intersection_coordinate  = st_coordinates(fetch_intersection) |> as.matrix()\n    distance_from_origin = st_distance(origin, fetch_intersection) |> as.vector()\n    closest_intersection = min(distance_from_origin)\n    n = which(near(distance_from_origin, closest_intersection)) \n    intersection_coordinate = intersection_coordinate[n,]\n  } else {\n    intersection_coordinate  = st_point(as.matrix(fetch_limit)) |> st_sfc(crs = st_crs(map_layer))\n    distance_from_origin = st_distance(origin, intersection_coordinate) |> as.vector()\n    closest_intersection = distance_from_origin\n    intersection_coordinate = st_coordinates(intersection_coordinate) |> as.matrix()\n  }\n  X = matrix(c(st_coordinates(origin), intersection_coordinate[1:2]), ncol = 2, byrow =T)\n  fetch_vector = st_linestring(X) |> st_sfc(crs = st_crs(map_layer))\n  fetch_length = st_length(fetch_vector) \n  fetch_vector |> st_as_sf() |> mutate(length = fetch_length)\n}\n\n\n################################################################################\n# Prepare data set #############################################################\n# GPS coordinates to determine wind fetch ######################################\nmatsushimagps = c(38.34549669653925, 141.0807915733725)\nhirotagps     = c(39.02402594131857, 141.78725806724896)\nbisegps       = c(26.704302654710496, 127.85974269102186)\narikawagps    = c(32.98827976845565, 129.11838896005543)\ntainouragps   = c(32.95134175383013, 129.1096027426365)\nomuragps      = c(32+52/60+11.9/60/60, 129+58/60+24.5/60/60)\n\ngps_info = rbind(matsushimagps, hirotagps, bisegps, arikawagps, tainouragps, omuragps) |> \n  as_tibble(.name_repair = ~c(\"lat\", \"long\")) |> \n  mutate(name = c(\"matsushimagps\", \"hirotagps\", \n                  \"bisegps\", \"arikawagps\", \"tainouragps\", \"omuragps\")) |> \n  mutate(label = str_to_sentence(str_remove(name, pattern = \"(gps)\"))) \n\ngps_info = gps_info |> \n  mutate(label2 = str_to_sentence(label)) |> \n  mutate(label2 = str_glue(\"{label2} {ifelse(str_detect(label2, 'Bise'), 'Point', 'Bay')}\"))\n\n# Prepare the Coordinate Reference System to be EPSG:4326 (Which is WGS 84)\n# See st_crs(4326) for details\ngps_info = gps_info |> select(long, lat, name) |> st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326, agr = \"constant\")\n\n# Load the map shape files #####################################################\n# The map uses the ITRF94 system (st_crs(map_poly))\n# gsi_low = read_sf(\"~/Lab_Data/Japan_map_data/GSI/coastl_jpn.shp\")\n# gsi_low = read_sf(\"~/Lab_Data/Japan_map_data/GADM_old/JPN_adm1.shp\")\nmap_poly = read_sf(\"~/Lab_Data/Japan_map_data/GSI/polbnda_jpn.shp\")\nmap_poly = map_poly |> select(nam, geometry)\n\n# Convert the CRS to EPSG:2450 #################################################\nmap_poly = st_transform(map_poly, st_crs(2450))\ngps_info  = st_transform(gps_info, st_crs(2450))\n\n################################################################################\n# Do the analysis one location at a time. ######################################\nptsize = 1\nmax_dist = 10 # In km\nn_vectors = 3*9 # The number of vectors in every quadrant.\n\nlocation = \"Hirota Bay\"\npolygon_layer = subset(map_poly, str_detect(nam, \"Iwate\")) |> st_union() \nsite_layer    = subset(gps_info, str_detect(name, \"hiro\"))\nfetch_limits = calc_circle(site_layer, max_dist = max_dist, n_vectors = n_vectors)\nfout = fetch_limits$fetch_limits |> \n  mutate(fe  = map2(X,Y,function(x,y) cbind(x,y))) |> \n  mutate(geometry = map(fe, calc_intersection, origin = site_layer, map_layer = polygon_layer))\nfout = fout |> select(geometry) |>  unnest(geometry) |> st_as_sf()\ntemp_layer = st_crop(polygon_layer, st_bbox(fetch_limits$fetch_circle))\nmean_fetch = fout |> pull(length) |> mean() |> as.numeric()\nsd_fetch = fout |> pull(length) |> sd() |> as.numeric()\nmax_fetch = fout |> pull(length) |> as.numeric()\nman_n = sum(near(max_fetch, max_dist * 1000))\ntot_n = length(max_fetch)\n\np1 = ggplot() + \n  geom_sf(data = temp_layer, color = NA) +\n  geom_sf(data = fout) +\n  geom_sf(data = site_layer, color = \"red\", size = ptsize) +\n  labs(title = str_glue(\"The mean ± sd fetch for {location} is {format(mean_fetch, digits = 4)} ± {format(sd_fetch, digits = 4)} m.\"),\n       subtitle = str_glue(\"{man_n} out of {tot_n} vectors were at the upper limit.\"))\n\nlocation = \"Matsushima Bay\"\npolygon_layer = subset(map_poly, str_detect(nam, \"Miyag\")) |> st_union() \nsite_layer    = subset(gps_info, str_detect(name, \"matsu\"))\nfetch_limits = calc_circle(site_layer, max_dist = max_dist, n_vectors = n_vectors)\nfout = fetch_limits$fetch_limits |> as_tibble() |> \n  mutate(fe  = map2(X,Y,function(x,y) cbind(x,y))) |> \n  mutate(geometry = map(fe, calc_intersection, origin = site_layer, map_layer = polygon_layer))\nfout = fout |> select(geometry) |>  unnest(geometry) |> st_as_sf()\ntemp_layer = st_crop(polygon_layer, st_bbox(fetch_limits$fetch_circle))\nmean_fetch = fout |> pull(length) |> mean() |> as.numeric()\nsd_fetch = fout |> pull(length) |> sd() |> as.numeric()\nmax_fetch = fout |> pull(length) |> as.numeric()\nman_n = sum(near(max_fetch, max_dist * 1000))\ntot_n = length(max_fetch)\n\np2 = ggplot() + \n  geom_sf(data = temp_layer, color = NA) +\n  geom_sf(data = fout) +\n  geom_sf(data = site_layer, color = \"red\", size = ptsize) +\n  labs(title = str_glue(\"The mean ± sd fetch for {location} is {format(mean_fetch, digits = 4)} ± {format(sd_fetch, digits = 4)} m.\"),\n       subtitle = str_glue(\"{man_n} out of {tot_n} vectors were at the upper limit.\"))\n\nlocation = \"Bise Point\"\npolygon_layer = subset(map_poly, str_detect(nam, \"Okinawa\")) |> st_union() \nsite_layer    = subset(gps_info, str_detect(name, \"bise\"))\nfetch_limits = calc_circle(site_layer, max_dist = max_dist, n_vectors = n_vectors)\nfout = fetch_limits$fetch_limits |> as_tibble() |> \n  mutate(fe  = map2(X,Y,function(x,y) cbind(x,y))) |> \n  mutate(geometry = map(fe, calc_intersection, origin = site_layer, map_layer = polygon_layer))\nfout = fout |> select(geometry) |>  unnest(geometry) |> st_as_sf()\ntemp_layer = st_crop(polygon_layer, st_bbox(fetch_limits$fetch_circle))\nmean_fetch = fout |> pull(length) |> mean() |> as.numeric()\nsd_fetch = fout |> pull(length) |> sd() |> as.numeric()\nmax_fetch = fout |> pull(length) |> as.numeric()\nman_n = sum(near(max_fetch, max_dist * 1000))\ntot_n = length(max_fetch)\n\np3 = ggplot() + \n  geom_sf(data = temp_layer, color = NA) +\n  geom_sf(data = fout) +\n  geom_sf(data = site_layer, color = \"red\", size = ptsize) +\n  labs(title = str_glue(\"The mean ± sd fetch for {location} is {format(mean_fetch, digits = 4)} ± {format(sd_fetch, digits = 4)} m.\"),\n       subtitle = str_glue(\"{man_n} out of {tot_n} vectors were at the upper limit.\"))\n\nlocation = \"Omura Bay\"\npolygon_layer = subset(map_poly, str_detect(nam, \"Nagasaki\")) |> st_union() \nsite_layer    = subset(gps_info, str_detect(name, \"omura\"))\nfetch_limits = calc_circle(site_layer, max_dist = max_dist, n_vectors = n_vectors)\nfout = fetch_limits$fetch_limits |> as_tibble() |> \n  mutate(fe  = map2(X,Y,function(x,y) cbind(x,y))) |> \n  mutate(geometry = map(fe, calc_intersection, origin = site_layer, map_layer = polygon_layer))\nfout = fout |> select(geometry) |>  unnest(geometry) |> st_as_sf()\ntemp_layer = st_crop(polygon_layer, st_bbox(fetch_limits$fetch_circle))\nmean_fetch = fout |> pull(length) |> mean() |> as.numeric()\nsd_fetch = fout |> pull(length) |> sd() |> as.numeric()\nmax_fetch = fout |> pull(length) |> as.numeric()\nman_n = sum(near(max_fetch, max_dist * 1000))\ntot_n = length(max_fetch)\n\np4 = ggplot() + \n  geom_sf(data = temp_layer, color = NA) +\n  geom_sf(data = fout) +\n  geom_sf(data = site_layer, color = \"red\", size = ptsize) +\n  labs(title = str_glue(\"The mean ± sd fetch for {location} is {format(mean_fetch, digits = 4)} ± {format(sd_fetch, digits = 4)} m.\"),\n       subtitle = str_glue(\"{man_n} out of {tot_n} vectors were at the upper limit.\"))\n\nlocation = \"Arikawa Bay\"\npolygon_layer = subset(map_poly, str_detect(nam, \"Nagasaki\")) |> st_union() \nsite_layer    = subset(gps_info, str_detect(name, \"arik\"))\nfetch_limits = calc_circle(site_layer, max_dist = max_dist, n_vectors = n_vectors)\nfout = fetch_limits$fetch_limits |> as_tibble() |> \n  mutate(fe  = map2(X,Y,function(x,y) cbind(x,y))) |> \n  mutate(geometry = map(fe, calc_intersection, origin = site_layer, map_layer = polygon_layer))\nfout = fout |> select(geometry) |>  unnest(geometry) |> st_as_sf()\ntemp_layer = st_crop(polygon_layer, st_bbox(fetch_limits$fetch_circle))\nmean_fetch = fout |> pull(length) |> mean() |> as.numeric()\nsd_fetch = fout |> pull(length) |> sd() |> as.numeric()\n\nmax_fetch = fout |> pull(length) |> as.numeric()\nman_n = sum(near(max_fetch, max_dist * 1000))\ntot_n = length(max_fetch)\n\np5 = ggplot() + \n  geom_sf(data = temp_layer, color = NA) +\n  geom_sf(data = fout) +\n  geom_sf(data = site_layer, color = \"red\", size = ptsize) +\n  labs(title = str_glue(\"The mean ± sd fetch for {location} is {format(mean_fetch, digits = 4)} ± {format(sd_fetch, digits = 4)} m.\"),\n       subtitle = str_glue(\"{man_n} out of {tot_n} vectors were at the upper limit.\"))\n\nlocation = \"Tainoura Bay\"\npolygon_layer = subset(map_poly, str_detect(nam, \"Nagasaki\")) |> st_union() \nsite_layer    = subset(gps_info, str_detect(name, \"tain\"))\nfetch_limits = calc_circle(site_layer, max_dist = max_dist, n_vectors = n_vectors)\nfout = fetch_limits$fetch_limits |> as_tibble() |> \n  mutate(fe  = map2(X,Y,function(x,y) cbind(x,y))) |> \n  mutate(geometry = map(fe, calc_intersection, origin = site_layer, map_layer = polygon_layer))\nfout = fout |> select(geometry) |>  unnest(geometry) |> st_as_sf()\ntemp_layer = st_crop(polygon_layer, st_bbox(fetch_limits$fetch_circle))\nmean_fetch = fout |> pull(length) |> mean() |> as.numeric()\nsd_fetch = fout |> pull(length) |> sd() |> as.numeric()\n\nmax_fetch = fout |> pull(length) |> as.numeric()\nman_n = sum(near(max_fetch, max_dist * 1000))\ntot_n = length(max_fetch)\n\np6 = ggplot() + \n  geom_sf(data = temp_layer, color = NA) +\n  geom_sf(data = fout) +\n  geom_sf(data = site_layer, color = \"red\", size = ptsize) +\n  labs(title = str_glue(\"The mean ± sd fetch for {location} is {format(mean_fetch, digits = 4)} ± {format(sd_fetch, digits = 4)} m.\"),\n       subtitle = str_glue(\"{man_n} out of {tot_n} vectors were at the upper limit.\"))\n\n\n(p1 + p2 + p3) / (p4 + p5 + p6)\n\n\n\n\n\npdfname = \"~/Downloads/Determine_fetch.pdf\"\npngname = str_replace(pdfname, \"pdf\", \"png\")\nggsave(pdfname, width = 5*80, height = 4*80, units = \"mm\")\nimg = image_read(pdfname, density = 300)\nimg |> image_write(pngname, format = \"png\")"
  }
]